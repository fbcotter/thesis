\chapter{Introduction}\label{ch:intro}

This work is stimulated by the intuition that wavelet decompositions, in particular
complex wavelet transforms, are good building blocks for doing image recognition
tasks. Their well understood and well defined behaviour as well as the
similarities seen in learned networks, implies that there is potential gain for
thinking about CNN layers in a new light. 

To explore and test this intuition, we begin by looking at one of the most popular current uses of
wavelets in image recognition tasks, in particular the Scattering Transform. 

\section{Series Expansions of Signals}
Look at the intro to Vetterli's book. Want to make a statement about expanding
signals in some form or another.

\section{Contributions}
The contributions and layout of this thesis are:

\begin{itemize}
  \item \textbf{Software for wavelets and $\DTCWT$ based ScatterNet (chapter 3)}
  \item \textbf{ScatterNet analysis and visualizations (chapter 4)}. Presented
    at MLSP2017, this chapter 
  \item \textbf{Invariant Layer/Learnable ScatterNet (chapter 5)} Presenting at
    ICIP2019.
  \item \textbf{Learning convolutions in the wavelet domain (chapter 6)}.
\end{itemize}

\subsection{Desirable Properties}
Unlike CNNs introduced earlier which have little prior constraints (apart
from the commonly used $L_2$ regularization), the scattering operator may be 
thought of as an operator $S$ that imposes structural priors on learning by
extracting features with manually chosen, desirable properties. The extracted
features can be used In classical
paradigms of image understanding, it makes sense to add these priors, but it
remains yet to be shown that these help learning.

limit variability these properties areview on these
properties are manually chosen with the ultimate goal of aiding image
understanding. 

