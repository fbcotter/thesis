\section{Scatternets}\label{ch:scatternets}
% Let us define the pairs of authors here
  Scatternets have been a very large influence on
  our work, as well as being quite distinct from the previous discussions on
  learned methods. They were first introduced by  
  \citeauthor{bruna_classification_2011} in their work 
  \cite{bruna_classification_2011}, and then were rigorously defined by Mallat
  in \cite{mallat_group_2012}.   

  While CNNs have the ability to learn invariances to nuiscance variabilities, 
  the properties and optimal configurations are not well understood. 
  It typically takes multiple trials
  by an expert to find the correct hyperparameters for these networks. A
  scattering transform instead builds well understood and well defined invariances. 
  
  We first review some of the desirable invariances before describing how a
  ScatterNet achieves them.

\subsection{Desirable Properties}
\subsubsection{Translation Invariance}
  Translation is often defined as being uninformative for classification --- an
  object appearing in the centre of the image should be treated the same way as
  an the same object appearing in the corner of an image, i.e.,\ a
  representation $\Phi x$ is invariant to global translations $x_c(\bmu{u}) =
  x(\bmu{u}-\bmu{c})$ by 
  $\bmu{c}=(c_1,c_2) \in \mathbb{R}^2$ if
  % The first requirement for Scatternets - translation invariance
  \begin{equation}\label{eq:scat_trans_invariance}
    \norm{\Phi x_c - \Phi x} \leq C
  \end{equation}
  for some small constant $C>0$.
  Note that we may instead want only local translation invariance and
  restrict the distance $|\bmu{c}|$ for which \eqref{eq:scat_trans_invariance}
  is true. 
  
  Note that CNNs are naturally covariant to translations in the pixel space, 
  so $\Phi x_c = (\Phi x)_c,\ \bmu{c} \in \mathbb{Z}^2$. Of course, natural objects
  exist in continuous space and are sampled, and any two images of the same
  scene taking with small camera disturbances are unlikely to be at integer pixel 
  shifts of each other.

\subsubsection{Stability to Noise}
  Stability to additive noise is another useful invariance to build,
  as it is a common feature in sampled signals. Stability is defined in terms of
  Lipschitz continuity, which is a strong form of uniform continuity for
  functions, which we briefly introduce here.

  Formally, a Lipschitz continuous function is limited in how fast it can change;
  there exists an upper bound on the gradient the function can take, although it
  doesn't necessarily need to be differentiable everywhere. The modulus operator
  $|x|$ is a good example of a function that has a bounded derivative and so is
  Lipschitz continuous, but isn't differentiable everywhere. Alternatively, the
  modulus squared has derivative everywhere but is not Lipshitz continuous as
  its gradient grows with $x$.

  \begin{figure}
    \begin{center}
      \includegraphics[scale=0.6]{\imgpath/Lipschitz_continuity.png}
      \mycaption{A Lipschitz continuous function}{There is a cone for this
              function (shown in white) such that the graph always remains entirely outside
              the cone as it is shifted across. The minimum gradient needed for this to hold
              is called the `best Lipschitz constant'.}
      \label{fig:lipschitz}
    \end{center}
  \end{figure}

  To be stable to additive noise, we require that for 
  a new signal $x'(\bmu{u}) = x(\bmu{u}) + \epsilon(\bmu{u})$, there must exist
  a bounded $C>0$ s.t.
  % The second requirement - noise stability
  \begin{equation}\label{eq:scat_noise_stability}
    \|\Phi x' - \Phi x\| \leq C \|x' - x\|
  \end{equation}

\subsubsection{Stability to Deformations}
  Small deformations are important to be invariant to. However, this must be
  limited. It is important to ignore intra-class variations 
  but not so invariant that an object can
  morph into another (in the case of MNIST for example, we do not want to be so
  stable to deformations that 7s can map to 1s). 
  
  Formally, for a new signal
  $x_{\tau}(\bmu{u}) = x(\bmu{u}-\tau(\bmu{u}))$, where $\tau(\bmu{u})$ is a non
  constant displacement field (i.e.,\ not just a translation) that deforms the
  image, we require a $C_\tau>0$ s.t.
  % The third requirement - deformation stability
  \protect\begin{equation}\label{eq:scat_deformation_stability}
    \|\Phi x_{\tau} - \Phi x \| \leq C_\tau \|x\| \sup_{\bmu{u}} |\nabla\tau(\bmu{u})|
  \protect\end{equation}
  The term on the right $|\nabla\tau(\bmu{u})|$ measures the deformation
  amplitude, so the supremum of it is a limit on the global defomation amplitude.

\subsection{Definition}
  A Fourier modulus satisfies the first two of these requirements, in that it is
  both translation invariant and stable to additive noise, but it is unstable to
  deformations due to the infinite support of the sinusoid basis functions it
  uses. It also loses too much information --- very different signals can all
  have the same Fourier modulus, e.g.\ a chirp, white noise and the Dirac delta
  function all have flat spectra.

  Another translation invariant and stable operator is the averaging kernel, and
  \Mallat\ use this to make the zeroth scattering coefficient:
  \begin{equation}
    S[\emptyset]x \definedas x \conv \phitd_J\left(2^J \bmu{u}\right)
  \end{equation}
  which is translation invariant to shifts less than $2^J$. It unfortunately
  results in a loss of information due to the removal of high frequency content.
  This is easy to see as the wavelet operator 
  $Wx = \{ x \conv \phitd_J, x \conv \psitd_\lambda \}_\lambda$
contains all the information of $x$, whereas the zeroth scattering coefficient
is simply the lowpass portion of $W$. 

This high frequency content can be `recovered' by keeping the wavelet
coefficients. The wavelet terms, like a convolutional layer in a CNN, is only
covariant to shifts rather than invariant. This covariance happens in the real 
and imaginary parts which both vary rapidly. Fortunately, its modulus is much
smoother and gives a good measure for the frequency-localized energy content at
a given spatial location\footnote{Interestingly, the modulus operator can often still be
inverted, and hence does not lose any information, due to the redundancies of
the complex wavelet transform \cite{waldspurger_phase_2012}}. Unlike
the Fourier modulus, the complex wavelet modulus is stable to deformations due
to the grouping together frequencies into dyadic packets
\cite{mallat_group_2012}. 

We combine the wavelet transform and modulus operators into one operator
$\tilde{W}$:
\begin{align}
  \tilde{W}x &= \{ x \conv \phitd_J,\ |x \conv \psitd_\lambda| \}_\lambda \label{eq:ch2:wave3}\\
             &= \{ x \conv \phitd_J,\ U[\lambda]x \}_\lambda 
\end{align}
where the $U$ terms are called the \emph{propagated} signals and $\lambda =
(j,k)$ indexes the scale and orientation of wavelet used.
These $U$ terms are invariant for shifts of up to $2^j$. \Mallat\ choose to
keep the same level of invariance as the zeroth order coefficients ($2^J$) 
by further averaging. This makes the first ordering scattering coefficients:
\begin{equation}
  S[\lambda_1]x \definedas U[\lambda_1]x \conv \phitd_J
  = |x \conv \psitd_{\lambda_1}| \conv \phitd_J
\end{equation}
Again this averaging comes at a cost of discarding high frequency information,
this time about the wavelet sparsity signal $U[\lambda] = |x \conv
\psi_\lambda|$ instead of the input signal $x$. We can recover this information
by repeating the above process. 
\begin{align}
  S[\lambda_1, \lambda_2]x &\definedas U[\lambda_2]U[\lambda_1]x \\
                           &=||x \conv \psitd_{\lambda_1}| \conv \psitd_{\lambda_2}| \conv \phitd_J
\end{align}
In general, let $p=(\lambda_1, \lambda_2, \ldots \lambda_m)$ be a path of length
$m$ describing the order of application of wavelets, and define:
\begin{align}
  U[p]x &= U[\lambda_m]U[\lambda_{m-1}]\cdots U[\lambda_1]x \\
        &= || \cdots |x \conv \psitd_{\lambda_1}| \conv \psitd_{\lambda_2} | \cdots
  \conv \psitd_{\lambda_m}|
\end{align}
and the $m$th order scattering coefficient along the path $p$ is $S[p]x = U[p]x
\conv \phitd_J$. Further, let $p+\lambda = (\lambda_1, \lambda_2, \ldots
\lambda_m, \lambda)$.  This allows us to recursively define the next set of
\emph{propagated} and \emph{scattering} coefficients by using $\tilde{W}$:
\begin{equation}
  \tilde{W}U[p]x = \{ S[p]x,\ U[p+\lambda]x \}_\lambda \label{eq:ch2:recursive}
\end{equation}
which is shown in \autoref{fig:ch2:scatternet_mallat}
  \begin{figure}
    \centering
      \includegraphics[width=\textwidth]{\imgpath/scatternet_diagram.png}
      \mycaption{The Scattering Transform}{Scattering outputs
               are the leftward pointing arrows $S[p]x$, and the intermediate 
               coefficients $U[p]x$ are the centre nodes of the tree. Taken
               from \cite{bruna_invariant_2013}.}
      \label{fig:ch2:scatternet_mallat}
  \end{figure}

\subsection{Resulting Properties}
For ease, let us define the `$m$th order scattering coefficients' as $S_m$ which
is the set of all coefficients with path length $m$. Further let $S$ be the set
of all scattering coefficients of any path length. The energy $\energy{Sx}$ we 
then define as
\begin{equation}
  \energy{Sx} = \sum_p \energy{S[p]x}
\end{equation}
We can make $W$ non-expansive with appropriate scaling. Further, define the energy $\energy{Wx}$ as
\begin{equation}
  \energy{Wx} = \energy{x\conv \phitd} + \sum_\lambda \energy{x \conv
  \psitd_\lambda} 
\end{equation}
then by Plancherel's formula
\begin{equation}
  (1-\epsilon)\energy{x} \leq \energy{Wx} \leq \energy{x}
\end{equation}
For the Morlet wavelets originally used in \cite{bruna_invariant_2013},
$\epsilon=0.25$, for the $\DTCWT$ $\epsilon = 0$.


\subsubsection{Translation Invariance}
\newcommand{\shift}{\mathcal{L}_c}

This is proven in section 2.4 of \cite{mallat_group_2012}. We
have so far described the Scattering representation as being `translation
invariant for shifts up to $2^J$'. We formalize this statement here.

For a 2-D averaging filter based on a father wavelet $\phitd$, 
$\phitd_J = 2^{-J}\phitd\left(2^{-J}\xy\right)$ it is proven in Appendix B of
\cite{mallat_group_2012} that shifting 
it by $\bmu{c}$, which we denote as $\mathcal{L}_c$, is Lipschitz continuous:
\begin{equation}
  \norm{\shift \phi_J - \phi_J} \leq 2^{-J+2}\lnorm{\nabla \phitd}{1}|\bmu{c}|
\end{equation}
where $\lnorm{\nabla \phitd}{1}$ is the $\ell_1$ norm of the grad of $\phitd$.

For simplicity, let us define $A_J x = \phitd_J \conv x$ and $Sx = A_J Ux$. Then we get:
\begin{align}
  \norm{S\shift x - Sx} & = \norm{\shift A_J Ux - A_J Ux} \\
                         &\leq \norm{\shift A_j - A_j}\norm{Ux} \\
                         & \leq 2^{-J+2}\lnorm{\nabla \phitd}{1}|\bmu{c}|\norm{x}
\end{align}
% as $\norm{Ux} \leq \norm{x}$. 

\subsubsection{Stability to Noise}
As $W$ is non-expansive and the complex modulus is also non-expansive
\begin{equation}
  \norm{\tilde{W}x - \tilde{W}y} \leq \norm{x-y}
\end{equation}
As we have already shown that $S$ is the repeated application of $\tilde{W}$ in
\eqref{eq:ch2:recursive}, we can then say
\begin{equation}
  \norm{Sx - Sy} \leq \norm{x-y}
\end{equation}
making scattering non-expansive and stable to noise.

\subsubsection{Stability to deformations}
From \cite{mallat_group_2012}, if $\mathcal{L}_\tau x = x(\xy -\tau(\xy))$ is a
diffeomorphism which is bounded with $\norm{\nabla \tau}_{\infty} \leq 1/2$,
then there exists a $K_L > 0$ such that:
%
\begin{equation}
  \norm{ S \mathcal{L}_{\tau}x  - S x} \leq K_L P F(\tau) \norm{x}
  \label{eq:stability}
\end{equation}
%
where $P = \F{length}(p)$ is the scattering order, and $F(\tau)$ is a function
of the size of the displacement, derivative and Hessian of $\tau$, $H(\tau)$
\cite{mallat_group_2012}: 
\begin{equation}
  F(\tau) = 2^{-J} \norm{\tau}_{\infty} + \norm{\nabla \tau}_{\infty} \max\left(\log
  \frac{\norm{\Delta \tau}_{\infty} }{\norm{\nabla \tau}_{\infty}}, 1 \right) +
  \norm{H(\tau)}_{\infty}
\end{equation}

\subsubsection{Energy Decay}
As $m \rightarrow \infty$ the invariant cofficients of path length $m$, $U_m$,
decay towards zero:
\begin{equation}
  \lim_{m \rightarrow \infty} U_m = 0
\end{equation}
This is an important property that suggests that we can stop scattering beyond a
certain point. For image sizes on the order of a few hundred pixels by a few
hundred pixels, $m=3$ captures about $99\%$ of the input energy. For many works
using scattering transforms after \cite{bruna_invariant_2013} such as
\cite{oyallon_deep_2015, oyallon_hybrid_2017, oyallon_scaling_2017}, setting
$m=2$ was found to be sufficient.

\subsubsection{Number of Coefficients}
While we have so far talked about non sampled signals $x(\xy),\ \xy \in
\reals[2]$, in practice we want to apply scattering to sampled signals $x[\nn],\
\nn \in \integers[2]$. The averaging by $\phitd_J$ means that we can subsample
$Sx$ by $2^J$ in each direction. However, now we need also need to index all the
paths $p$ that can be used to create the scattering coefficients. Limiting
ourselves to $m=2$ and using a wavelet transform with $J$ scales and $K$
discrete orientations the number of paths for each $S_m$ is the cardinality of
the set $p_m$:
\begin{align}
  n(p_0) &= 1 \\
  n(p_1) &= JK \\
  n(p_2) &= (J-1)K^2 + (J-2)K^2 + \ldots + K^2 \\
         &= \frac{1}{2}J(J-1)K^2
\end{align}
The reason $n(p_2) \neq J^2 K^2$ is due to the demodulating effect of the
complex modulus. As $|x \conv \psitd_\lambda|$ is more regular than $x \conv
\psitd_\lambda$, $|x \conv \psitd_\lambda| \conv \psitd_{\lambda'}$ is only
non-negligible if $\psitd_{\lambda'}$ is located at lower frequencies than
$\psitd_\lambda$. This means, we can discard over half of the scattering paths as
their value will be near zero.

Summing up the above three equations and factoring in the reduced sample rate
allowable due to averaging, for an input with $N$ pixels, the scattering
representation will have $\frac{N}{2^J}\left(1+JK+\frac{1}{2}J(J-1)K^2 \right)$
pixels.

