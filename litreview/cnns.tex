\section{Neural Networks}\label{sec:ch2:cnns}
\subsection{The Neuron and Single Layer Neural Networks}

\begin{figure}
  \centering
  % \input{\imgpath/singlelayer}
  \includegraphics{\imgpath/singlelayer.pdf}
  \mycaption{A single neuron}{The neuron is composed of inputs $x_i$, weights
    $w_i$ (and a bias term), as well as an activation function. Typical activation
    functions include the sigmoid function, tanh function and the ReLU}
  \label{fig:ch2:singlelayer}
\end{figure}

The neuron, shown in \autoref{fig:ch2:singlelayer} is the core building block of
Neural Networks. It takes the dot product between an input vector $\vec{x} \in
\reals[D]$ and a weight vector $\vec{w}$, before applying a chosen nonlinearity,
$g$. I.e.
%
\begin{equation}
  y = g(\langle\vec{x}, \vec{w}\rangle) = g\left(\sum_{i=0}^{D} x_i w_i \right) 
\end{equation}
%
where we have used the shorthand $b=w_0$ and $x_0 = 1$. Also note that we will
use the neural network common practice of calling the \emph{weights} $w$,
compared to the parameters $\theta$ we have been discussing thus far.

Typical nonlinear functions $g$ are the sigmoid function (already presented in 
\eqref{eq:ch2:sigmoid}), but also common are the tanh and ReLU functions:
\begin{align}
  \F{tanh}(x) &= \frac{e^{x} - e^{-x}}{e^{x} + e^{-x}} \label{eq:ch2:tanh}\\
  \F{ReLU}(x) &= \max (x, 0) \label{eq:ch2:relu}
\end{align}

See \autoref{fig:ch2:nonlinearities} for plots of these. The original Rosenblatt
perceptron \cite{rosenblatt_perceptron:_1958} used the Heaviside function
$\F{H}(x) = \mathbb{I}(x > 0)$. % The sigmoid nonlinearity
% naturally arises when we want to do classification with class labels $\{0,1\}$,
% as does the tanh function for classification with class labels $\{-1, 1\}$. The
% ReLU however is used more often in deeper architectures
% \cite{nair_rectified_2010}.

Note that if $\langle\vec{w}, \vec{w}\rangle = 1$ then $\langle\vec{x},
\vec{w}\rangle$ is the distance from the point $\vec{x}$ to the hyperplane with
normal $\vec{w}$. With general $\vec{w}$ this can be thought of as a scaled
distance. Thus, the weight vector $\vec{w}$ defines a hyperplane in $\reals[D]$
which splits the space into two. The choice of nonlinearity then affects how
points on each side of the plane are treated. For a sigmoid, points far below
the plane get mapped to 0 and points far above the plane get mapped to 1 (with
points near the plane having a value of 0.5). For tanh nonlinearities, these
points get mapped to -1 and 1. For ReLU nonlinearities, every point below the
plane ($\langle\vec{x}, \vec{w}\rangle < 0$) gets mapped to zero and every point
above the plane keeps its inner product value. 

Nearly all modern neural networks use the ReLU nonlinearity and it has
been credited with being a key reason for the recent surge in deep learning
success \cite{glorot_deep_2011, nair_rectified_2010}. In particular:
\begin{enumerate}
\item It is less sensitive to initial conditions as the gradients that
  backpropagate through it will be large even if $x$ is large. A common
  observation of sigmoid and tanh non-linearities was that their learning would
  be slow for quite some time until the neurons came out of saturation, and then
  their accuracy would increase rapidly before levelling out again at
  a minimum~\cite{glorot_understanding_2010}. The ReLU, on the other hand, has
  constant gradient.
\item It promotes sparsity in outputs, by setting them to a hard 0. Studies
  on brain energy expenditure suggest that neurons encode information in
  a sparse manner. \cite{lennie_cost_2003} estimates the percentage of
  neurons active at the same time to be between 1 and 4\%. Sigmoid and tanh
  functions will typically have \emph{all} neurons firing, while 
  the ReLU can allow neurons to fully turn off.
\end{enumerate}

\begin{figure}
    \qquad
    % \input{\imgpath/nonlinearities}
    \includegraphics{\imgpath/nonlinearities.pdf}
    \quad
    % \input{\imgpath/nonlinearities_grad}
    \includegraphics{\imgpath/nonlinearities_grad.pdf}
  \centering
  \mycaption{Common Neural Network nonlinearities and their gradients}{The sigmoid, tanh and ReLU
  nonlinearities are commonly used activation functions for neurons. Note the
  different properties. In particular, the tanh and sigmoid have the nice
  property of being smooth but can have saturation when the input is either
  largely positive or largely negative, causing little gradient to flow back
  through it. The ReLU does not suffer from this problem, and has the additional
  nice property of setting values to exactly 0, making a sparser output
  activation.}
  \label{fig:ch2:nonlinearities}
\end{figure}

\subsection{Multilayer Perceptrons}
As mentioned in the previous section, a single neuron can be thought of as a
separating hyperplane with an activation that maps the two halves of the space
to different values. Such a linear separator is limited, and famously cannot
solve the XOR problem \cite{minsky_perceptrons:_1988}. Fortunately, adding a
single hidden layer like the one shown in \autoref{fig:ch2:hidden} can change
this, and it is provable that with an infinitely wide hidden layer, a neural
network can approximate any function \cite{hornik_multilayer_1989,
cybenko_approximation_1989}. 

The forward pass of such a network with one hidden layer of $H$ units is:
%
\begin{align}
  h_i & =  g\left(\sum_{j=0}^{D} x_j w_{ij}^{(1)}\right) \\
  y & =  \sum_{k=0}^{H} h_k w^{(2)}_{k}
\end{align}
%
where $w^{(l)}$ denotes the weights for the $l$-th layer, of which
\autoref{fig:ch2:hidden} has 2. Note that these individual layers are often
called \emph{fully connected} as each node in the previous layer affects every
node in the next.

If we were to expand this network to have $L$ such fully connected layers, we
could rewrite the action of each layer in a recursive fashion:
%
\begin{align}
  Y^{(l+1)} &= W^{(l+1)}X^{(l)}  \label{eq:ch2:fc1}\\
  X^{(l+1)} &= g\left(Y^{(l+1)}\right) \label{eq:ch2:fc2} 
\end{align}
where $W$ is now a weight matrix, acting on the vector of previous layer's
outputs $X^{(l)}$. As we are now considering every layer an input to the next
stage, we have removed the $h$ notation, and added the superscript $(l)$ to
define the depth. $X^{(0)}$ is the network input and $Y^{(L)}$ is the network
output. Let us say that the output has $C$ nodes, and a hidden layer $X^{(l)}$
has $C_l$ nodes.

\begin{figure}[t]
  \centering
  % \input{\imgpath/network_mlp}
  \includegraphics{\imgpath/network_mlp.pdf}
  \mycaption{Multi-layer perceptron}{Expanding the single neuron from 
  \autoref{fig:ch2:singlelayer} to a network of neurons. The internal
  representation units are often referred to as the \emph{hidden layer} as they
  are an intermediary between the input and output.}
  \label{fig:ch2:hidden}
\end{figure}

\subsection{Backpropagation}
It is important to truly understand backpropagation when designing neural
networks, so we describe the core concepts now for a neural network with
$L$ layers.

The delta rule, initially designed for networks with no hidden layers 
\cite{widrow_neurocomputing:_1988}, was expanded to what we now consider
\emph{backpropagation} in \cite{rumelhart_parallel_1986}. While backpropagation
is conceptually just the application of the chain rule, Rumelhart, Hinton, and
Williams successfully updated the delta rule to networks with hidden layers,
laying a key foundational step for deeper networks. 

With a deep network, calculating $\dydx{J}{w}$ may not seem
particularly obvious if $w$ is a weight in one of the earlier layers. We need
to define a rule for updating the weights in all $L$ layers of the network,
$W^{(1)}, W^{(2)}, \ldots W^{(L)}$ however, only the final set $W^{(L)}$ are
connected to the objective function $J$. 

\subsubsection{Regression Loss}
Let us start with writing down the derivative of $J$ with respect to the network
output $Y^{(L)}$ using the regression objective function \eqref{eq:ch2:mle_reg}. 
As we now have two superscripts,
one for the sample number and one for the layer number, we combine them into a
tuple of superscripts. 
\begin{align}
  \dydx{J}{Y^{(L)}} &= \dydx{}{Y^{(L)}} \left( \frac{1}{N}\sum_{n=1}^{N_b}\frac{1}{2} \left(y^{(n)} - Y^{(L, n)}\right)^2\right) \\
                    &= \frac{1}{N}\sum_{n=1}^{N_b} \left(Y^{(L, n)} - y^{(n)} \right) \\
                    &= e \in \reals \label{eq:ch2:reg_b1}
\end{align}
where we have used the fact that for the regression case, $y^{(n)}, Y^{(L,n)}
\in \reals$. 

\subsubsection{Classification Loss}
For the classification case \eqref{eq:ch2:mle_class}, let us keep the output of
the network $Y^{(L,n)} \in \reals[C]$ and define an intermediate value $\hat{y}$ the
softmax applied to this vector $\hat{y}^{(n)}_c = \sigma_c\left(Y^{(L, n)}\right)$.
Note that the softmax is a vector valued function going from $\reals[C] \rightarrow
\reals[C]$ so has a jacobian matrix $S_{ij} = \dydx{\hat{y}_i}{Y^{(L)}_j}$ with
values:
\begin{equation}
  S_{ij} = \begin{cases}
    \sigma_i (1-\sigma_j) & \text{if $i=j$}\\
    -\sigma_i \sigma_j & \text{if $i\neq j$}\\
  \end{cases}
\end{equation}

Now, let us return to \eqref{eq:ch2:mle_class} and find the derivative of the
objective function to this intermediate value $\hat{y}$:
\begin{align}
  \dydx{J}{\hat{y}} &=  \dydx{}{\hat{y}} \left( \frac{1}{N}\sum_{n=1}^{N_b} \sum_{c=1}^C 
  y^{(n)}_c \log \hat{y}^{(n)}_c \right) \\
  &= \frac{1}{N}\sum_{n=1}^{N_b} \sum_{c=1}^C \frac{y^{(n)}_c}{\hat{y}^{(n)}_c} \\
  &= d \in \reals[C] \label{eq:ch2:class_b1}
\end{align}
Note that unlike \eqref{eq:ch2:reg_b1}, this derivative is vector valued. To
find $\dydx{J}{Y^{(L)}}$ we use the chain rule. It is easier to find the
partial derivative with respect to one node in the output first, and then expand
from here. I.e.:

\begin{align}
  \dydx{J}{Y^{(L)}_j} &= \sum_{i=1}^C \dydx{J}{\hat{y}_i}\dydx{\hat{y}_i}{Y^{(L)}_j} \\
                      &= S_j^T d
\end{align}
where $S_j$ is the $j$th column of the jacobian matrix $S$. It becomes clear now
that to get the entire vector derivative for all nodes in $Y^{(L)}$, we must
multiply the transpose of the jacobian matrix with the error term from
\eqref{eq:ch2:class_b1}:
\begin{equation}
  \dydx{J}{Y^{(L)}} = S^T d
\end{equation}

\subsubsection{Final Layer Weight Gradient} \label{sec:ch2:weight}
Let us continue by assuming $\dydx{J}{Y^{(L)}}$ is vector valued as was the case
with classification. For regression, it is easy to set $C=1$ in the following to
get the necessary results. Additionally for clarity, we will drop the layer
superscript in the intermediate calculations.

We call the gradient for the final layer weights the \emph{update} gradient. 
It can be computed by the chain rule again:
\begin{align}
  \dydx{J}{W_{ij}} &= \dydx{J}{Y_i} \dydx{Y_i}{W_{ij}} + 2\lambda W_{ij} \\
                   &= \dydx{J}{Y_i} X_j + 2\lambda W_{ij}
\end{align}
where the second term in the above two equations comes from the regularization
loss that is added to the objective. The gradient of the entire weight matrix is 
then:
\begin{align}
  \dydx{J}{W^{(L)}} &= \dydx{J}{\hat{y}} X^T +2\lambda W \\
                    &= S^T d \left(X^{(L-1)}\right)^T + 2\lambda W^{(L)} \in \reals[C \x C_{L-1}]
\end{align}

\subsubsection{Final Layer Passthrough Gradient} \label{sec:ch2:passthrough}
Additionally, we want to find the \emph{passthrough} gradients of the final
layer $\dydx{J}{X^{(L-1)}}$. In a similar fashion, we first find the gradient
with respect to individual elements in $X^{(L-1)}$ before generalizing to the
entire vector:
\begin{align}
  \dydx{J}{X_i} &= \sum_{j=1}^{C} \dydx{J}{Y_j} \dydx{Y_j}{X_i} \\
                &= \sum_{j=1}^{C} \dydx{J}{Y_j} W_{j,i} \\
                &= W_{i}^T\dydx{J}{Y} \\
\end{align}
where $W_i$ is the $i$th column of $W$. Thus
\begin{align}
  \dydx{J}{X^{(L-1)}} &= \left(W^{(L)}\right)^T \dydx{J}{Y^{(L)}} \\
                      &= \left( W^{(L)} \right)^T S^T d
\end{align}
This passthrough gradient then can be used to update the next layer's weights by
repeating \autoref{sec:ch2:weight} and \autoref{sec:ch2:passthrough}.

\subsubsection{General Layer Update}
The easiest way to handle this flow of gradients, and the basis for most
automatic differentiation packages, is the block definition shown in
\autoref{fig:ch2:block_form}. For all neural network components (even if they do
not have weights), the operation must not only be able to calculate the forward 
pass $y=f(x, w)$ given weights $w$ and inputs $x$, but also calculate the
\emph{update} and \emph{passthrough} gradients $\dydx{J}{w}, \dydx{J}{x}$ given
an input gradient $\dydx{J}{y}$. The input gradient will have the same shape as
$y$ as will the update and passthrough gradients match the shape of $w$ and $x$.
This way, gradients for the entire network cam be computed in an iterative
fashion starting at the loss function and moving backwards.

\begin{figure}
  \centering
  % \input{\imgpath/deeplearn_block}
  \includegraphics{\imgpath/deeplearn_block.pdf}
  \mycaption{General block form for autograd}{All neural network functions
  need to be able to calculate the forward pass $y=f(x,w)$ as well as the 
  update and passthrough gradients $\dydx{J}{w}, \dydx{J}{x}$. Backpropagation
  is then easily done by allowing data to flow backwards through these blocks
  from the loss.}
  \label{fig:ch2:block_form}
\end{figure}

\section{Convolutional Neural Networks}
Convolutional Neural Networks (CNNs) are a special type of of Neural Network where the
weights of the fully connected layer are shared across the layer. 
In this way, a neuron at a given layer is only affected by
nodes from the previous layer in a given neighbourhood rather than every node.

First popularized in 1998 by LeCun et. al in \cite{lecun_gradient-based_1998},
the convolutional layer was introduced to build invariance with respect to
translations, as well as reduce the parameter size of early neural networks for
pattern recognition. The idea of having a locally receptive field had already
been shown to be a naturally occurring phenomen by Hubel and Wiesel
\cite{hubel_receptive_1962}. They did not become popular immediately, and
another spatially based keypoint extractor, SIFT \cite{lowe_distinctive_2004},
was the mainstay of detection systems until the AlexNet CNN
\cite{krizhevsky_imagenet_2012} won the 2012 ImageNet challenge
\cite{russakovsky_imagenet_2014} by a large margin over the next competitors who
used SIFT and Support Vector Machines \cite{cortes_support-vector_1995}. This
CNN had 5 convolutional layers followed by 3 fully connected layers.

We will briefly describe the convolutional layer, as well as many other layers
that have become popular in the past few years.

\subsection{Convolutional Layers}
In the presentation of neural networks so far, we have considered column vectors 
$X^{(l)},Y^{(l)} \in \reals[C_{l}]$. Convolutional layers for image analysis
have a different format. In particular, the spatial component of the input is
preserved. 

Let us first consider the definition of 2-D convolution for single channel
images:
\begin{align}
  y[\nn] = (x\conv h)[\nn] &= \sum_{\mathbf{k}} x[\mathbf{k}]h[\nn-\mathbf{k}] \label{eq:ch2:conv1}\\
                           &= \sum_{k_1, k_2} x[k_1,k_2]h[n_1-k_1, n_2-k_2]
\end{align}
where the sum is done over the support of $h$. For an input $x\in \reals[H\x W]$
and filter $h\in \reals[K_1\x K_2]$ the output has spatial support $y\in
\reals[H+K_1-1 \x W+K_2-1]$. 

In the context of convolutional layers, this filter $h$ is a \emph{matched filter} 
that gives its largest output when the input contains $h$. If the input has
shapes similar to $h$ in many locations, each of these locations in $y$ will
also have large outputs. 

It is not enough to only have a single matched filter, often we would like to
have a bank of them, each sensitive to different shapes. For example, if $h_1$
was sensitive to horizontal edges, we may also want to detect vertical and
diagonal edges. Without specifying what each of the filters do, we can 
specify that we would like to detect $C$ different shapes over the spatial
extent of an input.

This then means that we have $C$ output channels:
\begin{align*}
  y_1[\nn] &= (x \conv h_1)[\nn] \\
  y_2[\nn] &= (x \conv h_2)[\nn] \\
           &\vdots & \\
  y_C[\nn] &= (x \conv h_C)[\nn] 
\end{align*}

If we stack red, green and blue input channels on top of each other\footnote{In deep 
learning literature, there is not consensus about whether to stack the outputs
with the channel first ($\reals[C\x H\x W]$) or last ($\reals[H\x W\x C]$). The
latter is more common in Image Processing for colour and spectral images,
however the former is the standard for the deep learning framework we use --
PyTorch \cite{paszke_automatic_2017}, so we use this in this thesis.} , we have a 
3-dimensional array $x \in \reals[C\x H\x W]$ with $C=3$.
In a CNN layer, each filter $h$ is 3 dimensional with spatial extent exactly
equal to $C$. The \emph{convolution} is done over the remaining two dimensions
and the $C$ outputs are summed at each pixel location. This makes
\eqref{eq:ch2:conv1}:
\begin{equation}
  y[\nn] = \sum_{c=1}^C \sum_{\mathbf{k}} x[\mathbf{k}]h[c, \nn-\mathbf{k}]
  \label{eq:ch2:conv2}
\end{equation}

Again, we would like to have many matched filters to find different shapes in
the previous layer, so we repeat \autoref{eq:ch2:conv2} $F$ times and stack the
output to give $y \in \reals[F \x H\x W]$: 
%
\begin{equation}
  y[f, \nn] = \sum_{c=1}^C \sum_{\mathbf{k}} x[c, \mathbf{k}]h_f[c, \nn-\mathbf{k}]
  \label{eq:ch2:conv3}
\end{equation}

After a convolutional layer, we can then apply a pointwise nonlinearity $g$ to
each output location in $y$. Revisiting \eqref{eq:ch2:fc1} and \eqref{eq:ch2:fc2}, we can 
rewrite this for a convolutional layer at depth $l$ with $C_l$ input and
$C_{l+1}$ output channels:
\begin{align}
  Y^{(l+1)}[f, \nn] &= \sum_{c=1}^{C_l} X^{(l)}[c, \nn] \conv h^{(l)}_f[c, \nn] \qquad 
    \text{for } 1 \leq f \leq C_{l+1} \label{eq:ch2:conv4}\\
  X^{(l+1)}[f, \nn] &= g\left(Y^{(l)}[f, \nn]\right)
\end{align}
This is shown in \autoref{fig:ch2:conv_layer}.

\begin{figure}
  \centering
  % \input{\imgpath/conv}
  \includegraphics[width=\textwidth]{\imgpath/conv.pdf}
  \mycaption{A convolutional layer}{A convolutional layer followed by a
  nonlinearity $g$. The previous layer's activations are convolved with a bank
  of $C_{l+1}$ filters, each of which has spatial size $k_h\x k_w$ and depth
  $C_l$. Note that there is no convolution across the channel dimension. Each
  filter produces one output channel in $y^{(l+1)}$.}
  \label{fig:ch2:conv_layer}
\end{figure}

\subsubsection{Padding and Stride}
Regular 2-D convolution expands the input from size $H\x W$ to $(H+K_H-1)\x
(W+K_W-1)$. In convolutional layers in neural networks, it is often desirable
and common to have the same output size as input size. This is achieved by
taking the central $H\x W$ outputs. We call this \emph{same-size convolution}.
Another option commonly used is to only evaluate the kernels where they fully
overlap the input signal, causing a reduction in the output size to $(H-K_H+1)
\x (W-K_W+1)$. This is called \emph{valid convolution} and was used in the
original LeNet-5 \cite{lecun_gradient-based_1998}.

Signal extension is by default \emph{zero padding}, and most deep learning
frameworks have no ability to choose other padding schemes as part of their
convolution functions. Other padding such as \emph{symmetric padding} can be
achieved by expanding the input signal before doing a valid convolution.

Stride is a commonly used term in deep learning literature. A stride of 2 means
that we evaluate the filter kernel at every other sample point. In signal
processing, this is simply called decimation.

\subsubsection{Gradients}
To get the update and passthrough gradients for the convolutional layer we will need to expand
\eqref{eq:ch2:conv4}. Again we will drop the layer superscripts for clarity: 
\begin{equation}
  % Y[f, n_1, n_2] &=& \sum_{c=1}^C \sum_{k_1=a}^{b} \sum_{k_2=a}^b x[c, k_1, k_2]
  % h_f[c, n_1-k_1, n_2-k_2] \label{eq:ch2:conv5}
  Y[f, n_1, n_2] = \sum_{c=1}^C \sum_{k_1} \sum_{k_2} x[c, k_1, k_2]
  h_f[c, n_1-k_1, n_2-k_2] \label{eq:ch2:conv5}
\end{equation}
It is clear from this that a single activation $X[c, n_1, n_2]$ affects
many output values. In particular, the derivative for an activation in $Y$
w.r.t.\ an activation in $X$ is the sum of all the chain rule applied to all
these positions:
\begin{equation}
  \dydx{Y_{f, n_1, n_2}}{X_{c, k_1, k_2}} = h_f[c, n_1 - k_1, n_2 - k_2]
\end{equation}
and the derivative from the loss is then:
\begin{align}
  \dydx{J}{X_{c,k_1,k_2}} &= \sum_f \sum_{n_1} \sum_{n_2} \dydx{J}{Y_{f, n_1, n_2}}
  \dydx{Y_{f, n_1, n_2}}{X_{c,k_1,k_2}} \\
  &= \sum_f \sum_{n_1} \sum_{n_2} \dydx{J}{Y_{f, n_1, n_2}} h_f[c, n_1 - k_1, n_2 - k_2] \label{eq:ch2:conv6}
\end{align}
Now we let $\Delta Y[f, n_1, n_2] = \dydx{J}{Y_{f, n_1, n_2}}$ be the passthrough gradient
signal from the next layer, and $\tilde{h}_\alpha[\beta, \gamma, \delta] = h_\beta[\alpha, -\gamma, -\delta]$
be a set of filters that have been mirror imaged in the spatial domain and had
their channel and filter number ordering swapped. Combining these two and
subbing into \eqref{eq:ch2:conv6} we get the \emph{passthrough gradient} for the
convolutional layer:
\begin{align}
  \dydx{J}{X_{c,k_1,k_2}} &= \sum_f \sum_{n_1} \sum_{n_2} \Delta Y[f, n_1, n_2] \tilde{h}_c[f, k_1-n_1, k_2-n_2 ] \\
                          &= \sum_f \Delta Y[f, \nn] \conv \tilde{h}_c[f, \nn]
\end{align}
which is the same as \eqref{eq:ch2:conv4}. I.e. we can backpropagate the
gradients through a convolutional block by mirror imaging the filters,
transposing them in the channel and filter dimensions, and doing a forward
convolutional layer with $\tilde{h}$ applied to $\Delta Y$. Similarly, we 
find the \emph{update gradients} to be:
\begin{align}
  \dydx{J}{h_{f,c,k_1,k_2}} &= \sum_{n_1} \sum_{n_2} \dydx{J}{Y_{f, n_1, n_2}} \dydx{Y_{f, n_1, n_2}}{h_{f, c, k_1, k_2}} \\
                            &= \sum_{n_1} \sum_{n_2} \Delta Y[f, n_1, n_2] X[x, n_1-k_1, n_2-k_2 ] \\
                            &= \left(\Delta Y[f, \nn] \star X[c, \nn]\right)[k_1, k_2]
\end{align}
where $\star$ is the cross-correlation operation. 


\subsection{Pooling}
Pooling layers are common in CNNs where we want to reduce the spatial size. As
we go deeper into a CNN, it is common for the spatial size of the activation to
decrease, and the channel dimension to increase. The $C_l$ values at a given
spatial location can then be thought of as a feature vector describing the
presence of shapes in a given area in the input image.

Pooling is useful to add some invariance to smaller shifts when downsampling. It
is often done over small spatial sizes, such as $2\x 2$ or $3\x 3$. Invariance
to larger shifts can be built up with multiple pooling (and convolutional) layers.

Two of the most common pooling techniques are \emph{max pooling} and
\emph{average pooling}. Max pooling takes the largest value in its spatial area,
whereas average pooling takes the mean. A visual explanation is shown in 
\autoref{fig:ch2:maxpool}. Note that pooling is typically a spatial operation,
and only in rare cases is done over the channel dimension.

A review of pooling methods in \cite{mishkin_systematic_2016} found them both
to perform equally well. While max pooling was the most popular in earlier state
of the art networks \cite{krizhevsky_imagenet_2012, simonyan_very_2014},
there has been a recent trend towards using average pooling \cite{huang_densely_2017}
or even to do away with pooling altogether in favour of strided convolutions
(this idea was originally proposed in \cite{springenberg_striving_2014-3} and
used notably in \cite{he_deep_2016, xie_aggregated_2017,
zagoruyko_wide_2016-1}).
  
\begin{figure}
  \centering
  % \input{\imgpath/pooling_both}
  \includegraphics{\imgpath/pooling_both.pdf}
  \mycaption{Max vs Average $2\x 2$ pooling}{}
  \label{fig:ch2:maxpool}
\end{figure}

\subsection{Dropout}
Dropout is a particularly harsh regularization scheme that randomly turns off
or zeros out neurons in a neural network \cite{hinton_improving_2012, srivastava_dropout:_2014}.
Each neuron has probability $p$ of having its value set to 0 during training
time, forcing the network to be more general and preventing `co-adaption' of
neurons. The main explanation given in \cite{srivastava_dropout:_2014} is
that dropout averages over several `thinner' models.

During test time, dropout is typically turned off, but can still be
used to get an estimate on the uncertainty of the network by averaging over
several runs \cite{gal_dropout_2016}.

\subsection{Batch Normalization}
Batch normalization proposed in \cite{ioffe_batch_2015} is a conceptually 
simple technique. Despite this, it has become very popular and has been found
to be very useful to train \emph{deeper} CNNs. 

Batch Normalization rescales CNN activations by channel. Define the mean
and standard deviations for a channel across the entire dataset as:
\begin{align}
  \mu_c &= \frac{1}{N} \sum_\nn X^{(n)}[c, \nn] \\
  \sigma_c^2 &=  \frac{1}{N} \sum_\nn \left(X^{(n)}[c, \nn]\right)^2 - \mu^2_c 
\end{align}
where $\mu, \sigma \in \reals[C]$.
Batch norm removes the natural mean and variance of the data, scales the data
by a learnable gain $\gamma$, and offsets it to a learnable mean $\beta$,
with $\gamma, \beta \in \reals[C]$:
\begin{equation}
  Y[c, \nn] = \frac{X[c, \nn] - \mu_c}{\sigma_c + \epsilon}\gamma_c + \beta_c
\end{equation}
where $\epsilon$ is a small value to avoid dividing by 0.

Of course, during training, we do not have access to the dataset $\mu, \sigma$
and these values must be estimated from the batch statistics. A typical practice
is to keep an exponential moving average estimate of these values to give us
$\tilde{\mu}, \tilde{\sigma}$. 

The passthrough and update gradients are:
\begin{align}
  \dydx{J}{X_{c, n_1, n_2}} &= \dydx{J}{Y_{c, n_1, n_2}}\frac{\gamma}{\sigma + \epsilon}\\
  \dydx{J}{\beta_c} &=  \sum_\nn \dydx{J}{Y_{c, \nn}}\\
  \dydx{J}{\gamma_c} &= \sum_\nn \dydx{J}{Y_{c, \nn}}\frac{X_{c, \nn} - \mu_c}{\gamma_c + \epsilon}
\end{align}

Batch normalization layers are typically placed \emph{between} convolutional layers
and non-linearities. I.e. consider the input $X=WU$ for some linear operation
on the previous layer's activations $U$ with weights $W$.

We see that it has the particular benefit of removing
the sensitivity of our network initial weight scale, as on the forward pass
$BN(aWU) = BN(WU)$.  
It is also particularly useful for backpropagation, as an increase in
weights leads to \emph{smaller} gradients \cite{ioffe_batch_2015}, making
the network far more resilient to the problems of vanishing and exploding
gradients:
\begin{align}
  \frac{\partial BN((aW)U)}{\partial U} & =  \frac{\partial
  BN(WU)}{\partial U} \nonumber\\
  \frac{\partial BN((aW)U)}{\partial (aW)} & =  \frac{1}{a} \cdot \frac{\partial
  BN(WU)}{\partial W} 
\end{align}

\section{Relevant Architectures}
In this section we briefly review some relevant CNN architectures that will be
helpful to refer back to in this thesis. 

\subsection{Datasets}
When doing image analysis tasks, it is important to know comparatively how well
different networks perform on the same challenege. To achieve this, the
community has developed several datasets that are commonly used to report
metrics. For image classification, there are five such datasets, listed here in
increasing order of difficulty:
\begin{enumerate}
  \item \textbf{MNIST}: 10 classes, 6000 images per class, $28\x 28$ pixels per image. 
    The images contain the digits 0--9 in greyscale on a blank background. The
    digits have been size normalized and centred. Dataset description and files can be obtained 
    at \cite{lecun_modified_1998}.
  \item \textbf{CIFAR-10}: 10 classes, 5000 images per class, $32\x 32$ pixels per image.
    The images contain classes of everyday objects like cars, dogs, planes etc.
    The images are colour and have little clutter or background. Dataset
    description can be found in \cite{krizhevsky_learning_2009} and files at
    \cite{krizhevsky_cifar_2009}.
  \item \textbf{CIFAR-100}: 100 classes, 500 images per class, $32\x 32$ pixels per image. 
    Similar to CIFAR-10, but now with fewer images per class and ten times as
    many classes. Dataset description can be found in
    \cite{krizhevsky_learning_2009} and files at \cite{krizhevsky_cifar_2009}.
  \item \textbf{Tiny ImageNet}: 200 classes, 500 images per class, 
    $64 \x 64$ pixels per image. A more recently introduced dataset that bridges
    the gap between CIFAR and ImageNet. Images are larger than CIFAR and there
    are more categories. Dataset description and files can be obtained at \cite{li_tiny_2017}.
  \item \textbf{ImageNet CLS}: There are multiple types of challenges in ImageNet, but CLS
    is the classification challenge, and is most commonly reported in papers. 
    It has 1000 classes of objects with a varying amount of images per class.
    Most classes have 1300 examples in the training set, but a few have less
    than 1000. The images have variable size, typically a couple of hundred
    pixels wide and a couple of hundred pixels high. The images can have varying
    amounts of clutter and can be at different scales, making it a particularly
    difficult challenge. Dataset description is in
    \cite{russakovsky_imagenet_2014} and the most reliable source of the data
    can be found at \cite{stanford_vision_lab_imagenet_2017}.
\end{enumerate}
Several other classification datasets do exist but are not commonly used, such
as PASCAL VOC \cite{Everingham15} and Caltech-101 and Caltech-256
\cite{li_fei-fei_learning_2004}\footnote{Tiny ImageNet is also not commonly 
used as it is quite new. We have included it the main list as we have found it 
to be quite a useful step up from CIFAR without requiring the weeks to train
experimental configurations on ImageNet.}.

\subsection{LeNet}
LeNet-5 \cite{lecun_gradient-based_1998} is a good network to start with: it
is simple yet contains many of the layers used in modern CNNs. Shown in
\autoref{fig:ch2:lenet} it has two convolutional and three fully connected
layers. The outputs of the convolutional layers are passed through a sigmoid
nonlinearity and downsampled with average pooling. The first two fully-connected
layers also have sigmoid nonlinearities. The loss function used is a combination
of tanh fucntions and MSE loss.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{\imgpath/cnns.png}
  \mycaption{LeNet-5 architecture}{The `original' CNN architecture used for
  handwriting recognition. LeNet has 2 convolutional and 3 fully connected
  layers making 5 parameterized layers. After the second convolutional layer,
  the $16\x 5\x 5$ pixel output is unravelled to a $400$ long vector. Image
  taken from \cite{lecun_gradient-based_1998}.}
  \label{fig:ch2:lenet}
\end{figure}

\subsection{AlexNet}
We have already seen AlexNet \cite{krizhevsky_imagenet_2012} in \autoref{ch:intro}. 
It is arguably one of the most important architectures in the development in CNNs as it 
was able to experimentally prove that CNNs can be used for complex tasks. This
required many innovations. In particular, they used multiple GPUs to do fast
processing on large images, used the ReLU to avoid saturation, and added dropout
to aid generalization. Training of AlexNet on 2 GPUs available in 2012 takes
roughly a week.

The first layer uses convolutions with a spatial support of $11\x11$, followed
by $5\x 5$ and $3\x 3$ for the final three layers.

% \begin{figure}
  % \centering
  % \includegraphics[width=\textwidth]{\imgpath/alexnet}
  % \mycaption{The AlexNet architecture}{Designed for the ImageNet challenge, 
  % AlexNet may look like \autoref{fig:ch2:lenet} but is
  % much larger. Composed of 5 convolutional layers and 3 fully connected layers.
  % Figure taken from \cite{krizhevsky_imagenet_2012}.}
  % \label{fig:ch2:alexnet}
% \end{figure}

\subsection{VGGnet}
The Visual Geometry Group (VGG) at Oxford came second in the ILSVRC challenge in
2014 with their VGG-nets \cite{simonyan_very_2014}, but remains an important
network for some of the design choices it inspired. In particular, their optimal
network was much deeper than AlexNet, with 19 convolutional layers on top of
each other before 3 fully connected layers. These convolutional layers all used
the smaller $3\x 3$ seen only at the back of AlexNet.

This network is particularly attractive due its simplicity, compared to the 
more complex Inception Network \cite{szegedy_going_2015} which won the 2014
ILSVRC challenge. VGG-16, the 16 layer variant of VGG stacks two or three
convolutional layers (and ReLUs) on top of each other before reducing spatial
size with max pooling. After processing at five scales, the resulting $512 \x 14
\x 14$ activation is unravelled and passed through a fully connected layer.

These VGG networks also marked the start of a trend that has since become
common, where channel depth is doubled after pooling layers. The doubling of
channels and quartering the spatial size still causes a net reduction in the
number of activations.

\subsection{The All Convolutional Network}
The All Convolutional Network \cite{springenberg_striving_2014-3} introduced two
popular modifications to the VGG networks:
%
\begin{itemize}
  \item They argued for the removal of max pooling layers, saying that a $3\x 3$
    convolutional layer with stride 2 works just as well.
  \item They removed the fully connected layers at the end of the network,
    replacing them with $1\x 1$ convolutions. Note that a $1\x 1$ convolution
    still has shared weights across all spatial locations. The output layer then
    has size $C_L \x H \x W$, where $H, W$ are many times smaller than the input
    image size, and the vector of $C_L$ coefficients at each spatial location
    can be interpreted as a vector of scores marking the presence/absence of
    $C_L$ different shapes. For classification, the output can be averaged over
    all spatial locations, whereas for localization it may be useful to keep
    this spatial information.
\end{itemize}
The new network was able to achieve state of the art results on CIFAR-10 and
CIFAR-100 and competetive performance on ImageNet, while only use a fraction of
the parameters of other networks.

\subsection{Residual Networks}\label{sec:ch2:resnets}
  \begin{figure}[tb]
    \centering
    \includegraphics[width=0.5\textwidth]{\imgpath/residual_unit.png}
    \mycaption{The residual unit from ResNet}
          {A residual unit. The identity mapping is always present, and the
            network learns the difference from the identity mapping, $\mathcal{F}(x)$.
            Taken from \cite{he_deep_2016}.}
      \label{fig:ch2:residual_unit}
  \end{figure}
  Resiudal Networks or ResNets won the 2015 ILSVRC challenge, introducing the
  residual layer. Most state of the art models today use this residual mapping
  in some way \cite{zagoruyko_wide_2016-1, xie_aggregated_2017}.

  The inspiration for the residual layer came from the difficulties
  experienced in training deeper networks.  Often, adding an extra layer would
  \emph{decrease} network performance. This is counter-intuitive as the deeper
  layers could simply learn the identity mapping, and achieve the same
  performance.

  To promote the chance of learning the identity mapping, they define a residual
  unit, shown in \autoref{fig:ch2:residual_unit}. If a desired mapping is
  denoted $\mathcal{H}(x)$, instead of trying to learn this, they instead learn
  $\mathcal{F}(x) = \mathcal{H}(x) - x$. Doing this promotes a strong diagonal
  in the Jacobian matrix which improve conditioning for gradient descent. 
  
  Recent analysis of a ResNet without nonlinearities
  \cite{bartlett_representing_2018, bartlett_gradient_2018} proves that SGD
  fails to converge for deep networks when the network mapping is far away from
  the identity, suggesting that a residual mapping is a good thing to do.
