Despite the success of CNNs, deep nets are often criticized for being `black box'
methods. Once you train a CNN, you can view the first layer of filters
quite easily (see \autoref{fig:ch1:alex_filt}) as they exist in RGB
space. Beyond that things get trickier, as the filters have a third, \emph{channel}
dimension, typically much larger than its two spatial dimensions.
Representing these dimensions with different colours becomes tricky and
uninformative, so we must do something else.

A recent paper titled `Why Should I Trust You?' \cite{ribeiro_why_2016}
explored the consequences of interpretability on human trust for machine learning models.
Unsurprisingly, a model with an interpretable methodology
was trusted more than those which did not have one, even if it had a lower
prediction accuracy on the test set.
To build trust and to aid training, we need to probe these networks and
visualize how and why they are making decisions.

Some good work has been done in this area. In particular,
Zeiler and Fergus \cite{zeiler_visualizing_2014}
design a DeConvNet to visualize what input patterns a filter in a given layer is mostly highly
activated by. In \cite{mahendran_understanding_2015},
\citeauthor{mahendran_understanding_2015} learn to invert
representations by updating a noisy input via GD until its latent feature vector
matches a desired target. \citeauthor{simonyan_deep_2014}
\cite{simonyan_deep_2014} develop saliency maps by projecting gradients back to
the input space and measuring where they have largest magnitude.

We introduced ScatterNets in \autoref{sec:ch2:scatternets} and looked at making
them faster in \autoref{ch:dtcwt_scat}.
They have been one of the main successes in applying wavelets to deep learning
systems, and are particularly inspiring due to their well-defined properties.
They are typically used as unsupervised feature extractors
\cite{bruna_invariant_2013, oyallon_deep_2015,
singh_dual-tree_2017, singh_multi-resolution_2016} and
can outperform CNNs for classification tasks with reduced
training set sizes, e.g.\ in CIFAR-10 and CIFAR-100 (Table 6 from
\cite{oyallon_scaling_2017} and Table 4 from \cite{singh_dual-tree_2017}).
They are also near state-of-the-art for Texture Discrimination tasks
(Tables 1--3 from \cite{sifre_rotation_2013}). Despite this, there still exists
a considerable gap between Scatternets and CNNs on challenges like CIFAR-10 with the
full training set ($83\%$ vs.\ $>90\%$). Even considering the benefits of
ScatterNets, this gap must be addressed.

While ScatterNets have good theoretical foundations and properties
\cite{mallat_group_2012}, it is difficult to understand the second-order
scattering. In particular, how useful are these coefficients for
training and how similar are the scattered features to a modern state of the art
convolutional network? To answer these questions, this chapter interrogates
ScatterNet frontends. Taking inspiration from the work done for
CNNs, we build a DeScatterNet to visualize what the second-order features are.
We also heuristically probe a trained hybrid network (ScatterNet front end + CNN
backend) and quantify the importance of the individual features.

\section{Chapter Layout}
We first redefine the operations that form a ScatterNet in
\autoref{sec:ch4:scatternet} before introducing our DeScatterNet in
\autoref{sec:ch4:descatternet}, and show how we can use it to examine the
layers of ScatterNets (using a similar technique to the CNN visualization in
\cite{zeiler_visualizing_2014}). We use this analysis tool to highlight what
patterns a ScatterNet is sensitive to (\autoref{sec:ch4:visualization}), showing
that they are very different from what their CNN counterparts are sensitive to,
and possibly less useful for discriminative tasks.

We then measure the ScatterNet channel saliency by performing occlusion tests
on a trained hybrid network ScatterNet-CNN, iteratively switching off individual
Scattering channels and measuring the effect this has on the validation accuracy
in \autoref{sec:ch4:occlusion}. The results from the occlusion tests strengthen
the idea that some of the ScatterNet patterns may not be well suited for deep
learning systems.

We use these observations to propose an architectural change to ScatterNets,
which have not changed much since their inception in \cite{mallat_group_2012},
and show that it is possible to get visually more appealing shapes by filtering
across the orientations of the ScatterNet. We present this in
\autoref{sec:ch4:corners}.
