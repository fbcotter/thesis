\chapter{Architecture Used for Experiments} \label{app:arch}
\def \path {dtcwt_scat}
\def \imgpath {\path/images}

The experiments for this thesis were run on a single server with 8 GPUs and 14
CPUs. The GPUs were each NVIDIA GeForce GTX 1080 cards released in May 2016.
They each have 8GiB of RAM, 2560 CUDA cores and 320 GB/s memory bandwidth.
The CPUs were Intel(R) Xeon(R) E5-2660 models. 

At the completion of the project, we were running CUDA 10.0 with cuDNN 7.6 and 
PyTorch version 1.1.

To do hyperparameter search we used the Tune package \cite{liaw2018tune} which
we highly recommend, as it makes running trials in parallel very easy.

\section{Run Times of some of the Proposed Layers}

\begin{table}[bt]
  \renewcommand{\arraystretch}{1.2}
  \centering
  \mycaption{Run time speeds for layers in thesis}{}
  % \begin{tabular}{@{}lllcllcll@{}}
  \begin{tabular}{@{}l ccccc@{}}
    \toprule
    & 16 & 32 & 64 & 128 & 256 \\
    \midrule
    conv & 0.2 & 0.8 & 6.2 & 22.4 & 112 \\
    $\DTCWT$ & 0.5 & 2.16 & 7.6 & 29.4 & 118 \\
    $\DTCWT^{-1}$ & 0.6 & 2.1 & 8.1 & 33.3 & 123\\
    Scatter & 0.6 & 2.1 & 8.7 & 31.8 & 125 \\
    Invariant & 0.7 & 2.4 & 9.6 & 37.4 & 144 \\
    Gain ($J=1$) & 1.5 & 5.7 & 21.6 & 80 &  336 \\
    Gain ($J=2$) & 1.7 & 4.7 & 16.7 & 61.6 & 236 \\
    \bottomrule
  \end{tabular}
\end{table}
\begin{table}[t]
  \renewcommand{\arraystretch}{1.2}
  \centering
  \mycaption{Run time speeds for layers in thesis}{}
  % \begin{tabular}{@{}lllcllcll@{}}
  \begin{tabular}{@{}l ccccc@{}}
    \toprule
    & 3 & 10 & 32 & 64 & 128 \\
    \midrule
    conv & 2 & 4.6 & 15.8 & 28.4 & 70 \\
    $\DTCWT$ & 3.2 & 10.5 & 30.0 & 58.6 & 126 \\
    $\DTCWT^{-1}$ & 4.1 & 13.3 & 37.0 & 79.4 & 152 \\
    Scatter & 3.4 & 11.0 & 31.4 & 65.8 & 133 \\
    Invariant & 3.6 & 11.8 & 34.6 & 73.6 & 164 \\
    Gain ($J=1$) & 9.7 & 28.4 & 79.4 & 158 & 371 \\
    Gain ($J=2$) & 7.4 & 20.4 & 64.4 & 121 & 245 \\
    \bottomrule
  \end{tabular}
\end{table}
