In this chapter we move away from the ScatterNet ideas from the previous 
chapters and instead look at using the wavelet domain as a new space in which to
learn. With ScatterNets, complex wavelets are used to scatter the energy into
different channels (corresponding to the different wavelet subbands), before the
complex modulus demodulates the signal to low frequencies. These channels can
then be mixed before scattering again (as we saw in the learnable scatternet),
but the progressive stages all result in a steady demodulation of signal energy
towards zero frequency. 

In this chapter we introduce the \emph{wavelet gain layer}
which starts in a similar fashion to the ScatterNet -- by taking the $\DTCWT$ of
a multi-channel input. Next, instead of taking a complex modulus, we learn a 
complex gain for each subband in each input channel. A single value here can 
amplify or attenuate all the energy in one part of the frequency plane. Then, 
while still in the wavelet domain, we mix the different input channels by subband (e.g.
all the $15\degs$ wavelet coefficients at a given scale are mixed together, but
the $75\degs$ and $45\degs$ coefficients are not). We can then return to the
pixel domain with the inverse wavelet transform. The shift-invariant properties
of the $\DTCWT$ allows gains and phases to be changed at will without
introducing sampling artifacts.

We also briefly explore the possibility of doing nonlinearities in the wavelet
domain. The goal being to ultimately connect multiple wavelet gain layers
together with nonlinearities before returning to the pixel domain. 

The proposed wavelet gain layer can be used in conjunction with regular
convolutional layers, with a network moving into the wavelet or pixel space and
learning filters in one that would be difficult to learn in the other.

Our experiments so far have shown some promise. We are able to learn complex
wavelet gains and have found that the ReLU works well as a wavelet nonlinearity.
However, replacing a convolutional layer with a gain layer degrades performance
by a small amount.

% Additionally, we have not been successful in finding a nonlinearity that works
% well in the wavelet domain. 

\section{Related Work}\label{sec:ch6:related} 
Fujieda et.\ al.\ use a DWT in combination with a
CNN to do texture classification and image annotation 
\cite{fujieda_wavelet_2017, fujieda_wavelet_2018}. In particular, they take a
multiscale wavelet transform of the input image, combine the actviations at each
scale independently with learned weights, and feed these back into the network
where the activation resolution size matches the subband resolution. The
architecture block diagram is shown in \autoref{fig:ch6:fujieda}, taken from the
original paper.  This work found that their dubbed `Wavelet-CNN' could
outperform competetive non wavelet based CNNs on both texture classification and
image annotation.

\begin{figure}[bt]
  \centering
  \includegraphics[width=\textwidth]{\imgpath/wavelet_CNN_3.pdf}
  \mycaption{Architecture using the DWT as a frontend to a CNN}{Figure 1 from
  \cite{fujieda_wavelet_2018}. Fujieda et.\ al.\ take a multiscale wavelet
  decomposition of the input before passing the input through a standard
  CNN\@. They learn convolutional layers independently on each subband and feed
  these back into the network at different depths, where the resolution of the
  subband and the network activations match.}
  \label{fig:ch6:fujieda}
\end{figure}

Several works also use wavelets in deep neural networks for super-resolution
\cite{guo_deep_2017} and for adding detail back into dense pixel-wise
segmentation tasks \cite{ma_detailed_2018}. These typically save wavelet
coefficients and use them for the reconstruction phase.

In \cite{rippel_spectral_2015}, \citeauthor{rippel_spectral_2015}
parameterize filters in the DFT domain. Rather than having a pixel domain filter
$\vec{w} \in \reals[F\x C\x K\x K]$, they learn a set of fourier coefficients
$\hat{\vec{w}} \in \complexes[F\x C\x K \x \ceil{K/2}]$
(the reduced spatial size is a result of enforcing that the inverse DFT of their
filter to be real, so the parameterization is symmetric). On the forward pass of
the neural network, they take the inverse DFT of $\hat{\vec{w}}$ to obtain
$\vec{w}$ and then convolve this with the input $\vec{x}$ as a normal CNN
would do.\footnote{The convolution may be done by taking both the image and
filter back into the fourier space but this is typically decided by the
framework, which selects the optimal convolution strategy for the filter and
input size. Note that there is not necessarily a saving to be gained by
enforcing it to do convolution by product of FFTs, as the FFT size needed for
the filter will likely be larger than $K\x K$, which would require resampling
the coefficients}. 

Note that an important point should be laboured about reparameterizing filters
in either the wavelet or Fourier domains: any invertible linear
transform of the parameter space will not change the updates if a linear
optimization scheme is used (for example standard gradient descent, or SGD with momentum)\footnote{This fact 
is something that \citeauthor{rippel_spectral_2015} briefly allude to, but do not make clear}.
This is proved in \autoref{app:ch6:invertible}.  
We make this point clear as a natural extension to continue the work
in \cite{rippel_spectral_2015} would be to parameterize filters in the wavelet domain,
taking inverse transforms to the pixel domain and doing normal convolution. 

The work presented in this chapter does not learn wavelet coefficients 
for convolutional filters to be applied in the pixel domain, rather we learn wavelet filters 
\emph{and} do convolution in the wavelet domain too.

\section{Background and Notation}
We make use of the 2-D $Z$-transform to simplify our analysis:
%
\begin{equation}
  X(\zz) = \sum_{n_1}\sum_{n_2} x[n_1, n_2]z_1^{-n_1}z_2^{-n_2} =
  \sum_{\nn}x[\nn]\zz^{-\nn}
\end{equation}
%
As we are working with three dimensional arrays (two spatial and one channel) but are
only doing convolution in two, we introduce a slightly modified 2-D $Z$-transform
which includes the channel index, $c$:
%
\begin{equation}
  X(c, \zz) = \sum_{n_1}\sum_{n_2} x[c, n_1, n_2]z_1^{-n_1}z_2^{-n_2} =
  \sum_{\nn}x[c, \nn]\zz^{-\nn} \label{eq:ch6:ztransform}
\end{equation}
We then define the product of these new $Z$-transform signals to be the
channel-wise convolution. E.g. for the 3-D filter $h[c, \nn]$ with $Z$-transform
$H(c, \zz)$ and the the 3-D signal $x[c, \nn]$ with $Z$-transform $X(c, \zz)$,
let us call the product of the two $Z$-transforms:
\begin{equation}
  H(c, \zz)X(c, \zz) = \sum_{\nn}\left(\sum_{\bmu{k}}h[c, \nn-\bmu{k}]x[c, \bmu{k}]\right)\zz^{-\nn} \label{eq:ch6:zproduct}
\end{equation}
%
Recall from \autoref{sec:ch2:conv_layers} that a typical convolutional
layer in a standard CNN gets the next layer's output in a two-step process:
%
\begin{align} 
  \cnndlact{y}{l+1}{f}{\nn} &= \sum_{c=0}^{C_l - 1} \cnndlact{x}{l}{c}{\nn} \conv \cnndfilt{l}{f}{c}{\nn}
    \label{eq:ch6:conv}\\
    \cnndlact{x}{l+1}{f}{\nn} & =  \sigma \left( \cnndlact{y}{l+1}{f}{\nn} \right) \label{eq:ch6:nonlin}
\end{align}
%
If we define the nonlinearity $\sigma_z$ to be the action of $\sigma$ to each
$z$-coefficient in the polynomial $Y(c, \zz)$, then we can rewrite
\eqref{eq:ch6:conv} and \eqref{eq:ch6:nonlin} as:
%
\begin{align}
  \cnnlact{Y}{l+1}{f}{\zz} &= \sum_{c=0}^{C_l - 1} \cnnlact{X}{l}{c}{\zz} H_f^{(l)}(c, \zz) \\
  \cnnlact{X}{l+1}{f}{\zz} &= \sigma_z(\cnnlact{Y}{l+1}{f}{\zz})
\end{align}
%
\subsection{$\DTCWT$ Notation}
For this chapter, we will work with lots of $\DTCWT$ coefficients so we define
some slightly new notation here.

A $J$ scale 2-D $\DTCWT$ gives $6J+1$ coefficients, 6 sets of complex
bandpass coefficients for each scale (representing the oriented bands from 15 to 165
degrees) and 1 set of real lowpass ($lp$) coefficients. 
\begin{equation}
  \DTCWT_J(x) = \{ u_{lp}, u_{j,k} \}_{1\leq j\leq J, 1\leq k\leq 6}
  \label{eq:ch6:dtcwt_coeffs}
\end{equation}

Each of these coefficients then has size:
%
\begin{eqnarray}
  u_{lp} &\in & \reals[N\x C\x \frac{H}{2^{J-1}} \x \frac{W}{2^{J-1}}] \\
  u_{j,k} &\in & \complexes[N\x C\x \frac{H}{2^{J}}\x \frac{W}{2^{J}}]
\end{eqnarray}
%
Note that the lowpass coefficients are twice as large as in a fully decimated
transform, a feature of the redundancy of the $\DTCWT$ and the fact that the
lowpass coefficients are most conveniently represented as purely real values,
whereas the bandpass ones are most conveniently complex in \eqref{eq:ch6:dtcwt_coeffs}.

If we ever want to refer to all the subbands at a given scale, we will
drop the $k$ subscript and call them $u_j$. Likewise, $u$ refers to the whole
set of $\DTCWT$ coefficients.

\section{Learning in Multiple Spaces}\label{sec:ch6:learning}

\begin{figure}[t]
  \centering
  \subfloat[A regular convolutional layer]{
  \includegraphics[width=0.8\textwidth]{\imgpath/fwd_chaina.pdf}
  }\\
  \subfloat[Gains applied in the wavelet domain]{
  \includegraphics[width=0.8\textwidth]{\imgpath/fwd_chainb.pdf}
  }\\
  \subfloat[Gains and nonlinearities applied in the wavelet domain]{
  \includegraphics[width=0.8\textwidth]{\imgpath/fwd_chainc.pdf}
  }
  % \includegraphics[width=0.8\textwidth]{\imgpath/fwd_chain.jpg}
  \mycaption{Proposed new forward pass in the wavelet domain}{Two network 
  layers with some possible options for processing. Solid lines denote the
  evaluation path and dashed lines indicate relationships. In (a) we see a
  regular convolutional neural network. We have included the dashed lines to
  make clear what we are denoting as $u$ and $v$ with respect to their
  equivalents $x$ and $y$. In (b) we get to $y^{(2)}$ through a different path.
  First we take the wavelet transform of $x^{(1)}$ to give $u^{(1)}$, apply a
  wavelet gain layer $G^{(1)}$, and take the inverse wavelet transform
  to give $y^{(2)}$. The dotted line for $H^{(1)}$ indicates that this
  path is no longer present. Note that there may not be any possible
  $G^{(1)}$ to make $y^{(2)}$ from (b) equal $y^{(2)}$ from (a). In
  (c) we have stayed in the wavelet domain longer, and applied a wavelet
  nonlinearity $\sigma_w$ to give $u^{(2)}$. We then return to the pixel domain
  to give $x^{(2)}$ and continue on from there in the pixel domain.}
  \label{fig:ch6:fwd_chain}
\end{figure}
At the beginning of each layer $l$ of a neural network we have the activations
$x^{(l)}$. Naturally, all of these activations have their equivalent wavelet
coefficients $u^{(l)}$. 

From \eqref{eq:ch6:conv}, convolutional layers also have intermediate
activations $y^{(l)}$. Let us discern these from the $x$ coefficients and
modify \eqref{eq:ch6:dtcwt_coeffs} to say the $\DTCWT$ of $y^{(l)}$ gives
$v^{(l)}$.

We now propose the \emph{wavelet gain layer} $G$.
The name `gain layer' comes from the inspiration for this chapter's work, as it
appears often the first layer of a CNN could be achieved by simply setting gains
for different regions in the frequency space of an image. 

The gain layer $G$ can be used instead of a convolutional layer. 
It is designed to work on the wavelet coefficients of an activation $u$,
to give wavelet domain outputs $v$. 

This can be seen as breaking the convolutional path in
\autoref{fig:ch6:fwd_chain} and taking a new route to get the next layer's
coefficients. From here, we can return to the pixel domain by taking the
corresponding inverse wavelet transform $W^{-1}$. Alternatively, we
can stay in the wavelet domain and apply wavelet based nonlinearities,
$\sigma_{lp}$ and $\sigma_{bp}$ for the lowpass and highpass coefficients
respectively, to give $u^{(l+1)}$. 

Ultimately we would like to explore architecture design with arbitrary sections
in the wavelet and pixel domain, but to do this we must first explore: 
\begin{enumerate}[itemsep=5pt,parsep=5pt,topsep=0pt]
  \item \textbf{How effective is a wavelet gain layer $G$ at replacing a
    standard convolutional layer $H$?}
  \item \textbf{What are effective wavelet nonlinearities $\sigma_{lp}$ and $\sigma_{bp}$?}
\end{enumerate}

\subsection{The $\DTCWT$ Gain Layer}
To do the mixing across the $C_l$ channels at each subband, giving $C_{l+1}$
output channels, we introduce the learnable filters $g_{lp},\ g_{j,k}$:
%
\begin{align}
  g_{lp} &\in \reals[C_{l+1}\x C_l\x k_{lp}\x k_{lp}] \label{eq:ch6:glp} \\
  g_{1,1} &\in \complexes[C_{l+1}\x C_l\x k_1\x k_1] \\
  g_{1,2} &\in \complexes[C_{l+1}\x C_l\x k_1\x k_1] \\
      & \vdots \nonumber \\
  g_{J,6} &\in \complexes[C_{l+1}\x C_l\x k_J\x k_J]  \label{eq:ch6:gj6}
\end{align}
%
where the $k_j$ are the sizes of the mixing kernels. These could be $1\x 1$ for
simple gain control, or could be larger, say $3\x 3$, to do more complex
filtering on the subbands. Importantly, we can select the support size
differently for each subband.

With these gains we define the action of the gain layer $v = Gu$ to be:
\begin{align} 
  v_{lp}[f, \nn] &=  \sum_{c=0}^{C_l-1} u_{lp}[c, \nn] \conv g_{lp}[f, c, \nn] \label{eq:ch6:gain1} \\
  v_{1,1}[f, \nn] &=  \sum_{c=0}^{C_l-1} u_{1,1}[c, \nn] \conv g_{1,1}[f, c, \nn] \\
  v_{1,2}[f, \nn] &=  \sum_{c=0}^{C_l-1} u_{1,2}[c, \nn] \conv g_{1,2}[f, c, \nn] \\
                  & \vdots \nonumber \\
  v_{J,6}[f, \nn] &=  \sum_{c=0}^{C_l-1} u_{J,6}[c, \nn] \conv g_{J,6}[f, c, \nn] \label{eq:ch:gain2}
\end{align}

To avoid ambiguity with complex conjugates we remind ourselves that for complex
signals $a, b$ the convolution $a \conv b$ is defined as $(a_r \conv b_r - a_i
\conv b_i) + j(a_r \conv b_i + a_i \conv b_r)$ (see
\autoref{sec:appE:complex_conv}).

The action of the gain layer with only a single-scale wavelet transform, $J=1$, is is shown in
\autoref{fig:ch6:block_diagrams}.

\begin{figure}[t!]
  \centering
  % \resizebox{\textwidth}{!}{\input{\imgpath/block_diagram_dtcwt}}
  \includegraphics[width=\textwidth]{\imgpath/block_diagram_dtcwt}
  \label{fig:ch6:dtcwt_blk_diagram}
  \mycaption{Diagram of proposed method to learn in the wavelet domain}{
  Activations are shaded blue and learned parameters red. Deeper shades of blue
  and red indicate complex valued activations/weights, and lighter values
  indicate real valued activations/weights. The input $x^{(l)}\in
  \mathbb{R}^{C_l\x H\x W}$ is taken into the wavelet domain (here $J=1$) and each 
  subband is mixed independently with $C_{l+1}$ sets of convolutional filters.
  After mixing, a possible wavelet nonlinearity $\sigma_w$ is applied to the
  subbands, before returning to the pixel domain with an inverse wavelet
  transform. Note the similarity to the regular convolutional layer in
  \autoref{fig:ch2:conv_layer}.}
  \label{fig:ch6:block_diagrams}
\end{figure}

\subsubsection{The Output}
% \begin{figure}[t]
  % \centering
  % \input{\imgpath/dtcwt_gain}
  % \mycaption{Block Diagram of a single channel 1-D $\DTCWT$ Gain Layer}{Here we show the real and
  % imaginary trees for a single subband. Note that while it may look similar to 
  % a DWT block diagram, this diagram represents the two trees for one subband rather
  % than a single tree with a pair of subbands. The gain layer does a complex
  % multiply, using both the real and imaginary parts of the decomposed signal.
  % This preserves the shift invariance of the $\DTCWT$ for the reconstructed
  % signal $Y$.
  % }
  % \label{fig:ch6:dtcwt_gain}
% \end{figure}
\begin{figure}[t]
  \centering
  \makebox[\linewidth][c]{
  \subfloat[]{%
    % \includegraphics[width=\textwidth]{\imgpath/dtcwt_gain}
    \input{\imgpath/dtcwt_gain}
    \label{fig:ch6:fwd_pass}
    }}
   \newline
  \makebox[\linewidth][c]{
  \subfloat[]{%
    \input{\imgpath/gain_layer}
    % \includegraphics[width=\textwidth]{\imgpath/gain_layer}
    \label{fig:ch6:bwd_pass}
  }}
  % \input{\imgpath/gain_layer}
  \mycaption{Forward and backward block diagrams for $\DTCWT$ gain layer}{Based
    on Figure~4 in \cite{kingsbury_complex_2001}. Ignoring the $G$ gains, the
    top and bottom paths (through $A_r, S_r$ and $A_i, S_i$ respectively) make up the
    the real and imaginary parts for \emph{one subband} of the dual tree system.
    Combined, $A_r+jA_i$ and $S_r-jS_i$ make the complex filters necessary to have
    support on one side of the Fourier domain (see
    \autoref{fig:ch6:dtcwt_bands}). Adding in the complex gain $G_r + jG_i$, we
    can now attenuate/shape the impulse response in each of the subbands. To
    allow for learning, we need backpropagation. The bottom diagram indicates
    how to pass gradients $\Delta Y(z)$ through the layer. Note that upsampling
    has become downsampling, and convolution has become convolution with the
  time reverse of the filter (represented by $z^{-1}$ terms).}
  \label{fig:ch6:fwd_bwd}
\end{figure}
Due to the shift invariant properties of the $\DTCWT$, each wavelet subband has
a unique transfer function which is almost free of aliasing (see
\autoref{thm:ch6:shiftinv}). If we do complex convolution of the wavelet
coefficients $u$ with gains $g$ as described in \eqref{eq:ch6:gain1} -
\eqref{eq:ch6:gain2}, then we preserve the shift invariant properties (see
\autoref{thm:ch6:shiftinvgain} and \autoref{thm:ch6:complex_multiply}). Then the
inverse $\DTCWT$ of the outputs $v$ are free from aliasing.

We can do a complex multiply of the subband coefficients $u$ with gains $g$ by
using the block diagram shown in \autoref{fig:ch6:fwd_pass}\footnote{Note that despite the resemblance to many block
diagrams for fully decimated DWTs, \autoref{fig:ch6:fwd_pass} is different.
The top rung corresponds to the real part of a subband and the bottom specifies
the imaginary part.}

Let us call the analysis filters $A = A_r + jA_i$ and 
the synthesis filters $S = S_r - jS_i$ (these are normally called $H$ and
$G$, but we keep those letters reserved for the CNN and gain layer filters). The
gain for a specific subband previously was called $g_{j,k}$ but we here refer to
it simply as $G = G_r + jG_i$. The output of this layer is:
\begin{align}\label{eq:ch6:dtcwt_fwd}
  Y(z) = \frac{2}{M}X(z) \left[\vphantom{z^{M}} \right. &  G_r(z^{M}) \left(A_r(z)S_r(z) + A_i(z)S_i(z)\right) \nonumber \\
  +  & \left. G_i(z^{M}) \left( A_r(z)S_i(z) - A_i(z)S_r(z)\right) \right] 
\end{align}
Again, see \autoref{app:ch6:dtcwt} for the derivation which shows how the aliasing
terms caused by the downsampling by $M$ are largely eliminated. 

The complex gain $G$ has a real and imaginary part. The real term $G_r$ 
modifies the subband gain $A_rS_r + A_iS_i$ and the imaginary term $G_i$ modifies its
Hilbert Pair $A_rS_i - A_iS_r$. \autoref{fig:ch6:dtcwt_bands} show the contour
plots for the frequency support of each of these subbands. The complex gain
vector $g$ has elements which adjust the gain and phase of each subband
independently. The magnitude of each element controls the amplitude of the
frequency response in the region of that subband, while its phase controls the
phase of the response and thus modifies the detailed wave-shape (e.g. the
locations of its zero crossings).

\begin{figure}[t]
  \centering
  \subfloat[]{%
    \includegraphics[height=6cm]{\imgpath/subbands.png}
    \label{fig:ch6:dtcwt_bands_freq}
  }
  \hspace{1cm}
%    \newline
  \subfloat[]{%
    \includegraphics[height=5.7cm]{\imgpath/impulses.png}
    \label{fig:ch6:dtcwt_bands_impulse}
  }
  \mycaption{$\DTCWT$ subbands}{\subref{fig:ch6:dtcwt_bands_freq} -1dB and -3dB contour 
  plots showing the support in the Fourier domain of the 6 subbands of the $\DTCWT$ at scales
  1 and 2, and the scale 2 lowpass. These are the product of the single side
  band filters $P(z)$ and $Q(z)$ from \autoref{thm:ch6:shiftinv}, or half of the
  support of the double side band filters $A_rS_r + A_iS_i$ and $A_rS_i -
  A_iS_r$ from \eqref{eq:ch6:dtcwt_fwd}.
  \subref{fig:ch6:dtcwt_bands_impulse} The pixel domain impulse responses for
  the second scale wavelets. The Hilbert pair for each wavelet is the underlying
  sinusoid phase shifted by 90 degrees.}
  \label{fig:ch6:dtcwt_bands}
\end{figure}

\subsubsection{Backpropagation}\label{sec:ch6:dtcwt_update}

We start with the property that for a convolutional block, the
gradient with respect to the input is the gradient with respect to the output
convolved with the time reverse of the filter (proved in \autoref{sec:ch2:conv_grad}).
More formally, if $Y(z) = H(z) X(z)$:
%
\begin{equation}\label{eq:ch6:backprop}
  \Delta X(z) = H(z^{-1}) \Delta Y(z)
\end{equation}
%
where $H(z^{-1})$ is the $Z$-transform of the time/space reverse of $H(z)$,
$\Delta Y(z) \triangleq \dydx{L}{Y}(z)$ is the gradient of the loss with respect
to the output, and $\Delta X(z) \triangleq \dydx{L}{X}(z)$ is the gradient of
the loss with respect to the input. 
If H were complex, the first term in \autoref{eq:ch6:backprop} would be
$\bar{H}(1/\bar{z})$, but as each individual block in the $\DTCWT$ of
\autoref{fig:ch6:fwd_bwd} is purely real, we can use the simpler form $H(z^{-1})$. 

Assume we already have access to the quantity $\Delta Y(z)$ (this is the input
to the backwards pass). \autoref{fig:ch6:bwd_pass} illustrates the
backpropagation procedure.

Let us calculate $\Delta V_r(z)$ and $\Delta V_i(z)$ by backpropagating
$\Delta Y(z)$ through the inverse $\DTCWT$. This is the same as doing the
forward $\DTCWT$ on $\Delta Y(z)$ with the synthesis and analysis filters
swapped and time reversed\footnote{An interesting result is that for orthogonal wavelet
transforms, $S(z^{-1}) = A(z)$, so the backwards pass of an inverse wavelet
transform is equivalent to doing a forward wavelet transform. Similarly, the
backwards pass of the forward transform is equivalent to doing the inverse
transform.}. Then the weight update equations are:
\begin{align}
  \Delta G_r(z) &= \Delta V_r(z) U_r(z^{-1}) + \Delta V_i(z) U_i(z^{-1})  \label{eq:ch6:gr_update}\\
  \Delta G_i(z) &=  -\Delta V_r(z) U_i(z^{-1}) + \Delta V_i(z) U_r(z^{-1})  \label{eq:ch6:gi_update} 
\end{align}
%
The passthrough equations have similar form to \eqref{eq:ch6:dtcwt_fwd}:
\begin{equation}\label{eq:ch6:dtcwt_passthrough}
    \Delta X(z) = \frac{2\Delta Y(z)}{M} \left[G_r(z^{-M})\left( A_r(z)S_r(z) + A_i(z)S_i(z) \right) + \\
       G_i(z^{-M}) \left(A_r(z)S_i(z) - A_i(z)S_r(z) \right) \right] 
\end{equation}

\subsection{Examples}
\autoref{fig:ch6:examples} show example impulse responses of the $\DTCWT$ gain
layer. For comparison, we also show similar `impulse responses' for a gain layer
done in the DWT domain\footnote{Modifying DWT coefficients causes a loss of the
alias cancellation properties so these are not true impulse response.}. The DWT 
outputs come from three random variables: a $1\x 1$ 
convolutional weight applied to each of the low-high, high-low and high-high
subbands. The $\DTCWT$ outputs come from twelve random variables, again a $1\x
1$ convolutional weight, but now applied to six complex subbands. 

To test the space of generated shapes by a vector gain layer gain $g_1$, we
generate $N$ random vectors of length 12, with each entry taken
from a Gaussian distribution with zero mean and
unit variance. We then generate the equivalent point spread functions from
\eqref{eq:ch6:dtcwt_fwd} for the $N$ different instances and measure their 
normalized cross-correlation. We then sort the values and compare the
distribution to a set of $N$ random vectors with $k$ degrees of freedom. 

Our experiments show that the distribution for the $\DTCWT$ gain layer matches
random vectors with roughly 11.5 degrees of freedom (c.f.\ the 12 variables 
the layer has). 
Similarly for the DWT, the 
the normalized cross-correlation matches the distribution for random vectors
with roughly 2.8 degrees of freedom (c.f. 3 random variables in the layer).
This is particularly reassuring for the $\DTCWT$ as it is showing that there is
still representatitve power despite the redundancy of the transform.

\begin{figure}
  \centering
  \subfloat[]{%
    \includegraphics[width=\textwidth]{\imgpath/dtcwt_examples.png}
    \label{fig:ch6:dtcwt_examples}
  }
  \newline
  \subfloat[]{%
  \includegraphics[width=\textwidth]{\imgpath/dwt_examples.png}
    \label{fig:ch6:dwt_examples}
  }
  \mycaption{Example outputs from an impulse input for the proposed gain layers}{
  Example outputs $y = W^{-1}GWx$ for an impulse
  $x$ for the $\DTCWT$ gain layer and for a similarly designed DWT gain layer.
  \subref{fig:ch6:dtcwt_examples} shows the output $y$ for a $\DTCWT$ based
  system. $g_{lp} = 0$ and $g_1$ has spatial size $1\x 1$. The 12 values
  in $g_1$ are independently sampled from a random normal of variance 1. 
  The 60 samples come from 60 different random draws of the weights.
  \subref{fig:ch6:dwt_examples}
  shows the outputs $y$ when $x$ is an impulse and $W$ is the DWT with a
  `db2' wavelet family. Here 3 random numbers are generated for the $g_1$
  coefficients. The strong horizontal and vertical properties of the DWT
  can clearly be seen in comparison to the much freer $\DTCWT$.}
  \label{fig:ch6:examples}
\end{figure}


\subsection{Implementation Details}
Before analyzing its performance, we compare the implementation properties of
our proposed layer to a standard convolutional layer.

\subsubsection{Parameter Memory Cost}\label{sec:ch6:memory}
A standard convolutional layer with $C_l$ input channels, $C_{l+1}$ output channels
and kernel size $k\x k$ has $k^2C_{l}C_{l+1}$ parameters, with $k=3$ or $k=5$
common choices for the spatial size.
\begin{equation}
  \text{\#conv params} = k^2C_lC_{l+1}
\end{equation}

We must choose the spatial sizes of both the lowpass and bandpass
mixing kernels. In our work, we are somewhat limited in how large we would like
to set the bandpass spatial size, as every extra pixel of support requires $2\x
6=12$ extra parameters. For this reason, we almost always set it to have 
support $1\x 1$. The lowpass gains are less costly, and we are free to set them to size
$k_{lp}\x k_{lp}$ (with $k_{lp} = 1,3,5$ in many of our experiments).
%
Further, due to the size of the datasets we test on, we typically limit
ourselves initially to only considering a single scale. If we wish, we can
decompose the input into more scales, resulting in a larger net area of effect.
In particular, it may be useful to do a two scale transform and discard the
first scale coefficients. This does not increase the number of gains to learn,
but changes the position of the bands in the frequency space.

The number of parameters for the gain layer with $k_{lp}=1$ is then:
\begin{equation}
  \text{\#params} = (2\x 6 + 1)C_lC_{l+1} = 13C_lC_{l+1} \label{eq:ch6:memcost2}
\end{equation} 
%
This is slightly larger than the $9C_lC_{l+1}$ parameters used in a
standard $3\x 3$ convolution, but as \autoref{fig:ch6:examples} shows, the
spatial support of the full filter is larger than an equivalent one
parameterized in the filter domain. If $k_{lp}=3$ then we would have $21C_l
C_{l+1}$ parameters, slightly fewer than the $25C_l C_{l+1}$ of a $5\x 5$ convolution.

\subsubsection{Activation Memory Cost}\label{sec:ch6:act_memory}
A standard convolutional layer needs to save the activation $x^{(l)}$ to
convolve with the backpropagated gradient $\dydx{L}{y^{(l+1)}}$ on the backwards
pass (to give $\dydx{L}{w^{(l)}}$). For an input with $C_l$ channels of spatial
size $H\x W$, this means
%
\begin{equation}
  \text{\#conv floats} = HWC_l 
\end{equation}

Our layers require us to save the wavelet coefficients $u_{lp}$ and $u_{j,k}$
for updating the $g$ terms as in \eqref{eq:ch6:gr_update} and \eqref{eq:ch6:gi_update}.  
For the $4:1$ redundant $\DTCWT$, this requires:
%
\begin{equation}
  \text{\#$\DTCWT$ floats} = 4HWC_l 
\end{equation}
%
to be saved for the backwards pass.  You can see this difference from the
difference in the block diagrams in \autoref{fig:ch6:block_diagrams}.

Note that a single scale $\DTCWT$ gain layer requires $16/7$ times as many
floats to be saved as compared to the invariant layer of the previous chapter.
The extra cost of this comes from two things. Firstly, we keep the real and
imaginary components for the bandpass (as opposed to only the magnitude),
meaning we need $3HWC_l$ floats, rather than $\frac{3}{2}HWC_l$. Additionally,
the lowpass was downsampled in the previous chapter, requiring only
$\frac{1}{4}HWC_l$, whereas we keep the full sample rate, costing $HWC_l$.

If memory is an issue and the computation of the $\DTCWT$ is very fast, then we
only need to save the $x^{(l)}$ coefficients and can calculate the $u$'s on the
fly during the backwards pass. Note that a two scale $\DTCWT$ gain layer would
still only require $4HWC_l$ floats.

\subsubsection{Computational Cost}\label{sec:ch6:computation}
A standard convolutional layer with kernel size $k\x k$ needs $k^2C_{l+1}$
multiplies per input pixel (of which there are $C_{l}\x H\x W$).

For the $\DTCWT$, the overhead calculations are the same as in
\autoref{sec:ch5:computation}, so we will omit their derivation here. The mixing
is however different, requiring complex convolution for the bandpass
coefficients, and convolution over a higher resolution lowpass. The bandpass has
one quarter spatial resolution at the first scale, but this is offset by the
$4:1$ cost of complex multiplies compared to real multiplies. Again assuming we
have set $J=1$ and $k_{lp} = 1$ then the total cost for the gain layer is:
%
\begin{equation}
  % \frac{7}{4}C_{l+1} + 48 \label{eq:comp}
  \text{\#mults/pixel} = \underbrace{\hphantom{1} \frac{6\x 4}{4}C_{l+1} \hphantom{1}}_{\textrm{bandpass}} +
  \underbrace{\hphantom{1}\vphantom{\frac{6}{4}} C_{l+1} \hphantom{1}}_{\textrm{lowpass}} + 
  \underbrace{\vphantom{\frac{6}{4}} 36}_{\DTCWT} + 
  \underbrace{\vphantom{\frac{6}{4}} 36}_{\DTCWT^{-1}} = \quad 7C_{l+1} + 72 \quad
  \label{eq:ch6:comp_dtcwt}
\end{equation}
which is marginally smaller than the $9C_{l+1}$ of a $3\x 3$ convolutional layer (if $C_{l+1}$ > 36).

\subsubsection{Parameter Initialization}
For both layer types we use the Glorot Initialization scheme \cite{glorot_understanding_2010}
with $a=1$: 
%
\begin{equation}
  g_{ij} \drawnfrom U\left[ -\sqrt{\frac{6}{(C_l + C_{l+1})k^2}},\ \sqrt{\frac{6}{(C_l + C_{l+1})k^2}}\
  \right] \label{eq:ch6:glorot}
\end{equation}
where $k$ is the kernel size.

\section{Gain Layer Experiments}\label{sec:ch6:gainlayer_experiments}
Before we explore the possibilities and performance of using a
nonlinearity in the wavelet domain, let us present some experiments and results
for the wavelet gain layer where we perform non-linearities in purely in the
spatial domain, as in a conventional CNN layer. This is the first objective in
\autoref{sec:ch6:learning}, comparing $G$ to $H$.

\subsection{CNN activation regression}\label{sec:ch6:regression}
\begin{figure}[t]
  \centering
  \includegraphics[width=\textwidth]{\imgpath/AlexNet_layer1_regression.pdf}
  \mycaption{Normalized mean squared error for conv layer and wavelet gain layer
  regression.}{We minimize of
  \eqref{eq:ch6:conv_regression} and
  \eqref{eq:ch6:gain_regression} on the ImageNet validation set, where the
  target is made from convolving the input with AlexNet's first layer filters. 
  This plot shows the final MSE score compared
  to the number of learnable parameters. The original conv layer has spatial
  support $11\x 11$, and the equivalent number of parameters is shown as a blue
  dotted line. The four points labelled `conv$n$' correspond to filters with
  $n\x n$ spatial support. The four points labelled `gl$abc$' correspond to two
  scale gain layers with $a\x a$ support in the lowpass, $b\x b$ spatial support
  in the first scale, and $c\x c$ spatial support in the second scale. The gain
  layer can regress to the AlexNet filters quite capably. In this example, it is
  important to have at least $3\x3$ lowpass support for the gain layer, and the
  second scale coefficients are more important than the first scale.}
  \label{fig:ch6:gainlayer_regression}
\end{figure}
One of the early inspirations for using wavelets in CNNs was the visualizations
of the first layer filters learned in AlexNet. These $11\x 11$ colour filters
(see \autoref{fig:ch1:alex_filt}) look very much like a 2-D oriented wavelet transform.

So how well can the gain layer emulate the action of this layer? How would it
compare to trying to use a reduced size convolutional kernel to learn the 
action of the layer? 

Let us call the action of our target layer $H_0$, our convolutional layer $H$
and our gain layer $G$. Let $\lnorm{H}{2}$, $\lnorm{G}{2}$ be the $\ell_2$ norm
of the weights for each layer. We would like assume that we do not have direct
access to $H_0$ but only the convolved outputs $Y=H_0 X$. Then, we would like to solve:
\begin{align}
  & \argmin_{H}\ (Y - HX)^2 %+ \frac{\lambda}{2} \lnorm{H}{2}^2 
  \condition{s.t. $h[c, \nn] = 0,\ \forall \nn \notin \mathcal{R}$} \label{eq:ch6:conv_regression}\\
  & \argmin_{G}\ (Y - W^{-1}GWX)^2 %+ \frac{\lambda}{2} \lnorm{G}{2}^2 
  \condition{s.t. $g_{j,k}[c, \nn] = 0,\ \forall \nn \notin \mathcal{R'}$} \label{eq:ch6:gain_regression}
\end{align}
for some support regions $\mathcal{R},\mathcal{R'}$. E.g. $\mathcal{R}$ could be a $3\x3$ or $5\x5$ 
block, and similarly $\mathcal{R'}$ could define a desired support for each gain in each 
subband. 

\eqref{eq:ch6:conv_regression} and \eqref{eq:ch6:gain_regression} are both
regression problems convex in the parameters of $H$ and $G$, with many possible
ways to solve. We are not worried with the optimization procedure chosen here,
but of the final distances $\norm{Y-HX}$ and $\norm{Y-W^{-1}GWx}$ (or
equivalently, their squares). We choose to find $H$ and $G$ by gradient descent,
using the validation set for ImageNet as the data input-output pair $(X, Y)$.
After 3--5 epochs, both $H$ and $G$ typically settle into their global minimum.
Because of the large size of the input filters, we allow for both a $J=1$ and
$J=2$ scale gain layer, but only learn weights at the lowest frequency bandpass
(i.e. for a 2 scale gain layer, we discard the first scale highpass outputs and
only learn $g_2$). 

After training we report the final normalized mean squared errors between the
target values $Y$ and the estimated outputs $\hat{Y}$, defined as:
\begin{equation}
  NMSE = \frac{1}{N} \sum_{n=1}^N \frac{\mathbb{E}\left[(Y-\hat{Y})^2\right]}{\mathbb{E}\left[Y^2\right]}
\end{equation}
The resulting NMSEs are shown in \autoref{fig:ch6:gainlayer_regression} . A label
`gl$ab$' indicates a single scale gain layer with $a\x a$ support in the lowpass
and $b\x b$ support in the highpass; a label `gl$abc$' indicates a two scale
gain layer with $a\x a$ support in the lowpass, $b\x b$ support in the scale 1
highpass and $c\x c$ support in the scale 2 bandpass gains.

This figure shows several interesting things yet unsurprising things. Firstly,
bigger lowpass support is very helpful -- see the difference between gl101,
gl301, and gl501, 3 instances that only vary in the size of the support of their
lowpass filter $g_{lp}$. Additionally, the second scale coefficients appear more
useful than the first scale -- see the difference between gl310 and gl301, two
instances that have the same number of parameters, but gl310 has $g_1$ with
non-zero support, and gl301 has $g_2$ with non-zero support.

\subsection{Ablation Studies}\label{sec:ch6:ablation}
\autoref{fig:ch6:gainlayer_regression} is a useful guide on how the gainlayer
might be placed in a deep CNN. gl110 (a gain layer with a $1\x1$ lowpass kernel
and a $1\x1$ bandpass kernel at the first scale), gl101 (same as gl110 but no
gain at first scale and $1\x 1$ at second scale), and conv3 all achieve similar
MSEs. Additionally gl310, gl301, and conv5 all achieve similar MSEs. 

\subsubsection{Small Kernel Ablation}
Most modern CNNs are built with small $3\x 3$ kernels, which we believe are not the best
use for the gain layer, built from large support wavelets. For this reason, we
deviate from the ablation study done in the previous chapter, and build a
\textbf{shallower} network with \textbf{larger} kernel sizes. 

For completeness, we also ran ablation tests on the the same deeper network
with small kernels used in \autoref{ch:invariant} and include the results in 
\autoref{app:ch6:more_results}.

\subsubsection{Large Kernel Ablation}
\input{\path/ablation_arch2}

\renewcommand{\_}{\textscale{.6}{\textunderscore}}
In this experiment, we build a 3 layer
CNN with $5\x 5$ convolutional kernels, described in \autoref{tab:ch6:ablation_arch}.
To help differentiate with the small kernel network introudeced in the 
ablation study of the previous chapter, we have labelled the convolutions here
`conv1', `conv2' and `conv3' (as opposed to `convA', `convB', `convC',
\ldots).

We then test the difference in accuracy achieved by replacing each of the three
convolution layers with gl310\footnote{Although the gain layers with no gain in
the first scale and gain in the second scale performed better than those with
gain gl310 and gl510 in \autoref{sec:ch6:regression}, we saw them perform
consistently worse in the following ablation studies. For ease of presentation,
we have shown only the results from the single scale gain layer.}. 
On the two CIFAR datasets, we train for 120 epochs, decaying learning rate by a
factor of 0.2 at 60, 80 and 100 epochs, and for the Tiny ImageNet dataset, we
train for 45 epochs, decaying learning rate at 18, 30 and 40 epochs. We set 
$\ell_2$ weight decay of $10^{-4}$ for the real gains in the system, and
$\ell_1$ weight decay of $10^{-5}$ for the complex gains in the system. See
\autoref{sec:appE:complex_reg} for information on how we handle regularizing complex
gains. The
experiment code is available at \cite{cotter_dtcwt_2018}.

The results of various combinations for our three datasets are shown in
\autoref{fig:ch6:gl_results}.
Note that as before, swapping `conv1' with a gain layer is marked by `gain1',
and swapping the first two conv layers with two gain layers is marked by
`gain1\_2' and so forth.  

The results are not too promising. Across all three datasets, changing a
convolutional layer for a gain layer of similar number of parameters
results in a small decrease in accuracy at all depths, and the more layers
swapped out the more this degradation compounds.

\begin{figure}
  \centering
  \subfloat[CIFAR-10]{%
    \includegraphics[width=\textwidth]{\imgpath/cifar10_gainlayer_large.pdf}
    \label{fig:ch6:cifar10_gl}
    }\\
  \subfloat[CIFAR-100]{%
    \includegraphics[width=\textwidth]{\imgpath/cifar100_gainlayer_large.pdf}
    \label{fig:ch6:cifar100_gl}
  }\\
  \subfloat[Tiny ImageNet]{%
    \includegraphics[width=\textwidth]{\imgpath/ti_gainlayer_large.pdf}
    \label{fig:ch6:ti_gl}
  }
  \mycaption{Large kernel ablation results CIFAR and Tiny ImageNet}{Results
  showing the percentage classification accuracies obtained by swapping
  combinations of the three conv layers in the reference architecture from
  \autoref{tab:ch6:ablation_arch} with gain layers. Results shown are averages
  of 3 runs with the $\pm1$ standard deviations shown as dark blue lines. These results
  show that changing a convolutional layer for a gain layer is possible, but
  comes with a small accuracy cost which compounds as more layers are swapped.}
  \label{fig:ch6:gl_results}
\end{figure}

\subsection{Network Analysis}
It is nonetheless interesting to see that a network with only gain layers
(`gain1\_2\_3') can achieve accuracies within a couple of percentage points of a
purely convolutional architecture. 

In this section, we look at some of the properties of the `gain1\_2\_3' for
CIFAR-100 and compare them to the reference architecture.

\subsubsection{Bandpass Coefficients}
When analyzing the `gain1\_2\_3' architecture, the most noticeable thing is the
distribution of the bandpass gain magnitudes. \autoref{fig:ch6:bp_dists} shows
these for the second gain layer, gain2. Of the $64\x 128=8192$ complex
coefficients most have very small magnitude, in particular the diagonal wavelet
gains. This raises an interesting question -- how many of these coefficients are
important for classification? What if we were to apply a hard thresholding
scheme to the weights, would setting some of these values to 0 impact the
entire network accuracy? 

We measure the dropoff in accuracy when a hard threshold $t$ is applied to the 
bandpass gains $g_1$ for the three gain layers of `gain1\_2\_3'. The resulting sparsity
of each layer and the network performance is shown in \autoref{fig:ch6:threshs}.
This figure shows that despite the high cost of the bandpass gains --
$12C_{l}C_{l+1}$ for a $1\x1$ gain, very few of these need to be nonzero.

\begin{figure}
  \centering
  \vspace{-0.5cm}
  \subfloat[Bandpass Magnitude Distributions]{
    \includegraphics[width=.9\textwidth]{\imgpath/complex_mag_dists.pdf}
    \label{fig:ch6:bp_dists}
    }\\\vspace{-0.3cm}
  \subfloat[Accuracy Dropoff from Thresholding]{
    \includegraphics[width=\textwidth]{\imgpath/cifar100_gainlayer_threshs.pdf}
    \label{fig:ch6:threshs}
    }
    % \mycaption{}{}
    \mycaption{Bandpass Gain Properties}{\subref{fig:ch6:bp_dists} shows the
    distribution of the magnitudes for bandpass coefficients for the second
    layer (gain2). Each orientation has $128\x 64=8192$ complex weights, most of
    which are close to or near 0. The $45\degs$ and $135\degs$ weights have many 
    fewer large coefficients. \subref{fig:ch6:threshs} shows the increase in
    sparsity and dropoff in classification accuracy when the weights are
    hard-thresholded with value $t$ (same threshold applied to all 3 layers).
    For a threshold value of $t=0.4$, 80\% of the weights in gain1 are 0, 99\%
    of the weights in gain2 are 0, 99.98\% of the weights in gain3 are 0 and
    classification accuracy is 0.5\% lower than the non-thresholded accuracy.}
  \label{fig:ch6:bp_info}
\end{figure}

\subsubsection{DeConvolution and Filter Sensitivity}
To visualize what the gain layer is responsive to, we build a deconvolutional
system similar to the one described in \autoref{ch:visualizing}. In
particular, we present the entire CIFAR-100 validation set to the reference
architecture and to the gain1\_2\_3 architecture, keeping track of what most
highly excites each channel. Once we have this information, we present the same
image again, storing the ReLU switches and max pooling locations for this same
image, then we zero out all but a single value for the given channel, and zero out all
other channels, and deconvolve to see the input pattern.

The resulting visualizations for the first two layers are shown in
\autoref{fig:ch6:visualizations}. We show only the top activation for each
filter, rather than the top-9. For the second layer filters, we show only 64 of
the 128 filter responses.

It is reassuring to see that despite the
performance difference between the reference architecture and the gain1\_2\_3
architecture, the filters are still responding to shapes. Note that for both the
first and second layer responses, the gain layer has a smoother roll-off at the
edges of the visualization, whereas the convolutional architecture has more
square reconstructions.

\begin{figure}
  \centering
  \subfloat[First layer]{
  \includegraphics{\imgpath/largekernel_conv1.pdf}
  } \\
  \subfloat[Second layer]{
  \includegraphics{\imgpath/largekernel_conv2.pdf}
  }
  \mycaption{Deconvolution reconstructions for the reference architecture and
  purely gain layer architecture}{Visualizations using a DeConvNet method similar to 
  the one described in \autoref{ch:visualizing}. Here we find the input images in CIFAR-100 validation
  set that most highly activate each filter. Each image is then re-shown to the
  network and the meta-information is used to prime the DeConvNet to create the visualizations seen here. 
  The left column has visualizations for the first and second layer filters for
  the all convolutional method, and the right column has visualizations for the
  the first and second layer filters for the all gain layer method. Note the
  smoother roll-off at the edge of visualizations in the gain layer compared to
  the rectangular support regions for the conv layers. Aside from that, the two
  networks appear to be learning similar shapes.}
  \label{fig:ch6:visualizations}
\end{figure}

\section{Wavelet Based Nonliearities}
Returning to the goals from \autoref{sec:ch6:learning}, the experiments from the
previous section have shown that while it is possible to use a wavelet gain
layer ($G$) in place of a convolutional layer ($H$), this may come with a small
performance penalty. Ignoring this effect for the moment, in this section, we
continue with our investigations into learning in the wavelet domain. In
particular, is it possible to replace a pixel domain nonlinearity $\sigma$ with
a wavelet based one $\sigma_w$?

But what sensible nonlinearity to use? Two particular options are good initial
candidates:
\begin{enumerate}
  \item The ReLU: this is a mainstay of most modern neural networks and has
    proved invaluable in the pixel domain. Perhaps its sparsifying properties
    will work well on wavelet coefficients too. 
  \item Thresholding: a technique commonly applied to wavelet
    coefficients for denoising and compression. Many proponents of compressed
    sensing and dictionary learning even like to compare soft thresholding to a
    two sided ReLU \cite{papyan_theoretical_2018, papyan_convolutional_2016}.
\end{enumerate}

In this section we will look at each, see if they add to the gain
layer, and see if they open the possibility of having multiple layers in the
wavelet domain. 

\subsection{ReLUs in the Wavelet Domain}
Applying the ReLU to the real lowpass coefficients is not difficult, but it does
not generalize so easily to complex coefficients. The simplest option is to apply
it independently to the real and imaginary coefficients, effectively only
selecting one quadrant of the complex plane:
\begin{align}
  u_{lp} &= \F{max}(0,\ v_{lp}) \\
  u_{j} &= \F{max}(0,\ \real{v_{j}}) + j\F{max}(0,\ \imag{v_j}) \label{eq:ch6:relu_bp}
\end{align}

Another option is to apply it to the magnitude of the bandpass coefficients. Of
course these are all strictly positive so the ReLU on its own would not do
anything. However, they can be arbitrarily scaled and shifted by using a batch
normalization layer. Then the magnitude could shift to (invalid) negative
values, which can then be rectified by the ReLU. 

Dropping the scale
subscript $j$ for clarity, let a bandpass coefficient at a given scale be
$v = r_v e^{j\theta_v}$ and define
$\mu_r = \mathbb{E}[r_j]$ and $\sigma_r^2 = \mathbb{E}[(r_v-\mu_r)^2]$, then
applying batch-normalization and the ReLU to the magnitude of $v_j$ means we
get:
\begin{align}
  r_u &= \F{ReLU}(BN(r_v)) = \max(0,\ \gamma \frac{r_v-\mu_r}{\sigma_r} + \beta) \label{eq:ch6:magrelu_bp} \\
  u &= r_u e^{j\theta_v} \label{eq:ch6:magrelu_bp2} 
\end{align}
This also works equivalently on the lowpass coefficients although $v_{lp}$ can
be negative unlike $r_v$:
\begin{equation}
  u_{lp} = \F{ReLU}(BN(v_{lp})) = \max(0, \gamma' \frac{v_{lp} - \mu_{lp}}{\sigma_{lp}} + \beta') \label{eq:ch6:bnrelu_lp}
\end{equation}
%
\subsection{Thresholding}
For $t \in \reals,\ z = re^{j\theta} \in \complexes$ the pointwise hard thresholding is defined as:
\begin{align}
  \mathcal{H}(z, t) &= \left\{ \begin{array}{ll}
    z & r \geq t \\
    0 & r < t\\
  \end{array} \right. \\
  &= \indic(r > t) z
\end{align}
The pointwise soft thresholding is:
\begin{align}
  \mathcal{S}(z, t) &= \left\{ \begin{array}{ll}
    (r-t)e^{j\theta} & r \geq t \\
    0 & r < t\\
  \end{array} \right. \\
  &= \max(0, r - t)e^{j\theta} \label{eq:ch6:relu_st}
\end{align}
Note that \eqref{eq:ch6:relu_st} is very similar to \eqref{eq:ch6:magrelu_bp} and \eqref{eq:ch6:magrelu_bp2}.
We can rewrite \eqref{eq:ch6:magrelu_bp} by taking the strictly positive terms
$\gamma$, $\sigma$ outside of the $\max$ operator:
\begin{align}
  r_u &= \max(0, \gamma \frac{r_v-\mu_r}{\sigma_r} + \beta) \\
      &= \frac{\gamma}{\sigma_r}\max\left(0, r_v - \left(\mu_r - \frac{\sigma_r\beta}{\gamma}\right)\right) \label{eq:ch6:bnrelu_soft}
\end{align}
then if $t' = \mu_v - \frac{\sigma_r\beta}{\gamma} > 0$, doing batch
normalization followed by a ReLU on the magnitude of the coefficients is the
same as soft shrinkage with threshold $t'$, scaled by a factor
$\frac{\gamma}{\sigma_r}$.  
The same analogy does not apply to the lowpass
coefficients, as $v_{lp}$ is not strictly positive. 

While they are similar to batch normalizations and ReLUs, we would also like 
to test how well thresholding performs as a wavelet nonlinearity.
To do this, we can:
\begin{itemize}
  \item Learn the threshold $t$ 
  \item Adapt $t$ as a function of the distribution of activations to achieve a
desired sparsity level. 
\end{itemize}
In early experiments we found that trying to set a
desired sparsity level by tracking the standard deviation of the statistics
and setting a threshold as a function of it performed very poorly (causing a
drop in top-1 accuracy of at least 10\%).
Instead, we choose to learn a threshold $t$. We make this an unconstrained
optimization problem by changing \eqref{eq:ch6:relu_st} to:
\begin{equation}
  \mathcal{S}(v, t) = \max(0, r-|t|)e^{j\theta}  \label{eq:ch6:relu_st2}
\end{equation}

Learning a threshold is only possible for soft thresholding, as $\dydx{L}{t}$ is
not defined for hard thresholding. 
% We can
% force some sparsity level however by keeping a track of the moments of the activations with an
% exponential moving average, and finding quantiles by matching the distribution of
% the activations. For the bandpass coefficients, the activations very nearly
% follow an exponential distribution. The lowpass coefficients are a bit more
% varied, following a generalized Gaussian distribution (GGD) with shape parameter
% $\beta$ ranging from 0.6 for the earlier layers to ~1.5 for deepr layers.

\subsection{Non-Linearity Experiments}
\begin{algorithm}[t]
  \caption{The \emph{wave layer} pseudocode}\label{alg:ch6:wavelayer}
\begin{algorithmic}[1]
  \Procedure{WaveLayer}{$x$}
  \State $u_{lp},\ u_{1} \gets \DTCWT(x, \mbox{nlevels}=1) $ 
  \State $v_{lp},\ v_{1} \gets G(u_{lp},\ u_{1}) $ \Comment{the normal wave gain layer}
  \State $w_{lp} \gets \sigma_{lp}(v_{lp})$ \Comment{lowpass nonlinearity}
  \State $w_{1} \gets \sigma_{bp}(v_{1})$ \Comment{bandpass nonlinearity}
  \State $y \gets \DTCWT^{-1}(w_{lp},\ w_{1})$
  \State $x \gets \sigma_{pixel}(y)$ \Comment{pixel nonlinearity}
  \State \textbf{return} $x$
\EndProcedure
\end{algorithmic}
\end{algorithm}
Taking the same `gain1\_2\_3' architecture used for CIFAR-100, we expand the
\emph{wave gain layer} into one bigger layer
dubbed the \emph{wave layer}, described in \autoref{alg:ch6:wavelayer}. In the wave
layer, we have 3 different nonliearities: the pixel, the lowpass 
and the bandpass nonlinearity.

For these experiments, we test over a grid of possible options for these three
functions:
\begin{table}[h!]
  \centering
\begin{tabular}{l l l l l l l}
  \toprule
  Nonlinearity & \hphantom{abc} & \multicolumn{4}{l}{Values} \\
  \midrule
  Pixel && None & BN+ReLU \\
  Lowpass && None & ReLU & BN+ReLU & $\mathcal{S}$ \\
  Bandpass && None & ReLU & BN+MagReLU & $\mathcal{S}$ 
  \\\bottomrule
\end{tabular}
\end{table}

Where:
\begin{itemize}
  \item `None' means no nonlinearity -- $\sigma(x) = x$.
  \item `BN+ReLU' is batch normalization and ReLU (applies only to real valued
    activations) e.g. \eqref{eq:ch6:bnrelu_lp}.
  \item `ReLU' is a ReLU without batch normalization. Can be applied to the real
    and imaginary parts of a complex activation independently i.e.
    \eqref{eq:ch6:relu_bp}. See \autoref{sec:appE:complex_relu} for equations
    for the passthrough gradients for this nonlinearity.
  \item `BN+MagReLU' applies batch normalization to the magnitude of complex
    coefficients and then makes them strictly positive with a ReLU. This action
    is defined in \eqref{eq:ch6:magrelu_bp}. See \autoref{sec:appE:bnrelu} for
    information on the passthrough and update equations for this nonlinearity.
  \item $\mathcal{S}$ is the soft thresholding of \eqref{eq:ch6:relu_st2}
    applied to the magnitudes of coefficients.  See
    \autoref{sec:appE:soft_shrink} for information on the passthrough and update
    equations for this nonlinearity.
    % We choose a
    % conservative sparsity level of 0.2 (20\% of coefficients set to 0) for these
    % thresholds. A full grid search over
    % sparsity levels would be beneficial, but setting it low initially allows us
    % to test its plausibility as a nonlinearity.
\end{itemize}

\input{\path/nonlinear_table}

As the pixel nonliearity has only two options, the results are best displayed as
a pair of tables, firstly for no nonlinearity and secondly for the
the standard batch normalization and ReLU. See
\autoref{tab:ch6:nonlinearities} for these two tables. 

Digesting this information gives us some useful insights: 
\begin{enumerate}
  \item It is possible to improve on the gainlayer from the previous experiments
    with the right nonlinearities. The previous section's gain layer corresponds
    to $\sigma_{lp} = \sigma_{bp} = \F{None}$ and $\sigma_{pixel} = \F{ReLU}$, or
    the top left entry of \autoref{tab:ch6:nonlinearities2}.
  \item Doing a ReLU on the real and imaginary parts of the bandpass
    coefficients independently (the second row of both tables) almost always
    performs worse than having no nonlinearity (first row of both tables).
  \item The best combination is to have batch normalization and a ReLU applied
    to the magnitudes of the bandpass coefficients and batch norm and a ReLU
    applied to either the lowpass or pixel coefficients with no nonlinearity in
    the pixel domain.
\end{enumerate}

The best accuracy score of $65.3\%$ is now $0.1\%$ lower than the fully
convolutional architecture, an improvement from the $62.8\%$ score achieved with
only a pixel nonlinearity.

\subsection{Ablation Experiments with Nonlinearities}
Now that we have found the best nonlinearity to use for the \emph{wave layer},
will this improve our ablation study from \autoref{sec:ch6:ablation}? To test this, 
we repeat the
same experiment on CIFAR-100 only, but use the \emph{wave layer} described in
\autoref{alg:ch6:wavelayer}, with $\sigma_{pixel} = \F{None},\ \sigma_{lp} =
\F{ReLU},\ \text{and } \sigma_{bp} = \F{BN+ReLU}$.

See \autoref{fig:ch6:nonlinear_ablation} for the results from these experiments.
When we use the wavelet based nonlinearities, the results change considerably. 
We see an improvement by $1\%$ when the first layer in the CNN is
changed for a wave layer, but any other changes degrade performance.

\begin{figure}[tb]
  \centering
  \includegraphics[width=\textwidth]{\imgpath/nonlinear_ablation.pdf}
  \mycaption{CIFAR-100 Ablation results with the \emph{wave layer}}{We use the
  same naming scheme from \autoref{sec:ch6:ablation} but to differentiate
  between the results from \autoref{fig:ch6:cifar100_gl} we call the options
  `waveX' (for \emph{wave layer} vs \emph{gain layer}). When we add
  nonlinearities in the wavelet domain, the ablation results from
  change dramatically. It appears that learning in
  the wavelet domain works best for the first layer of the CNN (wave1), and this
  improves on the purely convolutional method by a whole percentage point.
  Replacing the second and third layers degrades performance independently of
  what was used in the first layer. Swapping the first two layers (wave1\_2)
  performs nearly as well and with a narrower spread.}
  \label{fig:ch6:nonlinear_ablation}
\end{figure}
% \begin{table}[t]
  % \centering
  % \renewcommand{\arraystretch}{1.2}
% \begin{tabular}{ll| l l l l l}
  % && \multicolumn{5}{c}{$\sigma_{bp}$} \\
  % && None & ReLU & BN+MagReLU & $\mathcal{S}$ & $\mathcal{H}$ \\
  % \midrule
  % \multirow{5}{*}{$\sigma_{lp}$} & None & \\
                          % &ReLU & \\
                           % &BN+ReLU & \\
                           % &$\mathcal{S}$ & \\
                           % & $\mathcal{H}$ & \\ 
% \end{tabular}
% \end{table}

\section{Conclusion}
In this chapter we have presented the novel idea of learning filters by taking
activations into the wavelet domain. In the wavelet domain then we can apply
the proposed \emph{gain layer} $G$ instead of a pixel wise convolution. We can
return to the pixel domain to apply a ReLU, or stay in the wavelet domain
and apply wavelet based nonlinearities $\sigma_{lp}, \sigma_{bp}$. We have considered the possible
challenges this proposes and described how a multirate system can learn through
backpropagation. 

Our experiments have been promising but are still only preliminary. We have
shown that the \emph{gain layer} can learn in an end-to-end system, achieving nearly
the same accuracies on CIFAR-10, CIFAR-100 and Tiny ImageNet to the reference system with
convolutional layers (\autoref{fig:ch6:gl_results}). This is a good start and shows the plausibility of
the wavelet gain layer, but more experiments on larger datasets and more wavelet
gain layers is needed. Despite the slight reduction in performance, we saw some
nice properties to the gain layer. Most of the bandpass gains are near zero
(\autoref{fig:ch6:bp_info}),
which does not affect training but could offer speedups for inference.
Additionally, doing deconvolution to visualize the sensitivity of the filters in
a gain layer showed that the system was still learning sensible shapes with nice
spatial roll-off properties (\autoref{fig:ch6:visualizations}).

We have searched for good candidates for wavelet nonlinearities, and saw that
using a ReLU on the lowpass coefficients, and
Batch Normalization and a ReLU on the magnitudes of the bandpass coefficients
improved the performance of the gain layer considerably. This is an exciting
development and indicates that we may not need to return to the pixel domain at
all, possibily eliminating the need for the inverse wavelet
transforms used in our experiments (see steps 6 and 7 in
\autoref{alg:ch6:wavelayer}). However, one needs to be careful, as taking the inverse transform 
followed by taking the forward transform does not necessarily give the same
wavelet coefficients due to the redundancy of the $\DTCWT$, instead projecting
onto the range space of the transform. Removing the inverse transform is
something we did not have time to fully explore and so we have included it in
our future work section. 

When we added the nonlinearities to the gain layer to
make the \emph{wave layer}, we saw that we were able to achieve some
improvements in performance over a fully convolutional architecture
(\autoref{fig:ch6:nonlinear_ablation}). The \emph{wave layer}
worked best at the beginning of the CNN, which matches the intuition for doing
this work described in the introduction to the chapter. More research still
needs to be done with the wave layer as part of a deeper system. 
