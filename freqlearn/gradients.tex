\section{Gradients in critically sampled wavelet systems}

If wavelet transforms are to have any place in a deep learning architecture, it
is important that we are able to calculate the derivatives of wavelet
activations with respect to inputs, and use that to define efficient methods to
propagate gradients back from a loss function to any point. 

Let us start with the 1-D discrete wavelet transform for this. The 1-D DWT is
composed of the following components:

\begin{enumerate}
  \item Decimation
  \item Interpolation
  \item Convolution 
  \item Padding (often used to handle the borders of images before convolution)
\end{enumerate}

Once we define the derivative of the output \wrt the input for each of these
blocks, we can use the chain rule to arbitrarily find the derivatives through
any path in the system.

% \begin{lemma}
  % The gradient of decimation is interpolation and the gradient of interpolation is decimation
% \end{lemma}
% \begin{proof}
  % Easy to see, not sure how to prove.
% \end{proof}

% \begin{lemma}
  % The gradient of convolution is correlation.
% \end{lemma}

\begin{proof}
  This is a well-known property but we can prove it here for the discrete 1-D case. Let
  $$y[n] = (x \conv h)[n] = \sum_{m=-\inf}^{\inf} x[m]h[n-m] = \sum_{m=-\inf}^{\inf} x[n-m]h[m] $$
  Then 
\end{proof}
