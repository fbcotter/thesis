\section{Taking Wavelet Transforms of Inputs}\label{sec:ch6:gainlayer}

In contrast to the previous section where we only parameterized filters in the
wavelet domain and transformed the filters back to the pixel domain to do
convolution, this section explores learning wholly in the wavelet domain. I.e.,
we want to take a wavelet decomposition of the input and learn gains to apply to
these coefficients, and optionally return to the pixel domain.

As neural network training involves presenting thousands of training samples on
memory limited GPUs, we want our layer to be fast and as memory efficient as
possible. To achieve this we would ideally choose to use a critically sampled
filter bank implementation.  The fast 2-D Discrete Wavelet Transform (DWT) is a
possible option, but it has two drawbacks: it has poor directional selectivity
and any alteration of wavelet coefficients will cause the aliasing cancelling
properties of the reconstructed signal to disappear. Another option is to use
the $\DTCWT$ \cite{selesnick_dual-tree_2005}. This comes with a memory overhead
which we discuss more in \autoref{sec:ch6:memory}, but it enables us to have
have better directional selectivity and allows for the possibility of returning
to the pixel domain with minimal aliasing \cite{kingsbury_complex_2001}.

In the next section we describe in more detail how the proposed layer works,
agnostic of the wavelet transform used, before describing the differences between
using the DWT and the $\DTCWT$.
% \subsection{Aliasing in the DWT}
% Consider a single level critically sampled DWT in 1-D. The aliasing cancelling
% condition is:

% $$G_0(z)H_0(-z) + G_1(z)H_1(-z) = 0$$

% This is typically solved by using Quadrature Mirror Filters, i.e.

% \begin{align}
  % H_1(z) &= H_0(-z) \\
  % G_0(z) &= H_0(z) \\
  % G_1(z) &= -H_1(z) = -H_0(-z) 
% \end{align}

\subsection{Background}
As we now want to consider the DWT and the $\DTCWT$ which are both implemented
as filter bank systems, we deviate slightly from the notation in the previous
chapter (which was inspired by sampling a continuous wavelet transform). 

Firstly, instead of talking about the continuous spatial variable $\xy$, we now
consider the discrete spatial variable $\nn = [n_1, n_2]$. We switch to square 
brackets to make this clearer. With the new discrete notation, the output of a CNN at layer $l$ is:
%
\begin{equation}
  \cnndlact{x}{l}{c}{\nn}, \quad c\in \{0, \ldots C_l-1\}, \nn \in \integers[2]
\end{equation}
%
where $c$ indexes the channel dimension. 
We also make use of the 2-D $Z$-transform to simplify our analysis:
%
\begin{equation}
  X(\zz) = \sum_{n_1}\sum_{n_2} x[n_1, n_2]z_1^{-n_1}z_2^{-n_2} =
  \sum_{\nn}x[c, \nn]\zz^{-\nn}
\end{equation}
%
As we are working with three dimensional arrays (two spatial and one channel) but are
only doing convolution in two, we introduce a slightly modified 2-D $Z$-transform
which includes the channel index:
%
\begin{equation}
  X(c, \zz) = \sum_{n_1}\sum_{n_2} x[c, n_1, n_2]z_1^{-n_1}z_2^{-n_2} =
  \sum_{\nn}x[c, \nn]\zz^{-\nn} \label{eq:ch6:ztransform}
\end{equation}

Recall that a typical convolutional
layer in a standard CNN gets the next layer's output in a two-step process:
%
\begin{eqnarray} 
  \cnndlact{y}{l+1}{f}{\nn} &=& \sum_{c=0}^{C_l - 1} \cnndlact{x}{l}{c}{\nn} \conv \cnndfilt{l}{f}{c}{\nn}
    \label{eq:ch6:conv}\\
    \cnndlact{x}{l+1}{f}{\xy} & = & \sigma \left( \cnndlact{y}{l+1}{f}{\xy} \right) \label{eq:ch6:nonlin}
\end{eqnarray}
%
With the new $Z$-transform notation introduced in \eqref{eq:ch6:ztransform}, we
can rewrite \eqref{eq:ch6:conv} as:

\begin{equation}
  \cnnlact{Y}{l+1}{f}{\zz} = \sum_{c=0}^{C_l - 1} \cnnlact{X}{l}{c}{\zz}
  H_f^{(l)}(c, \zz)
\end{equation}
%
Note that we cannot rewrite \eqref{eq:ch6:nonlin} with $Z$-transforms as it is a nonlinear
operation.

Also recall that with multirate systems, upsampling by $M$ takes $X(z)$ to
$X(z^M)$ and downsampling by $M$ takes $X(z)$ to $\frac{1}{M}\sum_{k=0}^{M-1} X(W_M^k
z^{1/k})$ where $W_M^k = e^{\frac{j2\pi k}{M}}$. We will drop the $M$ subscript
below unless it is unclear of the sample rate change, simply using $W^k$.

\section{$\DTCWT$ Single Subband Gain}

\begin{figure}
  \centering
  \input{\imgpath/dtcwt}
  \mycaption{Block Diagram of 1-D $\DTCWT$}{Note the top and bottom paths are
  through the wavelet or scaling functions from just level m ($M=2^m$). Figure
  based on Figure~4 in \cite{kingsbury_complex_2001}.}
  \label{fig:ch6:dtcwt_two_tree}
\end{figure}

Let us consider one subband of the $\DTCWT$. This includes the coefficients from
both tree A and tree B. For simplicity in this analysis we will consider the 1-D
$\DTCWT$ without the channel parameter $c$. If we only keep coefficients from a given
subband and set all the others to zero, then we have a reduced tree as shown in
\autoref{fig:ch6:dtcwt_two_tree}. The end to end transfer function is:
%
\begin{equation}
  \frac{Y(z)}{X(z)} = \frac{1}{M} \sum_{k=0}^{M-1} \left[A(W^k z)C(z) + B(W^k z)D(z)\right]
  \label{eq:ch6:aliasing}
\end{equation}
%
where the aliasing terms are formed from the addition of the rotated z
transforms, i.e.\ when $k \neq 0$.

\begin{theorem} \label{thm:ch6:shiftinv}
  Suppose we have complex filters $P(z)$ and $Q(z)$ with support only in the
  positive half of the frequency space. If $A(z) = 2\real{P(z)}$, $B(z) =
  2\imag{P(z)}$, $C(z) = 2\real{Q(z)}$ and $D(z) = -2\imag{Q(z)}$, then the aliasing
  terms in \eqref{eq:ch6:aliasing} are nearly zero and the system is nearly
  shift invariant.
\end{theorem}

\begin{proof}
  See section 4 of \cite{kingsbury_complex_2001} for the full proof of
  this, and section 7 for the bounds on what `nearly' shift invariant means. 
  In short, from the definition of $A, B, C$ and $D$ it follows that:  
  \begin{eqnarray*}
    A(z) &=& P(z) + P^*(z) \\
    B(z) &=& -j(P(z) - P^*(z)) \\
    C(z) &=& Q(z) + Q^*(z) \\
    D(z) &=& j(Q(z) - Q^*(z))
  \end{eqnarray*}
  where $H^*(z) = \sum_n h^*[n]z^{-n}$ is the $Z$-transform of the complex
  conjugate of the complex filter $h$. This reflects the purely positive
  frequency support of $P(z)$ to a purely negative one. Substituting these into
  \eqref{eq:ch6:aliasing} gives:
  \begin{equation}
    A(W^k z)C(z) + B(W^k z)D(z) = 2P(W^kz)Q(z) + 2P^*(W^kz)Q^*(z)
    \label{eq:ch6:complex_filts}
  \end{equation}
 Using \eqref{eq:ch6:complex_filts}, Kingsbury shows that it is easier to design
 single side band filters so $P(W^kz)$ does not overlap with $Q(z)$ and
 $P^*(W^kz)$ does not overlap with $Q^*(z)$ for $k \neq 0$.
\end{proof}

Using \autoref{thm:ch6:shiftinv} \eqref{eq:ch6:aliasing} reduces to:
\begin{equation}
 \frac{Y(z)}{X(z)} = \frac{1}{M}\left[ A(z)C(z) + B(z)D(z) \right]
  \label{eq:ch6:aliasing_cancel}
\end{equation}

Let us extend this idea to allow for any linear gain applied to the passbands
(not just zeros and ones). Ultimately, we may want to allow for nonlinear
operations applied to the wavelet coefficients, but we initially restrict
ourselves to linear gains so that we can build from a sensible base. In
particular, if we want to have gains applied to the wavelet coefficients, it
would be nice to maintain the shift invariant properties of the $\DTCWT$.

\begin{figure}
  \centering
  \input{\imgpath/dtcwt2}
  \mycaption{Block Diagram of 1-D $\DTCWT$}{Note the top and bottom paths are
  through the wavelet or scaling functions from just level m ($M=2^m$). Figure
  based on Figure~4 in \cite{kingsbury_complex_2001}.}
  \label{fig:ch6:dtcwt_two_tree_gain}
\end{figure}

\autoref{fig:ch6:dtcwt_two_tree_gain} shows a block diagram of the extension of 
the above to general gains. This is a two port network with four individual
transfer functions. Let the transfer fucntion from $U_i$ to $V_j$
be $G_{ij}$ for $i, j \in \{a, b\}$. Then $V_a$ and $V_b$ are:
\begin{eqnarray}
  V_a(z) &=& U_a(z)G_{aa}(z) + U_b(z)G_{ba}(z) \\
         &=& \frac{1}{M} \sum_k X(W^{k} z^{1/k}) \left[A(W^k z^{1/k})G_{aa}(z) +
             B(W^k z^{1/k}) G_{ba}(z) \right] \\
  V_b(z) &=& U_a(z)G_{ab}(z) + U_b(z)G_{bb}(z) \\
         &=& \frac{1}{M} \sum_k X(W^{k} z^{1/k}) \left[A(W^k z^{1/k})G_{ab}(z) +
             B(W^k z^{1/k}) G_{bb}(z) \right] 
\end{eqnarray}
%
Further, $Y_a$ and $Y_b$ are:
\begin{eqnarray}
  Y_a(z) &=& C(z)V_a(z^M) \\
  Y_b(z) &=& D(z)V_b(z^M)
\end{eqnarray}
%
Then the end to end transfer function is:
\begin{equation}
  \begin{split}
  % \begin{multline}
    Y(z) = Y_{a}(z) + Y_{b}(z) = \frac{1}{M} \sum_{k=0}^{M-1} X(W^k z)
    & \left[  A(W^kz)C(z)G_{aa}(z^k) + B(W^kz)D(z)G_{bb}(z) + \right. \\
    & \left. \hphantom{[}  B(W^kz)C(z)G_{ba}(z^k) + A(W^kz)D(z)G_{ba}(z) \right] 
    \label{eq:ch6:transfer}
  % \end{multline}
  \end{split}
\end{equation}

\begin{theorem}\label{thm:ch6:shiftinvgain}
  If we let $G_{aa}(z^k) = G_{bb}(z^k) = G_r(z^k)$ and $G_{ab}(z^k) = -G_{ba}(z^k) = G_i(z^k)$
  then the end to end transfer function is shift invariant.
\end{theorem}
\begin{proof}
  Using the above substitutions, the terms in the square brackets of
  \eqref{eq:ch6:transfer} become:
  \begin{equation}\label{eq:ch6:realimag}
    G_r(z^k)\left[A(W^kz)C(z) + B(W^kz)D(z)\right] + G_i(z^k)\left[A(W^kz)D(z) - B(W^kz)C(z)\right]
  \end{equation}
  \autoref{thm:ch6:shiftinv} already showed that the $G_r$ terms are shift
  invariant and reduce to $A(z)C(z) + B(z)D(z)$. To prove the same for the $G_i$
  terms, we follow the same procedure. Using our definitions of $A, B, C, D$
  from \autoref{thm:ch6:shiftinv} we note that:
  %
  \begin{eqnarray}
    A(W^kz)D(z) - B(W^kz)C(z) &=& j\left[P(W^kz) + P^*(W^kz)\right]\left[Q(z) -Q^*(z)\right] +\\
                              &&j\left[P(W^kz) -P^*(W^kz)\right]\left[Q(z) + Q^*(z)\right] \\
                              &=& 2j\left[P(W^kz)Q(z) - P^*(W^kz)Q^*(z)\right]
  \end{eqnarray}
  We note that the difference
  between the $G_r$ and $G_i$ terms is just in the sign of the negative
  frequency parts, $AD - BC$ is the Hilbert pair of $AC+BD$. To prove shift
  invariance for the $G_r$ terms in \autoref{thm:ch6:shiftinv}, we ensured that
  $P(W^kz)Q(z) \approx 0$ and $P^*(W^kz)Q^*(z) \approx 0$ for $k\neq 0$. We can
  use this again here to prove the shift invariance of the $G_i$ terms in
  \eqref{eq:ch6:realimag}. This completes our proof.
\end{proof}

Using \autoref{thm:ch6:shiftinvgain}, the end to end transfer function with the
gains is now
\begin{eqnarray}
  \frac{Y(z)}{X(z)} &=& \frac{2}{M} \left[G_r(z^{M}) \left(A(z)C(z) + B(z)D(z)\right)
  + G_i(z^{M}) \left(A(z)D(z) - B(z)C(z)\right) \right] \\
  &=& \frac{2}{M}\left[G_r(z^{M}) \left(PQ + P^*Q^*\right)
  + jG_i(z^{M}) \left(PQ - P^*Q^*\right) \right]  \label{eq:ch6:end2end}
\end{eqnarray}

Now we know can assume that our $\DTCWT$ is well designed and extracts frequency
bands at local areas, then our complex filter $G(z)=G_r(z) + jG_i(z)$ allows us
to modify these passbands (e.g.\ by simply scaling if $G(z) = C$, or by more
complex functions.


\subsection{Backpropagation Analysis}
We start with the commonly known property that for a convolutional block, the
gradient with respect to the input is the gradient with respect to the output
convolved with the time reverse of the filter. More formally, if 
$Y(z) = H(z) X(z)$:
%
\begin{equation}\label{eq:ch6:backprop}
  \Delta X(z) = H(z^{-1}) \Delta Y(z)
\end{equation}
%
where $H(z^{-1})$ is the $Z$-transform of the time/space reverse of $H(z)$,
$\Delta Y(z) \triangleq \dydx{L}{Y}(z)$ is the gradient of the loss with respect
to the output, and $\Delta X(z) \triangleq \dydx{L}{X}(z)$ is the gradient of
the loss with respect to the input. If H were complex, the first term in
\autoref{eq:ch6:backprop} would be $\bar{H}(1/\bar{z})$, but as each individual
block in the $\DTCWT$ is purely real, we can use the simpler form. 

Assume we already have access to the quantity $\Delta Y(z)$ (this is the input
to the backwards pass). \autoref{fig:ch6:bwd_pass} illustrates the
backpropagation procedure. An interesting result is that the backwards pass of
an inverse wavelet transform is equivalent to doing a forward wavelet
transform.\footnote{As shown in \autoref{fig:ch6:bwd_pass}, the analysis and
synthesis filters have to be swapped and time reversed. For orthogonal wavelet
transforms, the synthesis filters are already the time reverse of the analysis
filters, so no change has to be done. The q-shift filters of the $\DTCWT$
\cite{kingsbury_design_2003} have this property.} Similarly, the backwards pass
of the forward transform is equivalent to doing the inverse transform. The
weight update gradients are then calculated by finding 
\textbf{FIX this - make it not use the $\DTCWT$ to get delta v}.
$\Delta V(z) = \DTCWT\left\{ \Delta Y(z) \right\}$ and then convolving with the 
time reverse of the saved wavelet coefficients from the forward pass - $U(z)$.

\begin{gather}
  \Delta G_r(z) = \Delta V_r(z) U_r(z^{-1}) + \Delta V_i(z) U_i(z^{-1})  \label{eq:ch6:gr_update}\\
  \Delta G_i(z) =  -\Delta V_r(z) U_i(z^{-1}) + \Delta V_i(z) U_r(z^{-1})  \label{eq:ch6:gi_update} 
\end{gather}

Unsurprisingly, the passthrough gradients have similar form to
\autoref{eq:ch6:end2end}:
\begin{equation}\label{eq:ch6:passthrough}
    \Delta X(z) = \frac{2\Delta Y(z)}{M} \left[G_r(z^{-M})\left( PQ + P^*Q^* \right)\right. + 
      \left. jG_i(z^{-M}) \left(PQ-P^*Q^* \right) \right] 
\end{equation}
where we have dropped the $z$ terms on $P(z), Q(z), P^*(z), Q^*(z)$ for brevity.

Note that we only need to evaluate equations
~\ref{eq:ch6:gr_update},\ref{eq:ch6:gi_update},\ref{eq:ch6:passthrough} over the
support of $G(z)$ i.e., if it is a single number we only need to calculate
$\left.\Delta G(z)\right\rvert_{z=0}$.

\input{\path/figure2}

\section{$\DTCWT$ Multiple Subband Gains}\label{sec:ch6:multiple_subbands}

Now that we have the framework for applying a complex gain at one subband, we
can extend this to all of the subbands in the $\DTCWT$. We also reintroduce the channel
dimension. 

In 2-D, a $J$ scale $\DTCWT$ gives $6J+1$ coefficients, 6 sets of complex
coefficients for each scale (representing the oriented bands from 15 to 165
degrees) and 1 set of real lowpass coefficients. Let us write this as:

\begin{equation}
  \DTCWT\{x\} = \{u_{lp}, u_{j,k} \}_{1\leq j\leq J, 1\leq k\leq 6}
  \label{eq:ch6:wave_coeffs}
\end{equation}
%
Sometimes we may want to refer to all of the subband coefficients at a single scale
with one variable, in which case we will call them $\vec{u}_{j}$.

To do the mixing across the $C_l$ channels at each subband, giving $C_{l+1}$
output channels, we introduce the learnable filters:
%
\begin{eqnarray}
  g_{lp} &\in& \reals[C_{l+1}\x C_l\x k_h\x k_w] \label{eq:ch6:glp} \\
  g_{1,1} &\in& \complexes[C_{l+1}\x C_l\x k_h\x k_w] \\
  g_{1,2} &\in& \complexes[C_{l+1}\x C_l\x k_h\x k_w] \\
      & \vdots &\\
  g_{J,6} &\in& \complexes[C_{l+1}\x C_l\x k_h\x k_w]  \label{eq:ch6:gj6}
\end{eqnarray}
%
where $k_h, k_w$ are the sizes of the mixing kernels. These could be $1\x 1$ for
simple gain control, or could be larger, say $3\x 3$, to do more complex
filtering on the subbands. Let us index the lowpass filters $g_{lp}$ and the
bandpass filters $g_{j,k}$ as $g_{lp}[f, c, \nn]$ and $g_{j,k}[f, c, \nn]$.

With these gains we create new coefficients:
\begin{eqnarray}
  v_{lp}[f, \nn] &= & \sum_{c=0}^{C_l-1} u_{lp}[c, \nn] \conv g_{lp}[f, c, \nn] \\
  v_{1,1}[f, \nn] &= & \sum_{c=0}^{C_l-1} u_{1,1}[c, \nn] \conv g_{1,1}[f, c, \nn] \\
  v_{1,2}[f, \nn] &= & \sum_{c=0}^{C_l-1} u_{1,2}[c, \nn] \conv g_{1,2}[f, c, \nn] \\
                  & \vdots & \\
  v_{J,6}[f, \nn] &= & \sum_{c=0}^{C_l-1} u_{J,6}[c, \nn] \conv g_{J,6}[f, c, \nn] 
\end{eqnarray}

I.e., we do independent mixing at each of the different subbands. For $1\x 1$
kernels, this is simply a matrix multiply of the wavelet coefficients. Note that
for complex signals $a, b$ the convolution $a \conv b$ is defined as $(a_r \conv
b_r - a_i \conv b_i) + j(a_r \conv b_i + a_i \conv b_r)$.


\subsection{Examples}
\autoref{fig:ch6:dtcwt_bands} show example impulse responses of our layer.
These impulses were generated by randomly initializing both the real and
imaginary parts of $g_{2,k} \in \complexes[1\x 1]$ from $\mathcal{N}(0,1)$
independently for the six orientations $k$. $g_{1,k}, g_{lp}$ are set to 0. 
I.e. each shape has 12 random variables. It is good
to see that there is still a large degree of variability between shapes. Our
experiments have shown that the distribution of the normalized cross-correlation
between 512 of such randomly generated shapes matches the distribution for
random vectors with roughly 11.5 degrees of freedom.
\autoref{fig:ch6:dtcwt_bands} shows the frequency support of the $6J+1$ subbands
for a two scale $\DTCWT$ as well as some of the equivalent impulse responses for
a randomly initialized set of $g$ filters.

\input{\imgpath/figure1}
