\section{The Wavelet Gain Layer}

\begin{figure}[t]
  \centering
  \includegraphics[width=0.8\textwidth]{\imgpath/fwd_chain.jpg}
  \mycaption{Proposed new forward pass in the wavelet domain}{Two network 
  layers with some possible options for processing. Solid lines denote the
  evaluation path and dashed lines indicate relationships. In (a) we see a
  regular convolutional neural network. We have included the dashed lines to
  make clear what we are denoting as $u$ and $v$ with respect to their
  equivalents $x$ and $y$. In (b) we get to $y^{(2)}$ through a different path.
  First we take the wavelet transform of $x^{(1)}$ to give $u^{(1)}$, apply a
  wavelet gain layer $\mathcal{G}^{(1)}$, and take the inverse wavelet transform
  to give $y^{(2)}$. The cross through $\mathcal{H}^{(1)}$ indicates that this
  path is no longer present. Note that there may not be any possible
  $\mathcal{G}^{(1)}$ to make $y^{(2)}$ from (b) equal $y^{(2)}$ from (a). In
  (c) we have stayed in the wavelet domain longer, and applied a wavelet
  nonlinearity $\sigma_w$ to give $u^{(2)}$. We then return to the pixel domain
  to give $x^{(2)}$ and continue on from there in the pixel domain.}
  \label{fig:ch6:fwd_chain}
\end{figure}


At the beginning of each stage of a neural network we have the activations
$x^{(l)}$. Naturally, all of these activations have their equivalent DWT 
coefficients $\mathtt{u}^{(l)}$ and $\DTCWT$ coefficients $u^{(l)}$. 

From \eqref{eq:ch6:conv}, convolutional layers also have intermediate
activations $y^{(l)}$. Let us differentiate these from the $x$ coefficients and
modify \eqref{eq:ch6:dwt_coeffs} and \eqref{eq:ch6:dtcwt_coeffs} to say the DWT
of $y^{(l)}$ gives $\mathtt{v}$ and the $\DTCWT$ of $y^{(l)}$ gives $v$.

We now propose the wavelet gain layers $\mathtt{G}$ and $G$ for the DWT and
$\DTCWT$ respectively, or $\mathcal{G}$ for an implementation agnostic layer.
The name `gain layer' comes from the inspiration for this chapter's work, in
that the first layer of CNN could be nearly done in the wavelet domain by
setting subband gains to 0 and 1. 

The gain layer $\mathcal{G}$ can be used instead of a convolutional layer. 
It is designed to work on the wavelet coefficients of an activation,
$\mathtt{u}$ and $u$ to give outputs $\mathtt{v}$ and $v$. 

This can be seen as breaking the convolutional path in
\autoref{fig:ch6:fwd_chain} and taking a new route to get to the next layer's
coefficients. From here, we can return to the pixel domain by taking the
corresponding inverse wavelet transform $\mathcal{W}^{-1}$. Alternatively, we
can stay in the wavelet domain and apply a wavelet based nonlinearity $\sigma_w$
to give the next layer's $u$ coefficients. Ultimately we would like to explore
architecture design with arbitrary sections in the wavelet and pixel domain, but
to do this we must first explore: 
\begin{itemize}
  \item How effective $\mathcal{G}$ is at replacing $\mathcal{H}$.
  \item How effective $\sigma_w$ is at replcaing $\sigma$.
\end{itemize}


\subsection{The DWT Gain Layer}
As mentioned previously, modifying the wavelet coefficients of a critically
sampled DWT will necessarily result in a loss of the aliasing cancelling
properties. However, in a deep neural network, this is not as obviously a bad
thing as it is for denoising or deconvolution. For this reason, we note that
there may be a problem in using a DWT, but proceed nonetheless.

For each subband in our $J$ scale system, we introduce a gain term $\mathtt{g}$. Let us specify our
input $\mathtt{u}$ has $C_l$ channels, and we would like our output $\mathtt{v}$
to have $C_{l+1}$ channels, then $\mathtt{g}$ is made up of:
\begin{align}
  \mathtt{g}_{lp} &\in \reals[C_{l+1}\x C_l\x k_{lp}\x k_{lp}] \label{eq:ch6:dwt_gstart} \\
  \mathtt{g}_{1,1} &\in \reals[C_{l+1}\x C_l\x k_{1}\x k_{1}] \\
  \mathtt{g}_{1,2} &\in \reals[C_{l+1}\x C_l\x k_{1}\x k_1] \\
      & \vdots  \nonumber \\
  \mathtt{g}_{J,3} &\in \reals[C_{l+1}\x C_l\x k_J\x k_J] \label{eq:ch6:dwt_gend}
\end{align}
%
Note that we have allowed for different kernel spatial sizes for the lowpass and for
each bandpass scale $\mathtt{g}_j$. This is to allow for the flexibility of
putting more emphasis on certain frequency areas if desired.

With these gains, we define $\mathtt{v}=\mathtt{Gu}$ to be:
\begin{align}
  \mathtt{v}_{lp}[f, \nn] &=  \sum_{c=0}^{C_l-1} \mathtt{u}_{lp}[c, \nn] \conv \mathtt{g}_{lp}[f, c, \nn] \\
  \mathtt{v}_{1,1}[f, \nn] &=  \sum_{c=0}^{C_l-1} \mathtt{u}_{1,1}[c, \nn] \conv \mathtt{g}_{1,1}[f, c, \nn] \\
  \mathtt{v}_{1,2}[f, \nn] &=  \sum_{c=0}^{C_l-1} \mathtt{u}_{1,2}[c, \nn] \conv \mathtt{g}_{1,2}[f, c, \nn] \\
                  & \vdots  \nonumber \\
  \mathtt{v}_{J,3}[f, \nn] &=  \sum_{c=0}^{C_l-1} \mathtt{u}_{J,3}[c, \nn] \conv \mathtt{g}_{J,3}[f, c, \nn] 
\end{align}
%
I.e., we do independent mixing at each of the different subbands. For $1\x 1$
kernels, this is simply a matrix multiply of the wavelet coefficients. This is shown
visually in \autoref{fig:ch6:dwt_blk_diagram}.

\subsubsection{The Output}

\begin{figure}[ht!]
  \centering
  \input{\imgpath/dwt_gain}
  \mycaption{Block Diagram of 1-D DWT Gain Layer}{Here we show the low and
  highpass for a single scale 1-D DWT, Gain Layer and inverse DWT. The gain layer
  has gains $\mathtt{g}_0$ and $\mathtt{g}_1$.}
  \label{fig:ch6:dwt_gain}
\end{figure}

To explore the action of the gain layer $\mathtt{G}$ on DWT coefficients, let
use examine a 1-D, single scale, and single channel system. We want to find the
action of the layer without any nonlinearities, i.e., 
\begin{equation}
y = \mathcal{W}^{-1}\mathtt{G}\mathcal{W}x
\end{equation}
or the path taken in \autoref{fig:ch6:fwd_chain}b. 

Let us call the low and highpass analysis filters of the DWT $A_0$ and $A_1$,
and the synthesis filters $S_0$ and $S_1$ (these are normally called $H$ and
$G$, but we keep those letters reserved for the CNN and gain layer filters). 
From the perfect reconstruction property of the DWT, we know 
\begin{equation}
  A_0(z)S_0(z) + A_1(z)S_1(z) = 2
\end{equation}
and from the aliasing cancellation property of the DWT, we know:
\begin{equation}
  A_0(-z)S_0(z) + A_1(-z)S_1(z) = 0
\end{equation}
%
If we add in gains $G_0$ and $G_1$ to the low and highpass coefficients, as
shown in \autoref{fig:ch6:dwt_gain}, then the output is:
\begin{equation}
  \begin{split}
   Y(z) =& \frac{1}{2} X(z) \left[A_0(z)S_0(z)G_0(z^2) + A_1(z)S_1(z)B(z^2) \right] + \\
         & \frac{1}{2}X(-z) \left[A_0(-z)S_0(z)G_0(z^2) + A_1(-z)S_1(z)G_1(z^2) \right]
\end{split}
\end{equation}
If we let $D(z) = G_1(z^2) - G_0(z^2)$ then the above becomes:
\begin{equation}
  Y(z) = \frac{1}{2} X(z) \left[A_0(z)S_0(z)G_0(z^2) + A_1(z)S_1(z)G_1(z^2) \right] + 
         \frac{1}{2}X(-z)D(z)A_1(-z)S_1(z) \label{eq:ch6:dwt_fwd}
\end{equation}

\subsubsection{Backpropagation}\label{sec:ch6:dwt_update}
We start with the commonly known property that for a convolutional block, the
gradient with respect to the input is the gradient with respect to the output
convolved with the time reverse of the filter. More formally, if 
$Y(z) = H(z) X(z)$:
%
\begin{equation}\label{eq:ch6:backprop}
  \Delta X(z) = H(z^{-1}) \Delta Y(z)
\end{equation}
%
where $H(z^{-1})$ is the $Z$-transform of the time/space reverse of $H(z)$,
$\Delta Y(z) \triangleq \dydx{L}{Y}(z)$ is the gradient of the loss with respect
to the output, and $\Delta X(z) \triangleq \dydx{L}{X}(z)$ is the gradient of
the loss with respect to the input. 

Assume we already have access to the quantity $\Delta Y(z)$ (this is the input
to the backwards pass). \autoref{fig:ch6:bwd_pass} illustrates the
backpropagation procedure. An interesting result is that for orthogonal wavelet
transforms, $S(z^{-1}) = A(z)$, so the backwards pass of an inverse wavelet
transform is equivalent to doing a forward wavelet transform. Similarly, the
backwards pass of the forward transform is equivalent to doing the inverse
transform. The weight update gradients are then calculated by finding 
$\Delta V_i(z) = S_i(z^{-1})Y(z)$ for $i=0,\ 1$, and then convolving with the
time reverse of the saved wavelet coefficients from the forward pass -
$U_i(z^{-1})$.

\begin{equation}
  \Delta G_i(z) = \Delta V_i(z) U_i(z^{-1}) \label{eq:ch6:g_update}
\end{equation}

Unsurprisingly, the passthrough gradients have similar form to \eqref{eq:ch6:dwt_fwd}
\begin{equation}
  \Delta X(z) = \frac{1}{2} \Delta Y(z) \left[A_0(z)S_0(z)G_0(z^{-2}) + A_1(z)S_1(z)G_1(z^{-2}) \right] + 
         \frac{1}{2}\Delta Y(-z)D(z)A_1(-z)S_1(z) \label{eq:ch6:dwt_passthrough}
\end{equation}

Note that we only need to evaluate \eqref{eq:ch6:g_update},
\eqref{eq:ch6:dwt_passthrough} over the support of $G(z)$ i.e., if it is a
single number we only need to calculate $\left.\Delta G(z)\right\rvert_{z=0}$.

\input{\path/figure2}
 
\subsubsection{The $\DTCWT$ Gain Layer}

The $\DTCWT$ gain layer is the same in principle to the action of the DWT gain
layer, but due to the different properties of the two transforms, the
implementation is slightly different. 
Now that we have the framework for applying a complex gain at one subband, we
can extend this to all of the subbands in the $\DTCWT$. We also reintroduce the channel
dimension. 

To do the mixing across the $C_l$ channels at each subband, giving $C_{l+1}$
output channels, we introduce the learnable filters:
%
\begin{align}
  g_{lp} &\in \reals[C_{l+1}\x C_l\x k_{lp}\x k_{lp}] \label{eq:ch6:glp} \\
  g_{1,1} &\in \complexes[C_{l+1}\x C_l\x k_1\x k_1] \\
  g_{1,2} &\in \complexes[C_{l+1}\x C_l\x k_1\x k_1] \\
      & \vdots \nonumber \\
  g_{J,6} &\in \complexes[C_{l+1}\x C_l\x k_J\x k_J]  \label{eq:ch6:gj6}
\end{align}
%
where $k, k$ are the sizes of the mixing kernels. These could be $1\x 1$ for
simple gain control, or could be larger, say $3\x 3$, to do more complex
filtering on the subbands. 

With these gains we define $v = Gu$ to be:
\begin{align}
  v_{lp}[f, \nn] &=  \sum_{c=0}^{C_l-1} u_{lp}[c, \nn] \conv g_{lp}[f, c, \nn] \\
  v_{1,1}[f, \nn] &=  \sum_{c=0}^{C_l-1} u_{1,1}[c, \nn] \conv g_{1,1}[f, c, \nn] \\
  v_{1,2}[f, \nn] &=  \sum_{c=0}^{C_l-1} u_{1,2}[c, \nn] \conv g_{1,2}[f, c, \nn] \\
                  & \vdots \nonumber \\
  v_{J,6}[f, \nn] &=  \sum_{c=0}^{C_l-1} u_{J,6}[c, \nn] \conv g_{J,6}[f, c, \nn] 
\end{align}

Note that for complex signals $a, b$ the convolution $a \conv b$ is defined as $(a_r \conv
b_r - a_i \conv b_i) + j(a_r \conv b_i + a_i \conv b_r)$. This is shown 
in Figure~\autoref{fig:ch6:dtcwt_blk_diagram}.

\subsubsection{The Output}
Unlike the DWT gain layer, the $\DTCWT$ gain layer can achieve aliasing
cancelling and therefore has a transfer function. The proof of this is done in
\autoref{app:ch6:dtcwt}. 

\begin{figure}[ht!]
  \centering
  \input{\imgpath/dtcwt_gain}
  \mycaption{Block Diagram of 1-D $\DTCWT$ Gain Layer}{Here we show the real and
  imaginary trees for a single subband. Note that while it may look similar to 
  \autoref{fig:ch6:dwt_gain}, this diagram represents the two trees for one subband rather
  than a single tree with a pair of subbands. The gain layer does a complex
  multiply, using both the real and imaginary parts of the decomposed signal.
  This preserves the shift invariance of the $\DTCWT$ for the reconstructed
  signal $Y$.
  }
  \label{fig:ch6:dtcwt_gain}
\end{figure}

For a single subband in the $\DTCWT$, the gain layer uses a complex learned
weight $g$, but otherwise produces the output $v$ in much the same way as the
DWT gain layer. \autoref{fig:ch6:dtcwt_gain} shows a single subband $\DTCWT$
based gain layer. The output of this layer is:

\begin{equation}\label{eq:ch6:dtcwt_fwd}
  Y(z) = \frac{2}{M}X(z) \left[G_r(z^{M}) \left(A_r(z)S_r(z) + A_i(z)S_i(z)\right)
  + G_i(z^{M}) \left(A_r(z)S_i(z) - A_i(z)S_r(z)\right) \right] \\
\end{equation}
See \autoref{app:ch6:dtcwt} for the derivation. The $G_r$ term modifies the
subband gain $A_rS_r + A_iS_i$ and the $G_i$ term modifies its Hilbert Pair
$A_rS_i - A_iS_r$. \autoref{fig:ch6:dtcwt_bands} show the contour plots for the
frequency support of each of these subbands. The complex gain $g$ can be used to
reshape the frequency response for each subband independently.

\subsubsection{Backpropagation}\label{sec:ch6:dtcwt_update}
If H were complex, the first term in \autoref{eq:ch6:backprop} would be
$\bar{H}(1/\bar{z})$, but as each individual block in the $\DTCWT$ is purely
real, we can use the simpler form $H(z^{-1})$. 

Again, let us calculate $\Delta V_r(z)$ and $\Delta V_i(z)$ by backpropagating
$\Delta Y(z)$ through the inverse $\DTCWT$. Again this is the same as doing the
forward $\DTCWT$ on $\Delta Y(z)$. Then the weight update equations are:

\begin{gather}
  \Delta G_r(z) = \Delta V_r(z) U_r(z^{-1}) + \Delta V_i(z) U_i(z^{-1})  \label{eq:ch6:gr_update}\\
  \Delta G_i(z) =  -\Delta V_r(z) U_i(z^{-1}) + \Delta V_i(z) U_r(z^{-1})  \label{eq:ch6:gi_update} 
\end{gather}
%
The passthrough equations have similar form to \eqref{eq:ch6:dtcwt_fwd}:
\begin{equation}\label{eq:ch6:dtcwt_passthrough}
    \Delta X(z) = \frac{2\Delta Y(z)}{M} \left[G_r(z^{-M})\left( A_r(z)S_r(z) + A_i(z)S_i(z) \right)\right. + 
      \left. jG_i(z^{-M}) \left(A_r(z)S_i(z) - A_i(z)S_r(z) \right) \right] 
\end{equation}

\begin{figure}[t]
  \centering
  \subfloat[]{%
    \includegraphics[height=6cm]{\imgpath/subbands.png}
    \label{fig:ch6:dtcwt_bands_freq}
  }
  \hspace{1cm}
%    \newline
  \subfloat[]{%
    \includegraphics[height=5.7cm]{freqlearn/images/impulses.png}
    \label{fig:ch6:dtcwt_bands_impulse}
  }
  \mycaption{$\DTCWT$ subbands}{\subref{fig:ch6:dtcwt_bands_freq} -1dB and -3dB contour plots showing
  the support in the Fourier domain of the 6 subbands of the $\DTCWT$ at scales
  1 and 2, and the scale 2 lowpass. These are the product of the single side
  band filters $P(z)$ and $Q(z)$ from \autoref{thm:ch6:shiftinv}.
  \subref{fig:ch6:dtcwt_bands_impulse} The pixel domain impulse responses for
  the second scale wavelets. The Hilbert pair for each wavelet is the underlying
  sinusoid phase shifted by 90 degrees.}
  \label{fig:ch6:dtcwt_bands}
\end{figure}

\begin{figure}[t!]
  \centering
  \subfloat[]{%
    \resizebox{\textwidth}{!} {\input{\imgpath/block_diagram_dwt}}
    \label{fig:ch6:dwt_blk_diagram}
  }\newline
  \vspace{1cm}
  \subfloat[]{%
    \resizebox{\textwidth}{!} {\input{\imgpath/block_diagram_dtcwt}}
    \label{fig:ch6:dtcwt_blk_diagram}
  }
  \mycaption{Block diagrams of proposed method to learn in the wavelet domain}{
  Activations are shaded blue and learned parameters red. Darker shades of
  blue and red indicate complex valued activations and weights, whereas the
  lighter shades indicate purely real values. The input $x^{(l)}\in
  \mathbb{R}^{C_l\x H\x W}$ is taken into the wavelet domain and each subband is
  mixed independently with $C_{l+1}$ sets of convolutional filters (this is what
  we call the `wavelet gain layer'. After mixing, a possible wavelet nonlinearity 
  $\sigma_w$ is applied to the subbands, before returning to the pixel domain
  with an inverse wavelet transform. \subref{fig:ch6:dwt_blk_diagram} shows 
  the system in the DWT domain. The DWT has fewer coefficients, and the gain
  layer requires fewer parameters; this is the simpler layer. 
  \subref{fig:ch6:dtcwt_blk_diagram} shows the $\DTCWT$
  based system. With 6 complex subbands and a larger lowpass, this is the more
  complex layer.}
  \label{fig:ch6:block_diagrams}
\end{figure}

\subsection{Examples}
\autoref{fig:ch6:examples} show example impulse responses of our layer for both
the DWT and $\DTCWT$ systems. The DWT outputs come from three random variables: a $1\x 1$ 
convolutional weight applied to each of the low-high, high-low and high-high
subbands. The $\DTCWT$ outputs come from twelve random variables. Again a $1\x
1$ convolutional weight, but now applied to six complex subbands. 
Our experiments have shown that the distribution of the normalized
cross-correlation between 512 of such randomly generated shapes for the DWT matches the
distribution for random vectors with roughly 2.8 degrees of freedom (c.f. 3
random variables in the layer). Similarly for the $\DTCWT$, the distribution of
the normalized cross-correlation matches the distribution for random vectors
with roughtly 11.5 degrees of freedom (c.f. 12 random variables in the layer).
This is particularly reassuring for the $\DTCWT$ as it is showing that there is
still representatitve power despite the redundancy of the transform.

\begin{figure}
  \centering
  \subfloat[]{%
  \includegraphics[width=\textwidth]{\imgpath/dwt_examples.png}
    \label{fig:ch6:dwt_examples}
  }
  \newline
  \subfloat[]{%
    \includegraphics[width=\textwidth]{\imgpath/dtcwt_examples.png}
    \label{fig:ch6:dtcwt_examples}
  }
  \mycaption{Example outputs from an impulse input for the proposed gain layers}{
  Example outputs $y = \mathcal{W}^{-1}\mathcal{G}\mathcal{W}x$ for an impulse
  $x$ for both the DWT and $\DTCWT$ based systems. \subref{fig:ch6:dwt_examples}
  shows the outputs $y$ when $x$ is an impulse, $\mathcal{W}$ is the DWT with a
  `db2' wavelet family and $\mathcal{G} = \mathtt{G}$ is the DWT gain layer. The
  gain layer weights $\mathtt{g}_{lp}$ coefficients are set to 0 and
  $\mathtt{g}_1$ coefficients have spatial size $1\x 1$ and are sampled
  independently from a random normal of variance 1. The 60 samples come from 60
  different random initializations.
  \subref{fig:ch6:dtcwt_examples} shows the output $y$ for a $\DTCWT$ based
  system. Again, $g_{lp} = 0$ and $g_1$ has spatial size $1\x 1$. The 12 values
  in $g_1$ are independently sampled from a random normal of variance 1.}
  \label{fig:ch6:examples}
\end{figure}

