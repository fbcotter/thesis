\section{Gain Layer Nonlinearity Experiments}\label{sec:ch6:nonlinear_exps}
\begin{algorithm}[t]
  \caption{The \emph{wavelet gain layer} pseudocode}\label{alg:ch6:wavelayer}
\begin{algorithmic}[1]
  \Procedure{WaveGainLayer}{$x$}
  \State $u_{lp},\ u_{1} \gets \DTCWT(x, \mbox{nlevels}=1) $ 
  \State $v_{lp},\ v_{1} \gets G(u_{lp},\ u_{1}) $ \Comment{the normal wave gain layer}
  \State $u_{lp} \gets \sigma_{lp}(v_{lp})$ \Comment{lowpass nonlinearity}
  \State $u_{1} \gets \sigma_{bp}(v_{1})$ \Comment{bandpass nonlinearity}
  \State $y \gets \DTCWT^{-1}(u_{lp},\ u_{1})$
  \State $x \gets \sigma_{pixel}(y)$ \Comment{pixel nonlinearity}
  \State \textbf{return} $x$
\EndProcedure
\end{algorithmic}
\end{algorithm}
Taking the same `gain1\_2\_3' architecture used for CIFAR-100, we expand the
\emph{wavelet gain layer} by including nonlinearities as described in 
\autoref{alg:ch6:wavelayer}. In this layer, we have three different nonlinearities:
the pixel, the lowpass, and the bandpass nonlinearity.

For these experiments, we test over a grid of possible options for these three
functions:
\begin{table}[h!]
  \centering
\begin{tabular}{l l l l l l l}
  \toprule
  Nonlinearity & \hphantom{abc} & \multicolumn{4}{l}{Options} \\
  \midrule
  Pixel && None & BN+ReLU \\
  Lowpass && None & ReLU & BN+ReLU & $\mathcal{S}$ \\
  Bandpass && None & ReLU & BN+MagReLU & $\mathcal{S}$ 
  \\\bottomrule
\end{tabular}
\end{table}
where:
\begin{itemize}
  \item `None' means no nonlinearity: $\sigma(x) = x$.
  \item `ReLU' is a ReLU without batch normalization. For real values, is a
    normal ReLU, for complex values is a ReLU applied independently to real and
    imaginary parts, i.e.
    \eqref{eq:ch6:relu_bp}. See \autoref{sec:appE:complex_relu} for equations
    for the passthrough gradients for this nonlinearity.
  \item `BN+ReLU' is batch normalization and ReLU (applicable only to real-valued
    activations) e.g. \eqref{eq:ch6:bnrelu_lp}.
  \item `BN+MagReLU' applies batch normalization to the magnitude of complex
    coefficients and then makes them strictly positive with a ReLU. This action
    is defined in \eqref{eq:ch6:magrelu_bp}. See \autoref{sec:appE:bnrelu} for
    information on the passthrough and update equations for this nonlinearity.
  \item $\mathcal{S}$ is the soft thresholding of \eqref{eq:ch6:relu_st2}
    applied to the magnitudes of coefficients with learnable thresholds.  See
    \autoref{sec:appE:soft_shrink} for information on the passthrough and update
    equations for this nonlinearity.
    % We choose a
    % conservative sparsity level of 0.2 (20\% of coefficients set to 0) for these
    % thresholds. A full grid search over
    % sparsity levels would be beneficial, but setting it low initially allows us
    % to test its plausibility as a nonlinearity.
\end{itemize}

\input{\path/tables/nonlinear_table}

As the pixel nonlinearity has only two options, the results are best displayed as
a pair of tables, firstly for no nonlinearity and secondly for the
standard batch normalization and ReLU. See
\autoref{tab:ch6:nonlinearities} for these two tables. 

Digesting this information gives us some useful insights: 
\begin{enumerate}
  \item It is possible to improve on the gain layer from the previous experiments
    with the right nonlinearities. The previous section's gain layer corresponds
    to $\sigma_{lp} = \sigma_{bp} = \F{None}$ and $\sigma_{pixel} = \F{ReLU}$, or
    the top left entry of \autoref{tab:ch6:nonlinearities2}.
  \item Doing a ReLU on the real and imaginary parts of the bandpass
    coefficients independently (the second row of both tables) almost always
    performs worse than having no nonlinearity (first row of both tables).
  \item The best combination is to have batch normalization and a ReLU applied
    to the magnitudes of the bandpass coefficients and batch norm and a ReLU
    applied to either the lowpass or pixel coefficients with no nonlinearity in
    the pixel domain.
\end{enumerate}

The best accuracy score of $65.3\%$ is now $0.1\%$ lower than the fully
convolutional architecture, an improvement from the $62.8\%$ score achieved with
only a pixel nonlinearity. This happens when there is no pixel nonlinearity, use
a ReLU on the lowpass coefficients and Batch Normalization and a ReLU on the
magnitudes of the bandpass coefficients.

\subsection{Ablation Experiments with Nonlinearities}
Now that we have found the best nonlinearity to use for the gain layer,
will this improve our ablation study from \autoref{sec:ch6:ablation}? To test this, 
we repeat the same experiment on CIFAR-100 using the newly found nonlinearities
in \autoref{alg:ch6:wavelayer} (i.e., $\sigma_{pixel} = \F{None},\ \sigma_{lp} =
\F{ReLU},\ \text{and } \sigma_{bp} = \F{BN+ReLU}$).

See \autoref{fig:ch6:nonlinear_ablation} for the results from these experiments.
When we use the wavelet-based nonlinearities, the results change considerably. 
We see an improvement by $1\%$ when the first layer in the CNN is
changed for a wave layer, but any other changes degrade performance from this.

\begin{figure}[tb]
  \centering
  \includegraphics[width=\textwidth]{\imgpath/nonlinear_ablation.pdf}
  \mycaption{CIFAR-100 Ablation results with the \emph{wave layer}}{We use the
  same naming scheme from \autoref{sec:ch6:ablation} but to differentiate
  between the results from \autoref{fig:ch6:cifar100_gl} we call the options
  `waveX'. Results show the mean of 3 runs with $\pm 1$ standard deviation lines
  in dark blue.
  When we add nonlinearities in the wavelet domain, the ablation
  results change dramatically. It appears that learning in the wavelet domain
  works best for the first layer of the CNN (wave1), and this improves on the
  purely convolutional method by a whole percentage point.  Replacing the second
  and third layers degrades performance independently of what was used in the
  first layer. Swapping the first two layers (wave1\_2) performs nearly as well
  and with a slightly narrower spread.}
  \label{fig:ch6:nonlinear_ablation}
\end{figure}

