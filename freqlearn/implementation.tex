\section{Implementation Details}

We use our PyTorch Wavelets package \cite{} to perform the $\DTCWT$. As
mentioned in \autoref{sec:ch6:multiple_subbands}, our layer requires a set
of learnable filters:
\begin{equation}
  \{ g_{lp}, g_{j,k} \}_{1\leq j \leq J, 1\leq k\leq 6}
\end{equation}
where each of these filters is a four dimensional array as is standard for a
normal convolutional layer. The lowpass coefficients of a $\DTCWT$ are real, and
therefore, $g_{lp}$ is also real. In contrast, the bandpass coefficients are
complex, and so to get the full use of the phase information of the $\DTCWT$, we
also learn complex bandpass filters. This comes with an overhead which we will 
fully quantify in a later section.

Once we have decided on the number of scales in the $\DTCWT$ to take, and the
spatial support.taken the forward transform

\include{\imgpath/block_diagram}. 

We now also must choose the spatial sizes of both the lowpass and bandpass
mixing kernels. In our work, we set the spatial support of the complex bandpass
filters $g_{j,k}$ to be $1\x 1$ and the lowpass filters $g_{lp}$ to be $3\x 3$.
Further, we limit ourselves initially to only considering a single scale
transform.

If we wish, we can learn larger spatial sizes to have more complex
attenuation/magnification of the subbands. We also can use more than
one wavelet scale. 

\subsection{Parameter Memory Cost}
Again considering a two scale transform --- instead of learning $w\in \reals[F
\x C\x K\x K]$ we learn complex gains at the two scales, and a real gain for the
real lowpass:

\begin{eqnarray*}
  g_1 &\in& \complexes[F\x C\x 6\x 1\x 1] \\
  g_2 &\in& \complexes[F\x C\x 6\x 1\x 1] \\
  g_{lp} &\in& \reals[F\x C\x 1\x 1]
\end{eqnarray*}

We have set the spatial dimension to be $1\x 1$ to show that this gain is
identical to a $1\x 1$ convolution over the complex wavelet coefficients. If we
wish, we can learn larger spatial sizes to have more complex
attenuation/magnification of the subbands. We also can use more/fewer than
2 wavelet scales.  At first glance, we have increased our parameterization by
a factor of 25 (13 subbands, of which all but the lowpass are complex), but
each one of these gains affects a large spatial size. For the first scale, the
effective size is about $5\x 5$ pixels, for the second scale it is about $15\x
15$.

\subsection{Computational Cost}
A standard convolutional layer needs $K^2 F$ multiplies per input pixel (of
which there are $C\x H\x W$). In comparison, the wavelet gain method does a set
number of operations per pixel for the forward and inverse transforms, and then
applies gains on subsampled activations. For a 2 level $\DTCWT$ the transform
overhead is about 60 multiplies for both the forward and inverse transform. It
is important to note that unlike the filtering operation, this does not scale
with $F$. The learned gains in each subband do scale with the number of output
channels, but can have smaller spatial size (as they have larger effective
sizes) as well as having fewer pixels to operate on (because of the decimation).
The end result is that as $F$ and $C$ grow, the overhead of the $C$ forward and
$F$ inverse transforms is outweighed by cost of $FC$ mixing processes, which
should in turn be significantly less than the cost of $FC$ $K\x K$ standard
convolutions for equivalent spatial sizes.


\subsection{Examples}
\autoref{fig:ch6:example_impulses} show example impulse responses of our layer.
These impulses were generated by randomly initializing both the real and
imaginary parts of $g_2 \in \complexes[6\x 1\x 1]$ from $\mathcal{N}(0,1)$ and
$g_1, g_{lp}$ are set to 0. I.e. each shape has 12 random variables. It is good
to see that there is still a large degree of variability between shapes. Our
experiments have shown that the distribution of the normalized cross-correlation
between 512 of such randomly generated shapes matches the distribution for
random vectors with roughly 11.5 degrees of freedom.

\begin{algorithm}[t]
\caption{$\DTCWT$ gain layer forward and backward passes}\label{alg:ch6:gain}
\begin{algorithmic}[1]
\Procedure{GAINFWD}{$x, w_l, $}
  \State $yl,\ yh \gets \DTCWT(x^l, \mbox{nlevels}=1) $ \Comment{yh has 6
  orientations and is complex}
  \State $U \gets \F{COMPLEXMAG}(yh)$
  \State $yl \gets \F{AVGPOOL2x2}(yl)$  \Comment{Downsample and recentre lowpass
  to match U size}
  \State $Z \gets \F{CONCATENATE}(yl,\ U)$ \Comment{concatenated along the
  channel dimension}
  \State $Y \gets AZ$ \Comment{Mix}
  \State \textbf{save} $Z$ \Comment{For the backwards pass}
  \State \textbf{return} $Y$ 
\EndProcedure
\end{algorithmic}\vspace{10pt}
\begin{algorithmic}[1]
\Procedure{GAINBWD}{$\dydx{L}{Y},\ A$}
  \State \textbf{load} $Z$
  \State $\dydx{L}{A} \gets \dydx{L}{Y} Z^T$ \Comment{The weight gradient}
  \State $\Delta{Z} \gets A^T\dydx{L}{Y}$
  \State $\Delta yl,\ \Delta U \gets \F{UNSTACK}(\Delta Z)$ 
  \State $\Delta yl \gets \F{AVGPOOL2x2BWD}(\Delta yl)$
  \State $\Delta yh \gets \F{COMPLEXMAGBWD}(\Delta U)$
  \State $\dydx{L}{x} \gets \F{\DTCWT BWD}(\Delta yl,\ \Delta yh)$ \Comment{The propagated gradient}
  \State \textbf{return} $\dydx{L}{x},\ \dydx{L}{A}$
\EndProcedure
\end{algorithmic}
\end{algorithm}

