\section{Experiments}\label{sec:ch6:results}
To explore the effectiveness of our gain layer, we do a similar ablation study
to \autoref{sec:ch5:conv_exp} on CIFAR-10, CIFAR-100 and Tiny ImageNet. We use
the same reference network so that we can also compare the wavelet gain layer to the
invariant layer (see \autoref{tab:ch5:cifar_tiny_arch}). We also use the same
naming technique, calling a network `gainX' means that the `convX' layer was
replaced with a wavelet gain layer, but otherwise keeping the rest of the
architecture the same. 

We train all our networks for with stochastic gradient descent with momentum.
The initial learning rate is $0.5$, momentum is $0.85$, batch size $N=128$ and
weight decay is $10^{-4}$. For CIFAR-10/CIFAR-100 we scale the learning rate by
a factor of 0.2 after 60, 80 and 100 epochs, training for 120 epochs in total.
For Tiny ImageNet, the rate change is at 18, 30 and 40 epochs (training for 45 in total).

\input{\path/cifar_ablation_table}

\autoref{tab:ch6:ablation_results} lists the results from these experiments.
These show a promising start to this work. We can get an improvement in accuracy
by over a percent when we use the gain layer \textbf{Finish me when results are
done}.

Unlike the scatternet inspired invariant layer from the previous chapter, the
gain layer does naturally downsample the output, so we are able to stack more of
them? Maybe.

