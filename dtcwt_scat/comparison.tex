\section{Comparisons}\label{sec:ch3:comparison}
Now that we have the ability to do a $\DTCWT$ based ScatterNet, how does this
compare with the original Matlab implementation \cite{oyallon_deep_2015} and the newly developed
KyMatIO \cite{andreux_kymatio:_2018}? \autoref{tab:ch3:scat_props} lists the different properties and
options of the competing packages.

\subsection{Speed}
{%
\renewcommand{\_}{\textscale{.6}{\textunderscore}}
We test the speed of the various packages on our reference architecture (see \autoref{app:arch})
with a moderately large input with $128 \x 3\x 256\x 256$ pixels. The CPU experiments
used all cores available, whereas the GPU experiments ran on a single GPU\@. We
include two permutations of our proposed ScatterNet with different length
filters and different padding schemes. Type A uses the long `near\_sym\_b' filters
and has symmetric padding, and Type B uses shorter `near\_sym\_a' filters and the faster
zero padding scheme. We compare it to a Morlet based implementation with $K=6$
orientations (the same number of orientations as in the $\DTCWT$).

See \autoref{tab:ch3:scat_speeds} for the execution time results. Type A
is {\raise.17ex\hbox{$\scriptstyle\mathtt{\sim}$}}7
times faster than the Fourier-based KyMatIO\@ on GPUs and Type B
has a 
{\raise.17ex\hbox{$\scriptstyle\mathtt{\sim}$}}14 times speedup over the Morlet backend.

Additionally, when compared with version 0.1.0 of KyMatIO, the memory footprint of the
$\DTCWT$ based implementation is only $2\%$ of KyMatIO's, highlighting the
importance of being explicit in not saving unnecessary information for
backpropagation.
}

\begin{table}[t]
  \centering
  \mycaption{Comparison of properties of different ScatterNet packages}{In
  particular, the wavelet backend used, the number of orientations available,
  the available boundary extension methods, whether it has GPU support and
  whether it supports backpropagation.}
  {\renewcommand{\arraystretch}{1.2}
  \begin{tabular}{@{}llllll}
    \toprule
    Package & Backend & Orientations & Boundary Ext. & GPU & Backprop \\\midrule
    ScatNetLight\cite{oyallon_deep_2015} & {FFT-based} & Flexible & Periodic & No & No \\
    KyMatIO\cite{andreux_kymatio:_2018} & {FFT-based}& Flexible & Periodic & Yes & Yes \\
    $\DTCWT$ Scat & {Separable filter banks} & 6 & Flexible & Yes & Yes \\
    \bottomrule
  \end{tabular}\label{tab:ch3:scat_props}
  }
% \begin{table}[t]
  \newline\newline
  \renewcommand{\arraystretch}{1.2}
  \centering
  \mycaption{Comparison of execution time for the forward and backward
  passes of the competing ScatterNet implementations}{Tests were run on the reference
  architecture described in \autoref{app:arch}. The input for these experiments
  is a batch of images of size $128\x 3\x 256\x 256$ in 4 byte floating
  precision. We list two different types of options for our ScatterNet. Type A
  uses 16 tap filters and has symmetric padding, whereas type B uses 6 tap
  filters and uses zero padding at the image boundaries. Values are given to 2
  significant figures, averaged over 5 runs.}
  \begin{tabular}{@{}lcrrcrr}
    \toprule
    Package &\phantom{ab} & \multicolumn{2}{c}{CPU} && \multicolumn{2}{c}{GPU} \\\cline{3-4}\cline{6-7}
    && Fwd (s) & Bwd (s) && Fwd (s) & Bwd (s) \\\midrule
    ScatNetLight\cite{oyallon_deep_2015}&& $>200.00$ & n/a && n/a & n/a \\
    KyMatIO\cite{andreux_kymatio:_2018}&& 95.0 & 130.0 && 1.44 & 2.5 \\
    $\DTCWT$ Scat Type A && 3.3 & 3.6 && 0.21 & 0.27 \\
    $\DTCWT$ Scat Type B && 2.8 & 3.2 && 0.10 & 0.16 \\\bottomrule
  \end{tabular}\label{tab:ch3:scat_speeds}
% \end{table}
\end{table}


\subsection{Performance}
\begin{table}[ht]
  \renewcommand{\arraystretch}{1.4}
  \centering
  \mycaption{Hybrid architectures for performance comparison}{Comparison of
  Morlet-based ScatterNets (Morlet6 and Morlet8) to the $\DTCWT$-based
  ScatterNet on CIFAR\@. The output after scattering has $3(K+1)^2$ channels (243 for 8
  orientations or 147 for 6 orientations) of spatial size $8\x 8$. This is
  passed to 4 convolutional layers of width $C=192$ before being average pooled
  and fed to a single fully connected classifier. $N_c=10$ for CIFAR-10 and
  $100$ for CIFAR-100. In the $\DTCWT$ architecture, we test different padding
  schemes and wavelet lengths.}
  \begin{tabular}{clclc}
    \toprule
    Morlet8 &\phantom{ab}& Morlet6 &\phantom{ab}& $\DTCWT$ \\\midrule
    \makecell{Scat \\ $J=2,\ K=8,\ m=2$ \\$y\in \reals[243\x8\x8]$} &&
    \makecell{Scat\\ $J=2,\ K=6,\ m=2$ \\$y\in \reals[147\x8\x8]$} &&
    \makecell{Scat\\ $J=2,\ K=6,\ m=2$ \\$y\in \reals[147\x8\x8]$} \\\cmidrule{1-1}\cmidrule{3-5}
    conv1, $w \in \reals[C\x 243\x 3\x 3]$ && \multicolumn{3}{c}{conv1, $w \in
    \reals[C\x 147\x 3\x 3]$} \\\midrule
    \multicolumn{5}{c}{conv2, $w\in \reals[C\x C\x 3\x 3]$}\\
    \multicolumn{5}{c}{conv3, $w\in \reals[2C\x C\x 3\x 3]$}\\
    \multicolumn{5}{c}{conv4, $w\in \reals[2C\x 2C\x 3\x 3]$}\\
    \multicolumn{5}{c}{avg pool, $8\x 8$}\\
    \multicolumn{5}{c}{fc, $w\in \reals[2C\x N_c]$} \\\bottomrule
  \end{tabular}\label{tab:ch3:scat_arch}
% \end{table}
% \begin{table}[t]
  \newline\newline
  \renewcommand{\arraystretch}{1}
  \centering
  \mycaption{Performance comparison for a $\DTCWT$-based vs.\
  a Morlet-based ScatterNet}{We report top-1 classification accuracy for the 3
  listed datasets as well as training time for each model in hours.}
  \label{tab:ch3:comparison}
  \begin{tabular}{lcrrcrrcrr}
    \toprule
    Type & \phantom{abc} & \multicolumn{2}{c}{CIFAR-10} && \multicolumn{2}{c}{CIFAR-100}
         && \multicolumn{2}{c}{Tiny ImgNet} \\\cmidrule{3-4}\cmidrule{6-7}\cmidrule{9-10}
         && Acc.\ (\%) & Time (h) && Acc.\ (\%) & Time (h) && Acc.\ (\%) & Time (h)\\\midrule
    Morlet8 && 88.6 & 3.4 && 65.3 & 3.4 && 57.6 & 5.6 \\
    Morlet6 && 89.1 & 2.4 && 65.7 & 2.4 && 57.5 & 4.4 \\
    $\DTCWT$ && 89.8 & 1.1 && 66.2 & 1.1 && 57.3 & 2.7 \\\bottomrule
    % \toprule
    % Type & \phantom{abc} & CIFAR-10 & CIFAR-100 & Tiny ImgNet \\ \midrule
    % Morlet8 && & &\\
    % Morlet6 && \\
    % $\DTCWT$ && & 66.2 & 57.2 \\\bottomrule
  \end{tabular}
\end{table}
To confirm that changing the ScatterNet core has not impeded the
performance of the ScatterNet as a feature extractor, we build a simple Hybrid
ScatterNet, similar to \cite{oyallon_scaling_2017}. Our net 
has a second-order scattering transform before four convolutional layers. See \autoref{tab:ch3:scat_arch}
for the network layout.
We use the optimal hyperparameter choices from the previous section, and compare
these to Morlet based ScatterNet with 6 and 8 orientations.

We run tests on the following datasets:
\begin{itemize}
  \item CIFAR-10: 10 classes, 5000 images per class, $32\x 32$ pixels per image.
  \item CIFAR-100: 100 classes, 500 images per class, $32\x 32$ pixels per image.
  \item Tiny ImageNet\cite{li_tiny_2017}: 200 classes, 500 images per class,
    $64 \x 64$ pixels per image.
\end{itemize}

The images in Tiny ImageNet are four times the size, so the output after
scattering is $16\x 16$. We add a max pooling layer after conv4, followed
by two more convolutional layers conv5 and conv6, before average pooling.

These networks are optimized with SGD with momentum. The
initial learning rate is $0.5$, momentum is $0.85$, batch size $N=128$ and
weight decay is $10^{-4}$. For CIFAR-10/CIFAR-100 we scale the learning rate by
a factor of 0.2 after 60, 80 and 100 epochs, training for 120 epochs in total.
For Tiny ImageNet, the rate change is at 18, 30 and 40 epochs (training for 45 in total).
Our experiment code is available at \cite{cotter_learnable_2019-1}.

The results of this
experiment are shown in \autoref{tab:ch3:comparison}. It is promising to see
that the $\DTCWT$ based ScatterNet has not only not sped up, but slightly improved
upon the Morlet based ScatterNet as a frontend. Interestingly, both with Morlet
and $\DTCWT$ wavelets, 6 orientations performed better than 8, despite having
fewer parameters in conv1.
