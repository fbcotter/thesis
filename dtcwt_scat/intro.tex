\section{The Design Constraints}
The original authors implemented their ScatterNet in matlab \cite{} using
a Fourier based Morlet wavelet transform. 

The standard procedure for using ScatterNets in a deep learning
framework up until recently has been to:
\begin{enumerate}
  \item Pre scatter a dataset and store the features to disk. This can take
    several hours to several days depending on the size of the dataset and the
    number of CPU cores available.
  \item Build a network in another framework, usually Tensorflow \cite{}
    or Pytorch \cite{}.
  \item Load the scattered data from disk and train on it.
\end{enumerate} 
We saw that this approach was suboptimal for a number of reasons:
\begin{itemize}
  \item It was slow and needed to be run on CPUs.
  \item It was inflexible to any changes you wanted to investigate in the
    Scattering design; you would have to re-scatter all the data and save
    elsewhere on disk.
  \item You could not easily do preprocessing techniques like random shifts and
    flips, as each of these would change the scattered data.
  \item The scattered features were often larger than the original images, and 
    required you to store entire datasets twice (or more) times.
  \item The features were fixed and could only be used as a front end to any
    deep learning system.
\end{itemize}

To address these shortcomings, all of the above limitations became design
constraints. In particular, the new software should be:
\begin{itemize}
  \item Able to run on GPUs (ideally on multiple GPUs in parallel).
  \item Flexible and fast so that it could be run as part of the forward pass of
    a neural network (allowing preprocessing techniques like random shifts and
    flips).
  \item Able to pass gradients through, so that it could be part of a larger
    network and have learning stages before scattering.
\end{itemize}

To achieve all of these goals, we choose to build our software on PyTorch, 
a popular open source deep learning framework that can do many operations on
GPUs with native support for automatic differentiation. PyTorch uses the CUDA 
and cuDNN \cite{} libraries for its GPU-accelerated primitives. Its popularity
is of key importance, as it means users can build complex networks involving
ScatterNets without having to use or learn extra software.

As mentioned earlier, the original authors of the ScatterNet also noticed the
shortcomings with their Scattering software, and recently released a new package
that could do Scattering in PyTorch \cite{kymatio} addressing the above design
constraints. The key difference between our proposed and their
improved packages is the use of the $\DTCWT$ as the core (their software still
uses the original Morlet wavelet design).

As part of our explanation we give pseudo code for the forward and backward
passes of all our key layers. This will help us later when we want to build on
simpler designs.
