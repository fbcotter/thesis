\section{The Design Constraints}
\subsection{Original Design}
The original authors implemented their ScatterNet in Matlab \cite{oyallon_deep_2015} using
a Fourier-domain based Morlet wavelet transform.
The standard procedure for using ScatterNets in a deep learning
framework up until recently has been to:
% \begin{enumerate}
\begin{enumerate}
  \item Pre-scatter a dataset using conventional CPU-based hardware and software
    and store the features to disk. This can take several hours to several days
    depending on the size of the dataset and the number of CPU cores available.
  \item Build a network in another framework, usually Tensorflow \cite{abadi_tensorflow:_2015}
    or Pytorch \cite{paszke_automatic_2017}.
  \item Load the scattered data from disk and train on it.
\end{enumerate}
We saw that this approach was suboptimal for a number of reasons:
% \begin{itemize}
\begin{itemize}
  \item It is slow and must run on CPUs.
  \item It is inflexible to any changes you wanted to investigate in the
    Scattering design; you would have to re-scatter all the data and save
    elsewhere on disk.
  \item You can not easily do preprocessing techniques like random shifts and
    flips, as each of these would change the scattered data.
  \item The scattered features are often larger than the original images and
    require you to store entire datasets twice (or more) times.
  \item The features are fixed and can only be used as a front end to any
    deep learning system.
\end{itemize}

\subsection{Improvements}
To address these shortcomings, all of the above limitations become design
constraints. In particular, the new software should be:
% \begin{itemize}
\begin{itemize}
  \item Able to run on GPUs (ideally on multiple GPUs in parallel).
  \item Flexible and fast so that it can run as part of the forward pass of
    a neural network (allowing preprocessing techniques like random shifts and
    flips).
  \item Able to pass gradients through, so that it can be part of a larger
    network and have learning stages before scattering.
\end{itemize}

To achieve all of these goals, we choose to build our software on PyTorch using
the $\DTCWT$. PyTorch is a popular open-source deep learning framework that can
do many operations on GPUs with native support for automatic differentiation.
PyTorch uses the CUDA and cuDNN libraries for its GPU-accelerated primitives.
Its popularity is of key importance, as it means users can build complex
networks involving ScatterNets without having to use or learn extra software.
