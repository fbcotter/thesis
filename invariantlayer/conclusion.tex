\section{Conclusion}\label{sec:ch5:conclusion}
In this work, we have proposed a new learnable scattering layer, dubbed the
locally invariant convolutional layer, tying together ScatterNets and CNNs.

The invariant layer takes a single scale complex wavelet decomposition of the
input. The bandpass coefficients are demodulated by using the complex modulus
nonlinearity. The resulting magnitude coefficients are then mixed with the
lowpass coefficients with a learnt mixing matrix (or $1\x 1$ convolution). 

We tested the invariant layer initially on MNIST and proved that it could
achieve comparable performance to a convolutional layer.
We did see some issues with using the invariant layer on its own, as the $1\x 1$
convolution may not have been large enough to separate the centres of the
wavelets to make the complex shapes necessary. When we used larger kernels,
either in the mixing matrix $A$ (see \autoref{sec:ch5:mnist_newlayer}) or in
convolutional layers after the invariant layer (see \autoref{sec:ch5:conv_exp})
the performance improved.

Our ablation studies on a VGG-like CNN showed that the invariant layer can
easily be shaped to allow it to drop in the place of a convolutional layer,
theoretically saving on parameters and computation (see \autoref{sec:ch5:conv_exp}).
However, care must be taken when doing this, as our ablation study showed that
the layer only improves upon regular convolution at certain depths. Typically,
it seems wise to use the invariant layer early in the network, but \emph{after} the
first layer. This is an interesting discovery, as typically other ScatterNet
approaches use them as a front end \cite{oyallon_scaling_2017, singh_scatternet_2017}. 
The invariant layer naturally
downsamples the input, so it works well when replacing a convolutional layer
followed by pooling, or a pooling layer followed by a convolution. 

We also tested the invariant layer on a hybrid ScatterNet architecture (see
\autoref{sec:ch5:scat_exp}). We saw that two invariant layers worked well as the
first two layers of a deep CNN, but performed even better when used as the
second and third layers, with a small learned convolutional before them. These
hybrid ScatterNets downsample the input by a factor of 16 and increasing the
channel dimension by 49. The reduced spatial size meant the networks were very
quick to train, yet were able to achieve near state-of-the-art performance.

% There is still much research to do - why does the proposed layer work best near,
% but not at the beginning of deeper networks? Why is it beneficial to precede an
% invariant layer with a convolutional layer? Can we combine invariant layers in
% Residual style architectures? The work presented here is still
% nascent but we hope that it will stimulate further interest and research into both
% ScatterNets and the design of convolutional layers in CNNs.

