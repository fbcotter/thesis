\section{Implementation Details}\label{sec:implementation}
Again, we use the $\DTCWT$ \cite{selesnick_dual-tree_2005} for our wavelet filters
$\psi_{j, \theta}$ due to their fast implementation with separable convolutions
which we will discuss more in \autoref{sec:computation}.  There are two side
effects of this choice. The first is that the number of orientations of wavelets
is restricted to $K=6$. The second is that we naturally downsample the output
activations by a factor of $2^j$ in each direction for each scale $j$. This represents the
source of the invariance in our layer. If we do not wish to downsample the
output (say to make the layer fit in a larger network), we can bilinearly
interpolate the output of our layer. This is computationally cheap to do on its
own, but causes the next layer's computation to be higher than necessary (there
will be no energy for frequencies higher than $f_s/4$).

In all our experiments we set $J=1$ for each invariant layer,
meaning we can mix the lowpass and bandpass coefficients at the same resolution.
\autoref{fig:block_diagram} shows how this is done. Note that setting $J=1$ for
a single layer does not restrict us from having $J>1$ for the entire system, as
if we have a second layer with $J=1$ after the first, including downsampling
($\downarrow$), we would have:
%
\begin{equation}
  \left(\left(\left(x \conv \phi_1\right) \downarrow 2\right) \conv \psi_{1, \theta}\right) \downarrow 2 = \left(x \conv \psi_{2, \theta}\right) \downarrow 4
\end{equation}

\subsection{Memory Cost}\label{sec:memory}
A standard convolutional layer with $C_l$ input channels, $C_{l+1}$ output channels
and kernel size $L$ has $L^2C_{l}C_{l+1}$ parameters. 

The number of learnable parameters in each of our proposed invariant layers with
$J=1$ and $K=6$ orientations is:
%
\begin{equation}
  \text{\#params} = (JK+1)C_{l}C_{l+1} = 7C_{l}C_{l+1}
\end{equation} 
%
The spatial support of the wavelet filters is typically $5\x 5$ pixels or more,
and we have reduced $\text{\#params}$ to less than $3\x3$ per filter, while
producing filters that are significantly larger than this.

\subsection{Computational Cost}\label{sec:computation}
A standard convolutional layer with kernel size $L$ needs $L^2C_{l+1}$
multiplies per input pixel (of which there are $C_{l}\x H\x W$).

As mentioned in \autoref{sec:memory}, we use the $\DTCWT$ for our complex, shift
invariant wavelet decomposition. We use the open source Pytorch implementation
of the $\DTCWT$ \cite{cotter_pytorch_2018} as it can run on GPUs and
has support for backpropagating gradients.

There is an overhead in doing the wavelet decomposition for each input channel. A
regular discrete wavelet transform (DWT) with filters of length $L$ will have
$2L\left(1-2^{-2J}\right)$ multiplies for a $J$ scale decomposition. A $\DTCWT$
has 4 DWTs for a 2-D input, so its cost is $8L\left(1-2^{-2J}\right)$, with
$L=6$ a common size for the filters. It is important to note that unlike the
filtering operation, this does not scale with $C_{l+1}$, the end result being that as
$C_{l+1}$ grows, the cost of $C_l$ forward transforms is outweighed by that of the mixing
process.

Because we are using a decimated wavelet decomposition, the sample rate decreases after each
wavelet layer. The benefit of this is that the mixing process then only works on
one quarter the spatial size after one first scale and one sixteenth the spatial
after the second scale. Restricting ourselves to $J=1$ as we mentioned in
\autoref{sec:implementation}, the computational cost is then:

\begin{equation}
  % \frac{7}{4}C_{l+1} + 48 \label{eq:comp}
  \underbrace{ \frac{7}{4}C_{l+1} }_{\textrm{mixing}} +
  \underbrace{\vphantom{\frac{7}{4}} 36}_{\DTCWT} \quad
  \textrm{multiplies per input pixel}\label{eq:comp}
\end{equation}
In most CNNs, $C_{l+1}$ is several dozen if not several
hundred, which makes \autoref{eq:comp} significantly smaller than
$L^2C_{l+1}=9C_{l+1}$ multiplies for $3\x 3$ convolutions.

\subsection{Forward and Backward Algorithm}
There are several hyperparameters to choose in our layer, in particular:
\begin{itemize}
  \item The number of output channels $C_{l+1}$. This may be restricted by the
    architecture.
  \item The variance of the weight initialization for the mixing matrix $A$.
  \item The optimizer parameters such as learning rate, momentum and weight
    decay.
\end{itemize}

These are also considerations for a regular CNN layer as well, so we can use
previous techniques to guide our choices here. We will explore the effects of
these choices in a later section. Assuming we have already chosen
hyperparameters then the forward and backward algorithms can be computed with
Algorithms~\autoref{alg:ch5:invfwd} and \autoref{alg:ch5:invbwd} respectively.

\begin{algorithm}[t]
\caption{Locally Invariant Convolutional Layer forward
pass}\label{alg:ch5:invfwd}
\begin{algorithmic}[1]
\Procedure{LOCALINV}{$x, A$}
  \State $yl,\ yh \gets \DTCWT(x^l, \mbox{nlevels}=1) $ \Comment{yh has 6
  orientations and is complex}
  \State $U \gets \F{COMPLEXMAG}(yh)$
  \State $yl \gets \F{AVGPOOL2x2}(yl)$  \Comment{Downsample and recentre lowpass
  to match U size}
  \State $Z \gets \F{CONCATENATE}(yl,\ U)$ \Comment{concatenated along the
  channel dimension}
  \State $Y \gets AZ$ \Comment{Mix}
  \State \textbf{save} $Z$ \Comment{For the backwards pass}
  \State \textbf{return} $Y$ 
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[t]
\caption{Locally Invariant Convolutional Layer backward
pass}\label{alg:ch5:invbwd}
\begin{algorithmic}[1]
\Procedure{LOCALINVBWD}{$\dydx{L}{Y},\ A$}
  \State \textbf{load} $Z$
  \State $\dydx{L}{A} \gets \dydx{L}{Y} Z^T$
  \State $\Delta{Z} \gets A^T\dydx{L}{Y}$
  \State $\Delta yl,\ \Delta U \gets \F{UNSTACK}(\Delta Z)$ 
  \State $\Delta yl \gets \F{AVGPOOL2x2BWD}(\Delta yl)$
  \State $\Delta yh \gets \F{COMPLEXMAGBWD}(\Delta U)$
  \State $\dydx{L}{x} \gets \F{\DTCWT BWD}(\Delta yl,\ \Delta yh)$
  \State \textbf{return} $\dydx{L}{x},\ \dydx{L}{A}$
\EndProcedure
\end{algorithmic}
\end{algorithm}
