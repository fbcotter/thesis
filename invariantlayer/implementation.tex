\section{Implementation Details}\label{sec:ch5:implementation}
Again, we use the $\DTCWT$ \cite{selesnick_dual-tree_2005} for our wavelet filters
due to their fast implementation with separable convolutions
which we discuss more in \autoref{sec:ch5:computation}. There are two side
effects of this choice. The first is that the number of orientations of wavelets
is restricted to $K=6$. The second is that we naturally downsample the output
activations by a factor of $2$ for each direction for each scale $j$, giving a 
$4^j$ downsampling factor overall. This represents the
source of the invariance in our layer. If we do not wish to downsample the
output (say to make the layer fit in a larger network), we can bilinearly
interpolate the output of our layer. This is computationally cheap to do on its
own, but causes the next layer's computation to be higher than necessary (there
will be almost no energy for frequencies higher than $f_s/4$).

In all our experiments we set $J=1$ for each invariant layer,
meaning we can mix the lowpass and bandpass coefficients at the same resolution.
\autoref{fig:ch5:block_diagram} shows how this is done. Note that setting $J=1$ for
a single layer does not restrict us from having $J>1$ for the entire system, as
if we have a second layer with $J=1$ after the first, including downsampling
($\downarrow$), we would have:
%
\begin{equation}
  \left(\left(\left(x \conv \phi_1\right) \downarrow 2\right) \conv \psi_{1, \theta}\right) 
    \downarrow 2 = \left(x \conv \psi_{2, \theta}\right) \downarrow 4
\end{equation}

\subsection{Parameter Memory Cost}\label{sec:ch5:memory}
A standard convolutional layer with $C_l$ input channels, $C_{l+1}$ output channels
and kernel size $L\x L$ has $L^2C_{l}C_{l+1}$ parameters. 

The number of learnable parameters in each of our proposed invariant layers with
$J=1$ and $K=6$ orientations is:
%
\begin{equation}
  \text{\#params} = (JK+1)C_{l}C_{l+1} = 7C_{l}C_{l+1} \label{eq:ch5:num_params}
\end{equation} 
%
The spatial support of the wavelet filters is typically $5\x 5$ pixels or more,
and we have reduced the number of parameters to fewer than $3\x3=9$ per filter, while
producing filters that are significantly larger than this.

\subsection{Activation Memory Cost}
A standard convolutional layer needs to save the activation $x^{(l)}$ to
convolve with the backpropagated gradient $\dydx{L}{y^{(l+1)}}$ on the backwards
pass (to give $\dydx{L}{w^{(l)}}$). For an input with $C_l$ channels of spatial size $H\x W$, this means
$HWC_l$ floats must be saved. 

Our layer requires us to save the activation
$z^{(l+1)}$ for updating the $\tilde{a}$ terms. This has $7C_l$ channels of
spatial size $\frac{HW}{4}$. This means that our proposed layer needs to save
$\frac{7}{4}HWC_l$ floats, a $\frac{7}{4}$ times memory increase on the standard
layer.

\begin{algorithm}[tb]
\caption{Locally Invariant Convolutional Layer forward and backward
passes}\label{alg:ch5:inv}
\begin{algorithmic}[1]
\Procedure{INVLAYER.Forward}{$x, A$}
\State $yl,\ yh \gets \F{\DTCWT.Forward}(x^l, \mbox{nlevels}=1) $ 
  \State $U \gets \F{MAG\_SMOOTH.Forward}(yh)$ \Comment{See \autoref{alg:appB:mag_smooth}}
  \State $yl \gets \F{AVGPOOL2x2}(yl)$  \Comment{Downsample lowpass to match U size}
  \State $Z \gets \F{CONCATENATE}(yl,\ U)$ \Comment{Concatenate along the channel dim}
  \State $Y \gets AZ$ \Comment{Mix}
  \State \textbf{save} $A, Z$ \Comment{For the backwards pass}
  \State \textbf{return} $Y$ 
\EndProcedure
\end{algorithmic}\vspace{10pt}
\begin{algorithmic}[1]
\Procedure{INVLAYER.Backward}{$\dydx{L}{Y}$}
  \State \textbf{load} $A, Z$
  \State $\dydx{L}{A} \gets \dydx{L}{Y} Z^T$ \Comment{Calculate update gradient}
  \State $\Delta{Z} \gets A^T\dydx{L}{Y}$ 
  \State $\Delta yl,\ \Delta U \gets \F{UNSTACK}(\Delta Z)$ 
  \State $\Delta yl \gets \F{AVGPOOL2x2.Backward}(\Delta yl)$
  \State $\Delta yh \gets \F{MAG\_SMOOTH.Backward}(\Delta U)$
  \State $\dydx{L}{x} \gets \F{\DTCWT.Backward}(\Delta yl,\ \Delta yh)$ \Comment{Calculate passthrough gradient}
  \State \textbf{return} $\dydx{L}{x},\ \dydx{L}{A}$
\EndProcedure
\end{algorithmic}
\end{algorithm}


\subsection{Computational Cost}\label{sec:ch5:computation}
A standard convolutional layer with kernel size $L\x L$ needs $L^2C_{l+1}$
multiplies per input pixel (of which there are $C_{l}\x H\x W$).

There is an overhead in doing the wavelet decomposition for each input channel.
A separable 2-D discrete wavelet transform (DWT) with 1-D filters of length $L$
will have $2L\left(1-2^{-2J}\right)$ multiplies per input pixel for a $J$ scale
decomposition. A $\DTCWT$ has 4 DWTs for a 2-D input, so its cost is
$8L\left(1-2^{-2J}\right)$, with $L=6$ a common size for the filters. It is
important to note that unlike the filtering operation, this does not scale with
$C_{l+1}$, the end result being that as $C_{l+1}$ grows, the cost of $C_l$
forward transforms is outweighed by that of the mixing process whose cost is
proportional to $C_l C_{l+1}$.

Because we are using a decimated wavelet decomposition, the sample rate
decreases after each wavelet layer. The benefit of this is that the mixing
process then only works on $1/4$ the spatial area after the first scale
and $1/16$ the spatial area after the second scale. Restricting ourselves to
$J=1$ as we mentioned in \autoref{sec:ch5:implementation}, the computational cost is
then:

\begin{equation}
  % \frac{7}{4}C_{l+1} + 48 \label{eq:comp}
  \underbrace{ \frac{7}{4}C_{l+1} }_{\textrm{mixing}} +
  \underbrace{\vphantom{\frac{7}{4}} 36}_{\DTCWT} \quad
  \textrm{multiplies per input pixel}\label{eq:ch5:comp}
\end{equation}
In most CNNs, $C_{l+1}$ is several dozen if not several
hundred, which makes \eqref{eq:ch5:comp} significantly smaller than
$L^2C_{l+1}=9C_{l+1}$ multiplies for $3\x 3$ convolutions.

\subsection{Forward and Backward Algorithm}
There are two layer-hyperparameters to choose:
\begin{itemize}
  \item The number of output channels $C_{l+1}$. This may be restricted by the
    architecture.
  \item The variance of the weight initialization for the mixing matrix $A$.
\end{itemize}

Assuming we have already chosen these values, 
then the forward and backward algorithms can be computed with
Algorithm~\autoref{alg:ch5:inv}.



