\section{Experiments}\label{sec:experiments}
In this section we examine the effectiveness of our invariant layer by testing
its performance on the well known datasets (listed in the order of increasing
difficulty):
\begin{itemize}
  \item MNIST: 10 classes, 6000 images per class, $28\x 28$ pixels per image.
  \item CIFAR-10: 10 classes, 5000 images per class, $32\x 32$ pixels per image.
  \item CIFAR-100: 100 classes, 500 images per class, $32\x 32$ pixels per image. 
  \item Tiny ImageNet\cite{li_tiny_nodate}: 200 classes, 500 images per class, 
    $64 \x 64$ pixels per image. 
\end{itemize}
Our experiment code is available at
\url{https://github.com/fbcotter/invariant_convolution}.

\subsection{Layer Introduction with MNIST}\label{sec:ch5:mnist}
To begin experiments on the proposed locally invariant layer, we look at how
well a simple system works on MNIST, and compare it to an equivalent system with
convolutions. To minimize the effects of learning from other layers, we build a
custom small network, as described in \autoref{tab:ch5:mnist_arch}.

The first two layers are learned convolutional/invariant layers, followed by
a fully connected layer with fixed weights that we can use to project down to
the number of output classes. Finally, we add a small learned layer that
linearly combines the 10 outputs from the random projection, to give 10 new
outputs. This is to facilitate reordering of the outputs to the correct class.
This simple network is meant to test the limits of our layer, rather than
achieve state of the art performance on MNIST.

Given that our layer is quite different to a standard convolutional layer, we
must do a full hyperparameter search over optimizer parameters such as the
learning rate, momentum, and weight decay, as well layer hyperparameters 
such as the variance of the random initialization for the mixing matrix $A$.

To simplify the weight variance search, we use Glorot Uniform Initialization
\cite{glorot_understanding_2010} and only vary the gain value $a$:
%
\begin{equation}
  A_{ij} \drawnfrom U\left[ -a\sqrt{\frac{6}{(C_l + C_{l+1})HW}},\ a\sqrt{\frac{6}{(C_l + C_{l+1})HW}}\
  \right] \label{eq:ch5:glorot}
\end{equation}
%
where $C_l,\ C_{l+1}$ are the number of input and output channels as before, and
the kernel size is $H = W = 1$ for an invariant layer and $H = W= 3$ for a
convolutional layer.

We do a grid search over these hyperparameters and use Hyperband
\cite{li_hyperband:_2016} to schedule early stopping of poorly performing runs.
Each run has a grace period of 5 epochs and can train for a maximum of 20
epochs. We do not do any learning rate decay.  We found the package Tune
\cite{liaw2018tune} was very helpful in organising parallel distributed training
runs.  
% Finally, we use fANOVA \cite{hutter_efficient_2014} analysis
% to find the importance of these hyperparameters to our layer, comparing to a
% standard layer. 
The hyperparameter options are described in
\autoref{tab:ch5:hyper_options}, note that we test $4^4=256$ different options.

Once we find the optimal hyperparameters for each network, we then run the two
architectures 10 times with different random seeds and report the mean and variance of
the accuracy. The results of these runs are listed in
\autoref{tab:ch5:mnist_initial_results}.

\input{\path/mnist_arch_table}

\input{\path/mnist_hyper_table}

\subsubsection{Proposed Expansions}\label{sec:ch5:mnist_newlayer}
The results from the previous section seem to indicate that our proposed
invariant layer is a slightly worse substitute for a convolutional layer.
However we believe that this is due to the centred nature of the wavelet bases
that were used to generate the $z$ and later the $y$ coefficients. A similar
effect was seen in the previous chapter in \autoref{fig:ch4:shapes} where the
space of shapes attainable by mixing wavelet coefficients in a $3\x 3$ area was
much richer than those attainable by only mixing in a $1\x 1$ area. 

To test this hypothesis, we change \autoref{eq:ch5:mixing} to be:
\begin{equation}
  y^{(l+1)}(f, \xy)  =  \sum_{q \in Q} z^{(l+1)}(q, \xy) \conv \left(\tilde{a}_f(q) \alpha_f(q, \xy) \right)
\end{equation}

Where $\alpha_f(q, \xy)$ is an introduced kernel designed to allow mixing of
wavelets from neighbouring spatial locations. We test a range of possible
$\alpha$'s each with varying complexity/overhead:

\begin{enumerate}[(a)]
  \item We randomly shift each of the $7C$ subbands horizontally
    by $\{-1, 0, 1\}$ pixels, and vertically by $\{-1, 0, 1\}$ pixels. This is
    determined at the beginning of a training session and is consistent between
    batches. This theoretically is free to do, but practically it involves
    convolving with a $3 \x 3$ kernel with a single $1$ and eight $0$'s.
  \item Instead of shifting impulses as in the previous option, we can shift a
    gaussian kernel by one pixel left/right and up/down, making a smoother filter. 
  \item Instead of imposing a lowpass/impulse structure, we can set $\alpha$ to
    be a random $3\x 3$ kernel. This is chosen once at the beginning of training and then
    is kept fixed between batches.
  \item We can set the $3\x 3$ kernel to be fully learned. This
    still makes for a novel layer, but now the parameter cost is 9 times higher
    than the $1\x 1$ conv layer, and 7 times higher than a vanilla $3\x 3$
    convolution.
  \item We can take the top three $3\x 3$ DCT coefficients of the $7C$
    subbands, allowing us to do something like the previous option 
    but with only a threefold parameter increase. The top three coefficients are
    the constant, the horizontal and the vertical filters.
\end{enumerate}

Again, we search over the hyperparameter space to find the optimal
hyperparameters and then run 10 runs at the best set of hyperparameters, and
report the results in \autoref{tab:ch5:mnist_new_results}. As we expected,
adding in random shifts significantly helps the invariant layer. Two systems of
note are the shifted impulse (a) system and the learned $3\x 3$ kernel (d)
system. The first improves the mean accuracy by $1.3\%$ without any extra
learning. The second improves the performance by $2.4\%$ but with a large
parameter cost. To explore an equivalent system, we also list in
\autoref{tab:ch5:mnist_new_results} a modification to the convolutional
architecture that uses $5\x 5$ convolutions and $C_1 = 10,\ C_2 = 100$ channels,
resulting in a system with comparable parameter cost to (d).

\input{\path/mnist_modified_results}

\subsection{Layer Comparison with CIFAR and Tiny ImageNet}\label{sec:conv_exp}
Now we look at expanding our layer to harder datasets, focusing more on the
final classification accuracy. We do this again by comparing to a reference
architecture. For this task, we choose a VGG-like network as our reference.
It has six convolutional layers for CIFAR and eight layers for Tiny ImageNet as shown in
\autoref{tab:ch5:cifar_arch}. The initial number of channels $C$ we use is 64. Despite
this simple design, this reference architecture achieves competitive performance
for the three datasets.

We perform an ablation study where we progressively swap out convolutional
layers for invariant layers keeping the input and output activation sizes the
same. As there are 6 layers (or 8 for Tiny ImageNet), there are too many
permutations to list the results for swapping out all layers for our locally
invariant layer, so we restrict our results to swapping 1 or 2 layers. We also
report the accuracy when we swap out all of the layers.
\autoref{tab:ch5:conv_results} reports the top-1 classification accuracies for
CIFAR-10, CIFAR-100 and Tiny ImageNet. In the table, `invX'
means that the `convX' layer from \autoref{tab:ch5:cifar_arch} was replaced with an 
invariant layer. If the convolutional layer is before a pooling layer, then we
do not interpolate the output of the invariant layer and we remove the pooling
layer. 

In addition to testing the original $1\x 1$ gain, we also report results for using 
the `shifted impulse' and `learned $3\x 3$' modified architectures from 
\autoref{sec:ch5:mnist_newlayer}. 

This network is optimized with stochastic gradient descent with momentum. The
initial learning rate is $0.5$, momentum is $0.85$, batch size $N=128$ and
weight decay is $10^{-4}$. For CIFAR-10/CIFAR-100 we scale the learning rate by
a factor of 0.2 after 60, 80 and 100 epochs, training for 120 epochs in total.
For Tiny ImageNet, the rate change is at 18, 30 and 40 epochs (training for 45 in total).

\input{\path/cifar_arch_table}

Interestingly, we see improvements when one or two invariant layers are used
near the start of a system, but not for the first layer. In particular, the best
position for the invariant layer seems to be just before a sample rate change.
Recalling that the magnitude operation in the ScatterNet effectively
demodulates the energy from higher spatial frequencies to a lower band, it
intuitively makes sense that good places for scattering layers are at
positions where you want to downsample. 

\input{\path/cifar_ablation_table}

\subsection{Network Comparison}\label{sec:scat_exp}
In the previous section, we examined how the locally invariant layer performs when
directly swapped in for a convolutional layer in an architecture designed
for convolutional layers. In this section, we look at how
it performs in a Hybrid ScatterNet-like \cite{oyallon_hybrid_2017,oyallon_scaling_2017},
network.

To build the second order ScatterNet, we stack two invariant layers on top of each
other. For 3 input channels, the output of these layers has $3(1 +
6 + 6 +36) = 147$ channels at $1/16$ the spatial input size. We then use 4
convolutional layers, similar to convC to convF in \autoref{tab:arch} with
$C=96$. In addition, we use dropout after these later convolutional layers with
drop probability $p=0.3$.

We compare a ScatterNet with no learning in between scattering orders
(ScatNet A) to one with our proposal for a learned mixing matrix $A$ (ScatNet B). Finally,
we also test the hypothesis seen from \autoref{sec:conv_exp} about putting conv
layers before an inv layer, and test a version with a small convolutional layer
before ScatNets A and B, taking the input from 3 to 16 channels, and call these ScatNet
architectures ScatNet C and D respectively.

See \autoref{tab:hybrid_scat} for results from these experiments. It is clear from
the improvements that the mixing layer helps the Scattering front end.
Interestingly, ScatNet C and D further improve on the A and B versions
(albeit with a larger parameter and multiply cost than the mixing operation). This reaffirms that there
may be benefit to add learning before as well as inside the ScatterNet.

For comparison, we have also listed the performance of other architectures as
reported by their authors in order of increasing complexity. Our proposed ScatNet D achieves
comparable performance with the the All Conv, VGG16 and FitNet architectures.
The Deep\cite{he_identity_2016} and Wide\cite{zagoruyko_wide_2016}
ResNets perform best, but with very many more
multiplies, parameters and layers.
%and
%being quick to train --- dropping the spatial size to $8\x 8$ on CIFAR after
%only two layers is unique to the ScatterNet design. 

ScatNets A to D with 6 layers like convC to convG from \autoref{tab:arch} after
the scattering, achieve $58.1, 59.6, 60.8$ and $62.1\%$ top-1 accuracy on Tiny ImageNet. As
these have more parameters and multiplies from the extra layers we exclude them
from \autoref{tab:hybrid_scat}.

\input{\path/cifar_scatnet_results}
