In this chapter, we explore tying together the ideas from Scattering Transforms
and Convolutional Neural Networks (CNN) for Image Analysis by proposing a learnable
ScatterNet. The work presented in \autoref{ch:visualizing} implies that while the Scattering 
Transform has been a promising start in using complex wavelets in image
understanding tasks, there is something missing from them. To address this, we
propose a learnable ScatterNet by building it with our proposed ``Locally Invariant
Convolutional Layers".

Previous attempts at combining ScatterNets with CNNs in hybrid networks
\cite{oyallon_scaling_2017, oyallon_hybrid_2017, singh_scatternet_2018} have
tended to keep the two parts separate, with the ScatterNet forming a fixed front
end and the CNN forming a learned backend. We instead look at adding learning
between scattering orders, as well as adding learned layers before the
ScatterNet. 

%
We do this by adding a second stage after a scattering order, which mixes output
activations together in a learnable way. The flexibility of the mixing we introduce allows
us to build a layer that acts as a Scattering Layer with no learning, or
as one that acts more as a convolutional layer with a controlled number of input
and output channels, or more interestingly, as a hybrid between the two. 

Our experiments show that these locally invariant layers can improve
accuracy when added to either a CNN or a ScatterNet.  We also discover some
surprising results in that the ScatterNet may be best positioned after one or
more layers of learning rather than at the front of a neural network.

\section{Chapter Layout}
In \autoref{sec:ch5:related} we discuss related work before briefly 
reviewing the convolutional layer and scattering notation we will use in \autoref{sec:ch5:background}.
We introduce our learnable scattering layers in \autoref{sec:ch5:method} and describe their
properties and implementation in \autoref{sec:ch5:implementation}.
Finally, we present some experiments we have run in \autoref{sec:ch5:experiments} and then
draw conclusions about how these new ideas might improve neural networks in the
future.
