\section{Introduction}

% We believe there is still much to explore in the way convolutional 
% filters are built and learned. 

% Current layers are randomly initialized and learned through backpropagation by
% minimizing a custom loss function.
% The properties and purpose of learned filters
% past the first layer of a CNN are not well understood, a deeply unsatisfying
% situation, particularly when we start to see problems in modern solutions such as significant redundancy
% \cite{denton_exploiting_2014} and weakness to adversarial attacks
% \cite{carlini_towards_2017}. 

% The Scattering Transform by Mallat et.\ al.\ \cite{mallat_group_2012, bruna_invariant_2013} 
% attempts to address the problems of poorly understood filtering layers 
% by using predefined wavelet bases whose properties are well known. Using
% this knowledge, Mallat derives bounds on the effect of 
% noise and deformations to the input. This work inspires us, but the
% fixed nature of ScatterNets has proved a limiting factor for them so far.


% To do this, we take inspiration from works like \cite{qiu_dcfnet:_2018,
% jacobsen_dynamic_2017, worrall_harmonic_2017, cotter_deep_2018, juefei-xu_local_2016},
% which decompose convolutional filters as a learned mixing of fixed harmonic
% functions. However we now propose to build a
% convolutional-like layer which is a learned mixing of the locally invariant
% scattering terms from the original ScatterNet \cite{bruna_invariant_2013}.

