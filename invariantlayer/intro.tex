\section{Introduction}

As we have seen from the work in the previous chapter, while Scattering is a
promising start at exploring if there is a connection with the field of image
coding, analysis and compression which led to the widespread development and use
of the wavelet transform, and the machine learning networks of today, the
network introduced by Mallat et al may be too restrictive. We would like to
relax this by allowing learning in between scattering orders, as well as being
more flexible to the number of input and output channels we use. 

We explore the sensitivity of our new layer to changes in hyperparameters by
doing a series of dummy experiments on MNIST. We are not concerned here with the
absolute accuracy of classification, as the network is designed to be naive (so
as to minimize the complex interactions between our proposed layer and other
layers in the system). Instead, we focus on the sensitivity and changes to
classification accuracy induced by slightly different hyperparameters. Further,
we look at the variance of classification accuracy on consecutive runs of the
same hyperparameters from slightly different initial conditions. In these
experiments, we compare our analysis to a single 3x3 convolutional layer.

Below are some tables and plots that show for this naive task, our system works best when initialized
with variance x, learned with learning rate y, and momentum z. The distribution
of scores for some sets of hyperparamaters is shown in figures 1 and 2,
alongside the distribution of scores for a convolutional layer for these same
hypers.

\textbf{here is a bigger plot of the real and imaginary parts with magnitude}

The first thing that is noticeable is the gap in accuracy between the
convolutional and invariant layer. While the task is naive, the CNN quite easily
performs a whole percent better than the invariant layer. Our intuition for this
is the way that the invariant layer is designed limits the spatial interactions
between pixels further afield. Of course, there is a $2^j$ spread in the peak of
the wavelets between the real and imaginary parts, but any displacement gets
centred by the effect of the magnitude operation. In comparison, a simple $3\x
3$ convolutional layer allows the network to learn differences between
neighbouring pixels more easily (I think I'd need to quantify this a bit
better). 

I don't know about this. think a bit harder.

To test this theory out without compromising the deliberate simplicity of the
network, we introduce a second non-learned layer to the invariant net. This goes
in between the two learned layers, and is a randomly initialized $3\x 3$
convolution. We could also use a set of predefined $3\x 3$ filters here, so long
as they allow for taking differences that are further afield. Again is this
quantifiable??

The results show that this closes the gap to a fraction of a percent between the
invariant layer and the convolutional layer. Further, the variance and
sensitivity to hyperparameters is plotted alongside the first 2.


The experiments run on CIFAR and Tiny Imagenet give some promising results for
the invariant layer. We saw again that it is needed to have some spatial
separation in between the scattering orders. Interestingly we also saw that it
is particularly helpful to add an extra layer at the front before Scattering.
This goes against all the research done so far into applying Scattering as a
front end to a deep system. 

But how much of this was just the extra width?
Difficult to tell. Maybe I can train a wider network too. Yeah, that shouldn't
be too hard. What about not learning that first layer?


Anyway.

How does this new layer compare with the previous non-learned scatternets and
CNNs in terms of the features it is sensitive to after x scales? To look at
this, we repeat the experiments of the previous chapter, but this time on the
Tiny Imagenet validation set. I.e. for ScatNets A-D as well as the reference
ConvNet, we visualize what causes large activations at certain depths of the network 
by deconvolving and descattering it.

Figures x and y show the results of doing this.

\subsection{Something else}
There is an inherent problem with this layer, which can be seen from algorithm
1. We need to save the intermediate activations before the mixing. While they
have a lower spatial size, the channel dimension is 7 times larger than the
input. We don't need to save the input activation anymore, but we have now
increased the memory load on the GPU from $x$ to $7/4 x$. Further, the delay has
increased from 1 convolution to 3 convolutions (2 for the separable \DTCWT and 1
for the mixing). In experiments, we saw this manifest itself as an $x\%$
increase in memory usage and a $y\%$ increase in cpu usage.

\subsection{Something Else}
We see from Figure 1a that the invariant layer has similar hyperparameter
sensitivity as a convolutional network, but it is significantly underperforming.
We believe that this may be a limitation of the $1\x 1$ convolution proposed in
the above analysis. In particular, having only $1\x 1$ convolutions does not
allow for the network to separate the centres of the wavelet responses. 


The results of these experiments are shown in Figure 1b.
% We believe there is still much to explore in the way convolutional 
% filters are built and learned. 

% Current layers are randomly initialized and learned through backpropagation by
% minimizing a custom loss function.
% The properties and purpose of learned filters
% past the first layer of a CNN are not well understood, a deeply unsatisfying
% situation, particularly when we start to see problems in modern solutions such as significant redundancy
% \cite{denton_exploiting_2014} and weakness to adversarial attacks
% \cite{carlini_towards_2017}. 

% The Scattering Transform by Mallat et.\ al.\ \cite{mallat_group_2012, bruna_invariant_2013} 
% attempts to address the problems of poorly understood filtering layers 
% by using predefined wavelet bases whose properties are well known. Using
% this knowledge, Mallat derives bounds on the effect of 
% noise and deformations to the input. This work inspires us, but the
% fixed nature of ScatterNets has proved a limiting factor for them so far.


% To do this, we take inspiration from works like \cite{qiu_dcfnet:_2018,
% jacobsen_dynamic_2017, worrall_harmonic_2017, cotter_deep_2018, juefei-xu_local_2016},
% which decompose convolutional filters as a learned mixing of fixed harmonic
% functions. However we now propose to build a
% convolutional-like layer which is a learned mixing of the locally invariant
% scattering terms from the original ScatterNet \cite{bruna_invariant_2013}.

