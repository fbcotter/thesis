% Include the figure here so it goes to the top of the third page
\section{Locally Invariant Layer} \label{sec:ch5:method}
We propose to mix the terms at the output of each wavelet modulus propagator
$\tilde{W}$. The second term in $\tilde{W}$, the $U$ terms are often called `covariant' terms but
in this work we will call them locally invariant, as they tend to be invariant up to a
scale $2^j$. We propose to mix the locally invariant terms $U$ and the
lowpass terms $S$ with learned weights $a_{f,\lambda}$ and $b_f$. 


For example,
consider the wavelet modulus propagator from \eqref{eq:ch5:wave_mod},
and let the input to it be $x^{(l)}$. Our proposed output is then:
%
\begin{eqnarray} \label{eq:ch5:comb1}
  y^{(l+1)}(f, \xy) & = & \sum_{\lambda} \sum_{c=0}^{C-1} \left|x^{(l)}(c, \xy)\conv \psi_\lambda(\xy)\right|\ a_{f, \lambda}(c) \nonumber \\
                    & + & \sum_{c=0}^{C-1} \left(x^{(l)}(c, \xy) \conv \phi_J(\xy)\right) b_f(c)
  \label{eq:ch5:layer}
\end{eqnarray}

Recall that $\lambda$ is the tuple $(j, k)$ for  $1 \leq j \leq J,\ 1 \leq k \leq K$
and is used to select the bandpass wavelet at scale $j$ and orientation $k$.
Note that an input to the wavelet modulus propagator $\tilde{W}$ with $C$ channels
has $(JK+1)C$ output channels -- $C$ lowpass channels and $JKC$ modulus bandpass
channels. Let us define a new output $z$ with index variable $q$ such that:
%
\begin{equation}
  z^{(l+1)}(q, \xy) =  \left\{
    \begin{array}{ll}
      x^{(l)}(c, \xy) \conv \phi_J(\xy) & \mbox{if } 0 \leq q < C \\
      |x^{(l)}(c, \xy)\conv \psi_\lambda(\xy)| & \mbox{if }	C \leq q < (JK+1)C
    \end{array}
    \right. \label{eq:ch5:z}
\end{equation}
i.e.\ the lowpass channels are the first $C$ channels of $z$, the modulus of the
$15\degs$ ($k=1$) wavelet coefficients with $j=1$ are the next $C$ channels,
then the modulus coefficients with $k=2$ and $j=1$ are the third $C$ channels,
and so on.

We do the same for the weights $a,\ b$ by defining $\tilde{a}_f = \{b_f, a_{f,
\lambda} \}_{\lambda}$ and let:
\begin{equation}
  \tilde{a}_f(q) =  \left\{
    \begin{array}{ll}
      b_f(c) & \mbox{if } 0 \leq q < C \\
      a_{f, \lambda}(c) & \mbox{if }	C \leq q < (JK+1)C
    \end{array}
    \right. \label{eq:ch5:tilde_a}
\end{equation}
%
we can then use \eqref{eq:ch5:z} and \eqref{eq:ch5:tilde_a} to simplify 
\eqref{eq:ch5:layer}, giving:
\begin{equation}
  y^{(l+1)}(f, \xy)  =  \sum_{q=0}^{(JK + 1)C - 1} z^{(l+1)}(q, \xy) \tilde{a}_f(q) \label{eq:ch5:mixing}
\end{equation}
or in matrix form with $A_{f,q} = \tilde{a}_f(q)$
%
\begin{equation}
  Y^{(l+1)}(\xy)  =  A Z^{(l+1)}(\xy) \label{eq:ch5:matrix}
\end{equation}

This is very similar to the standard convolutional layer from
\eqref{eq:ch5:conv}, except we have replaced the previous layer's $x$ with
intermediate coefficients $z$ (with $|Q| = (JK+1)C$ channels), and the
convolutions of \eqref{eq:ch5:conv} have been replaced by a matrix multiply
(which can also be seen as a $1\x 1$ convolutional layer). We can then apply
\eqref{eq:ch5:nonlin} to \eqref{eq:ch5:mixing} to get the next layer's output:

\begin{equation}
  x^{(l+1)}(f, \xy) = \sigma\left( y^{(l+1)}(f, \xy) \right)
  \label{eq:ch5:xnext}
\end{equation}

\autoref{fig:ch5:block_diagram} shows a block diagram for this process. 

\input{\imgpath/figure1}

\subsection{Properties}
\subsubsection{Recovering the original ScatterNet Design}
The first thing to note is that with careful choice of $A$ and $\sigma$, we can
recover the original translation invariant ScatterNet
\cite{bruna_invariant_2013, oyallon_scaling_2017}. If $C_{l+1} = (JK+1)C_l$ 
and $A$ is the identity matrix $I_{C_{l+1}}$, we remove the mixing and then $y^{(l+1)} = \tilde{W}x$.

Further, if $\sigma = \F{ReLU}$ as is commonly the case in training CNNs, it has
no effect on the positive locally invariant terms $U$. It will affect the averaging terms
if the signal is not positive, but this can be dealt with by adding a channel
dependent bias term $\alpha_c$ to $x^{(l)}$ to ensure it is positive. This bias term
will not affect the propagated signals as $\int \alpha_c \psi_\lambda(\xy) d\xy =
0$. The bias can then be corrected by subtracting $\alpha_c \norm{\phi_J}_2$ from
the averaging terms after taking the ReLU, then $x^{(l+1)} = \tilde{W}x$.

This makes one layer of our system equivalent to a first order scattering
transform, giving $S_0$ and $U_1$ (invariant to input shifts of $2^1$). Repeating the
same process for the next layer again works, as we saw in \eqref{eq:ch5:u_paths},
giving $S_1$ and $U_2$ (invariant to shifts of $2^2$).  If we want to build
higher invariance, we can continue or simply average these outputs with an average pooling
layer.

\subsubsection{Flexibilty of the Layer}
Unlike a regular ScatterNet, we are free to choose the size of $C_{l+1}$. This
means we can set $C_{l+1} = C_{l}$ as is commonly the case in a CNN, and make a
convolutional layer from mixing the locally invariant terms. This avoids the
exponentially increasing complexity that comes with extra network layers that 
standard ScatterNets suffer from.

\subsubsection{Stability to Noise and Deformations}
Let us define the action of our layer on the scattering
coefficients to be $Vx$. We would like to find a bound on $\norm{V\mathcal{L}_\tau x -
V x}$. To do this, we note that the mixing is a linear operator and hence is
Lipschitz continuous. The authors in \cite{qiu_dcfnet:_2018} find constraints on the mixing
weights to make them non-expansive (i.e. Lipschitz constant 1).
Further, the ReLU is non-expansive meaning the combination of the two is
also non-expansive, so $\norm{V\mathcal{L}_\tau x - V x} \leq \norm{S\mathcal{L}_\tau
x - Sx}$, and \eqref{eq:ch5:stability} holds.
% We would also like to ensure that we can derive a similar bound in stability to
% \eqref{eq:ch5:stability}. Let us call the action of our layer (with learned
% weights and nonlinearity) $Vx = \sigma(M[\gamma]\tilde{W}[\gamma]x)$. We would like to find a bound on
% $\norm{V\mathcal{L}_\tau x - V x}$. Fortunately, $M[\gamma]$ is a linear
% operator, so it is not difficult to find constraints on it that make it
% non-expansive. Proposition 3.1 in \autoref{qiu_dcfnet:ch5:_2018} does just this for
% the ReLU and convolutional layer

% \begin{eqnarray}
% \norm{V\mathcal{L}_\tau x - V x} = 
% \end{eqnarray}

